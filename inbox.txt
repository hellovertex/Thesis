[Outline - initial]

Fr. 21.01 : Thema festgelegt
Fr. 28.01 : Anmeldung eingereicht
Fr. 04.02 : DB - Parser implementiert
Fr. 11.02 : Baseline auf DB training kickoff
Fr. 18.02 : Rainbow agent impl. fertig
Fr. 04.03 : Muesli agent impl. fertig
Fr. 11.03 : Traininsbeginn RL
ab April: Opponent Modelling


[Outline - 01-30]
TL;DR:
JANUARY: TOPIC + ENVIRONMENT
FEBRUARY: BASELINES
MARCH: RL+NE architecture + training
APRIL: Opponent Modelling
May: Analysis
June: Writing?

[x/2] Fr. 21.01 : decided on topic
[x/2] Fr. 28.01 : registration done on my side
[x/2]Fr. 04.02 : implemented text-file parser for pokerstars databases
[-]Fr. 11.02 : Baseline on database using supervised learning: training kickoff
[-]Fr. 18.02 : Rule-Based Agent Implementation 1/2
[-]Fr. 25.02 : Rule-Based Agent Implementation 2/2
[-]Fr. 04.03 : RL+NE 1/3: architecture, DQN + Baselines
[-]Fr. 11.03 : - EXAM
[-]Fr. 18.03 : RL+NE 2/3: training
[-]Fr. 25.03 : RL+NE 3/3: analysis (potential refactoring/debugging)
[-]Fr. 01.04 : Implement Opponent Exploitation from paper [1], 1/3
[-]Fr. 08.04 : Implement Opponent Exploitation from paper [1], 2/3
[-]Fr. 15.04 : Implement Opponent Exploitation from paper [1], 3/3

[-]ab April: Opponent Modelling
[potential issues]
starting stack sizes are converted to int, which may lead to invalid raises unless we add back the rounding diff
[CATCH-UP]
implement pypoker.PokerObservation.last_moves()

[TMP]
01-30: regarding last-moves and all-player-cards
=> format must not change between supervised and RL observations

[RL - Seminar Key Messages]
02-07: Bandits
- [UCB] Upper confidence Bound Action Selection
-- subtly favoring actions that have received fewer samples
-- by adding to Q, the ratio of ln(t) vs N_t(a), the ladder dominating the first
02-07: MDPs
- Everything is Markovian if lifted to infinite dimensional space
- misalignment of reward leads to convergence to wrong goal
- formal MDPs : https://isis.tu-berlin.de/mod/videoservice/view.php/cm/1271465/browse
- control theory : https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/


[TODOS]
02-13: create print_augmented_obs function
02-11: move core/* to supervised_learning/core/*
02-08: check how encoder base class can be typed with a generic vectorizer
02-07: reward as fraction of starting stack (blinds small negativ and lost all in is -1)
02-06: write environment wrapper with augmented observation
02-06: write environment wrapper with seer mode for RL with openboard
02-06: 2. Implement Observation augmentation
02-06: 1. Implement build_action [x]
02-07: also get observation and action for showdown [x]
[[24-01 Replace equity calculator inside neuron_poker package]]
 - postponed and set to [Optional], didnt find a full ring equity calculator
[[24-01 Obtain Baselines]]
- 1. 0.5/0.6 equity agents
- 2. Dont use Nash Equilibrium agent as this will not contribute to exploitability
- 3. Implement TA/LA, TP/LP
[[24-01 Environment features]]
- dynamic number of betting rounds [hold incrementing stage index]
- SEER mode
- [-] Specify Texas Hold Em No-Limit vs Poker taxonomy in docstrings
- [-] 

[[Misc]]
- 02-11: consider training for each num_players separately (with lower dimensional input)
- 01-31: build DataParser and Processor
- 01-31: deal with two hero scenario
- 01-28: sufficient statistics - last moves: for each player, the last two moves per round
==> if two re-raises this sufficiently covers the rounds behaviour for the player
==> 2*num_players*max_rounds*action_length additional bits ==> e.g. 2*6*4*3
- 01-26: use PokerRL env instead of rewriting from scratch
- 01-22: add input neuron that is pretrained to predict equity
- 01-22: implement rule based baselines TAG, LAG, TAP, LAP
- 01-23: review poker env alternatives, czanoci, rlcard.env.make("no-limit-holdem")
- 01-24: add input neuron that predicts poker hand evaluation / simulates twoplustwo evaluator
- 01-24: get env with SEER mode
- 01-24: train in SEER mode ablations (prune until len(private observations=1)
- 01-24: from parser, maintain playerdatabase
- 01-25: when parsing, trim seat numbers to have no gaps in order to match env-seats
- compute hand win chances for all hands with a omnipotent controller
- in pokerstars preflop nur ein raise possible
- flop mehrere raises
- add mlflow logging to SelfPlay.equity_improvement
- add mlflow[extras] to requirements.txt
- colab: run neuroevolution of equity players
- parse showdown traceback to trajectory [TDD regex approach]
- 1. baseline before RL
- 2.
 
[LINKBOX]
[HAND DOWNLOAD]
https://handhq.com/download
[[Equilibrium Approximation Quality of no limit poker bots]]
https://arxiv.org/pdf/1612.07547.pdf

[[Poker Evaluator SUmmary]]
https://www.codingthewheel.com/archives/poker-hand-evaluator-roundup/

[[Opponent Modelling in Poker]]
[1] Opponent modeling and exploitation in poker using evolved recurrent neural networks(same paper as [2]):
https://dl.acm.org/doi/pdf/10.1145/3205455.3205589
[2] Dynamic Adaptation and Opponent Exploitation in Computer Poker  (same paper as [1])
https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/viewFile/17239/15584
[3] Efficient Opponent Exploitation in No-Limit Texas Hold’em Poker:
A Neuroevolutionary Method Combined with Reinforcement Learning
https://www.mdpi.com/2079-9292/10/17/2087
[4] Building a No Limit Texas Hold’em Poker Agent Based on Game Logs using Supervised Learning
https://paginas.fe.up.pt/~niadr/PUBLICATIONS/LIACC_publications_2011_12/pdf/C55_Building_No_Limit_LFT_LPR.pdf
Spotlight Talk - Opponent Modeling and Exploitation in Poker Using Evolved Recurrent Neural Network
https://www.youtube.com/watch?v=dtew-zqNb2o


[BRAINDUMP]
- parser
- infrastructure
- credit
- credit assignment problem
- two separate paths: RL and Evolutionary Baseline

[[2022-18-01]]
reward design? 0-1 reward? -> entscheidung vertagen
erstmal baseline hinbekommen mittels supervised learning und evolutionary algos
spaeter alles sowie opponent modelling, transfer learning
=> Baseline >> RL 

equity rollouts wie oft gewinnen sie den pot
erstmal genug haende kaufen, teuer warten auf geld

[[2022-19-01]]
