{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Environment\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "compute_target = ws.compute_targets['sla-k80']\n",
    "PROJECT_ROOT = '../../'\n",
    "# environment = Environment.from_pip_requirements('env', PROJECT_ROOT+'../requirements_azure')\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000001E41B504F70>\n"
     ]
    }
   ],
   "source": [
    "# data_ref = ds.path('preprocessed.zip').as_download()\n",
    "# print(data_ref.to_config().__dict__)\n",
    "\n",
    "input_data = Dataset.File.from_files(ds.path('0.25-0.50/')).as_named_input('input').as_mount()\n",
    "print(input_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# data_ref = ds.path('preprocessed.zip').as_download()\n",
    "# path_on_compute='~/cloudfiles/code/Users/sascha.lange/data/'\n",
    "# command = f'bash setup.sh && python baselines/supervised_learning/modelling/train.py --t {str(data_ref)}'.split()\n",
    "# command = f'bash setup.sh && python baselines/supervised_learning/modelling/train.py'.split()\n",
    "command = ['bash',\n",
    " 'setup.sh',\n",
    " '&&',\n",
    " 'python',\n",
    " 'baselines/supervised_learning/modelling/train.py', '-t', input_data]\n",
    "config = ScriptRunConfig(\n",
    "    command=command,\n",
    "    compute_target=compute_target,  # compute target used to run train.py script\n",
    "    source_directory=PROJECT_ROOT\n",
    "    # environment=environment,\n",
    "    # script='baselines/supervised_learning/modelling/train.py',\n",
    ")\n",
    "\n",
    "# config.run_config.data_references[data_ref.data_reference_name] = data_ref.to_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: supervised_baseline_training,\n",
      "Id: supervised_baseline_training_1647987601_670c7cb8,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Preparing)\n",
      "RunId: supervised_baseline_training_1647987601_670c7cb8\n",
      "Web View: https://ml.azure.com/runs/supervised_baseline_training_1647987601_670c7cb8?wsid=/subscriptions/2eab4702-c402-4a9c-8fa9-a432d38f3551/resourcegroups/master-sla/workspaces/master-sla-ws&tid=291d05b0-1020-4f36-aac5-b26d20661fe5\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_9303dbd08d779d91f2f2915cff84c89b5e3035ae6043bf94e736b027e68ff53f_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2022-03-22T22:20:40Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=256987 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2022-03-22T22:20:40Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/mounts/workspaceblobstore -- stdout/stderr: \n",
      "2022-03-22T22:20:41Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2022-03-22T22:20:41Z Starting output-watcher...\n",
      "2022-03-22T22:20:41Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2022-03-22T22:20:41Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2022-03-22T22:20:41Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_f7dca5f5f0dac270c156fcb7f532f287\n",
      "Digest: sha256:1ba0fe158be6cf30393d682a14b245ddcb6727074eec0217cedc80252795e38b\n",
      "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_f7dca5f5f0dac270c156fcb7f532f287:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_f7dca5f5f0dac270c156fcb7f532f287:latest\n",
      "2022-03-22T22:20:42Z Check if container supervised_baseline_training_1647987601_670c7cb8_DataSidecar already exist exited with 0, \n",
      "\n",
      "d5c7491e22c184c12ad69f28fed3b6a072f9ddb73cfda43d9f6f84c67a97149c\n",
      "2022-03-22T22:20:42Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2022-03-22T22:20:42Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-7345ea717564d6e1830c42caaa04111b-4e6302de4cd43dd4-01 -sshRequired=false] \n",
      "2022/03/22 22:20:42 Didn't get JobInfoJson from env, now read from file\n",
      "2022/03/22 22:20:42 Suceeded read JobInfoJson from file\n",
      "2022/03/22 22:20:42 Starting App Insight Logger for task:  containerSetup\n",
      "2022/03/22 22:20:42 Version: 3.0.01894.0001 Branch: .SourceBranch Commit: 77cd624\n",
      "2022/03/22 22:20:42 Entered ContainerSetupTask - Preparing infiniband\n",
      "2022/03/22 22:20:42 Starting infiniband setup\n",
      "2022/03/22 22:20:42 Python Version found is Python 3.7.9\n",
      "\n",
      "2022/03/22 22:20:42 Returning Python Version as 3.7\n",
      "2022-03-22T22:20:42Z VMSize: standard_nc6, Host: ubuntu-18, Container: ubuntu-16.04\n",
      "2022-03-22T22:20:42Z Not setting up Infiniband in Container\n",
      "2022/03/22 22:20:42 VMSize: standard_nc6, Host: ubuntu-18, Container: ubuntu-16.04\n",
      "2022/03/22 22:20:42 VMSize: standard_nc6, Host: ubuntu-18, Container: ubuntu-16.04\n",
      "2022/03/22 22:20:42 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2022/03/22 22:20:42 Not setting up Infiniband in Container\n",
      "2022/03/22 22:20:42 Not setting up Infiniband in Container\n",
      "2022/03/22 22:20:42 Python Version found is Python 3.7.9\n",
      "\n",
      "2022/03/22 22:20:42 Returning Python Version as 3.7\n",
      "2022/03/22 22:20:42 sshd inside container not required for job, skipping setup.\n",
      "2022/03/22 22:20:42 All App Insights Logs was sent successfully or the close timeout of 10 was reached\n",
      "2022/03/22 22:20:42 App Insight Client has already been closed\n",
      "2022/03/22 22:20:42 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2022-03-22T22:20:42Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_9303dbd08d779d91f2f2915cff84c89b5e3035ae6043bf94e736b027e68ff53f_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2022-03-22T22:20:44.553701] Entering job preparation.\n",
      "[2022-03-22T22:20:45.197621] Starting job preparation.\n",
      "[2022-03-22T22:20:45.197664] Extracting the control code.\n",
      "[2022-03-22T22:20:45.197959] Starting extract_project.\n",
      "[2022-03-22T22:20:45.198008] Starting to extract zip file.\n",
      "[2022-03-22T22:20:45.221344] Finished extracting zip file.\n",
      "[2022-03-22T22:20:45.224425] Using urllib.request Python 3.0 or later\n",
      "[2022-03-22T22:20:45.224472] Start fetching snapshots.\n",
      "[2022-03-22T22:20:45.224511] Start fetching snapshot.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 47\n",
      "[2022-03-22T22:21:05.529757] Finished fetching snapshot.\n",
      "[2022-03-22T22:21:05.529815] Finished fetching snapshots.\n",
      "[2022-03-22T22:21:05.529847] Finished extract_project.\n",
      "[2022-03-22T22:21:05.529938] Finished fetching and extracting the control code.\n",
      "[2022-03-22T22:21:05.537456] Start run_history_prep.\n",
      "[2022-03-22T22:21:05.546158] Job preparation is complete.\n",
      "[2022-03-22T22:21:05.546293] Entering Data Context Managers in Sidecar\n",
      "[2022-03-22T22:21:05.547025] Running Sidecar prep cmd...\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (azure-core 1.22.1 (/opt/miniconda/lib/python3.7/site-packages), Requirement.parse('azure-core<1.22')).\n",
      "[2022-03-22T22:21:05.879360] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/azureml/supervised_baseline_training_1647987601_670c7cb8\n",
      "[2022-03-22T22:21:05.880012] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
      "[2022-03-22T22:21:06.124] Enter __enter__ of DatasetContextManager\n",
      "[2022-03-22T22:21:06.125] SDK version: azureml-core==1.38.0.post2 azureml-dataprep==2.27.0. Session id: af9bc9e8-65a3-4a4d-a40a-801b3a8d7c7d. Run id: supervised_baseline_training_1647987601_670c7cb8.\n",
      "[2022-03-22T22:21:06.125] Processing 'input'.\n",
      "[2022-03-22T22:21:06.125] Mode: 'mount'.\n",
      "[2022-03-22T22:21:06.125] Path on compute is specified: 'False'.\n",
      "[2022-03-22T22:21:08.480] Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('workspacefilestore', '0.25-0.50/')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"071c5ada-c0db-46a4-98b3-1e169144c024\",\n",
      "    \"name\": \"txt_0.25\",\n",
      "    \"version\": 1,\n",
      "    \"workspace\": \"Workspace.create(name='master-sla-ws', subscription_id='2eab4702-c402-4a9c-8fa9-a432d38f3551', resource_group='master-sla')\"\n",
      "  }\n",
      "}\n",
      "[2022-03-22T22:21:10.127] Mounting input to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024 as folder.\n",
      "[2022-03-22T22:21:10.142] Mounting input to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024.\n",
      "[2022-03-22T22:21:11.144] Mounted input to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024.\n",
      "[2022-03-22T22:21:11.244] Exit __enter__ of DatasetContextManager\n",
      "uri entered in sidecar: None\n",
      "Set Dataset input's target path to /mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024\n",
      "[2022-03-22T22:21:11.245120] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
      "[2022-03-22T22:21:11.921281] Ran Sidecar prep cmd.\n",
      "[2022-03-22T22:21:11.921379] Running Context Managers in Sidecar complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "2022/03/22 22:21:15 Didn't get JobInfoJson from env, now read from file\n",
      "2022/03/22 22:21:15 Suceeded read JobInfoJson from file\n",
      "2022/03/22 22:21:15 Starting App Insight Logger for task:  runTaskLet\n",
      "2022/03/22 22:21:15 Version: 3.0.01894.0001 Branch: .SourceBranch Commit: 77cd624\n",
      "2022/03/22 22:21:15 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2022/03/22 22:21:15 Send process info logs to master server succeeded\n",
      "2022/03/22 22:21:15 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "2022/03/22 22:21:15 Send process info logs to master server succeeded\n",
      "[2022-03-22T22:21:15.080394] Entering context manager injector.\n",
      "[2022-03-22T22:21:15.644358] context_manager_injector.py Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['bash setup.sh && python baselines/supervised_learning/modelling/train.py -t DatasetConsumptionConfig:input'])\n",
      "Script type = COMMAND\n",
      "[2022-03-22T22:21:15.647955] Command=bash setup.sh && python baselines/supervised_learning/modelling/train.py -t DatasetConsumptionConfig:input\n",
      "[2022-03-22T22:21:15.648465] Entering Run History Context Manager.\n",
      "[2022-03-22T22:21:16.403342] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/azureml/supervised_baseline_training_1647987601_670c7cb8\n",
      "[2022-03-22T22:21:16.403432] Starting Linux command : bash setup.sh && python baselines/supervised_learning/modelling/train.py -t /mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024\n",
      "Running setup script\n",
      "Requirement already satisfied: requests in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests) (2020.6.20)\n",
      "Collecting PokerRL\n",
      "  Downloading PokerRL-0.0.3-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: requests in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from PokerRL) (2.27.1)\n",
      "Collecting gym==0.10.9\n",
      "  Downloading gym-0.10.9.tar.gz (1.5 MB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (279 kB)\n",
      "Collecting pycrayon==0.5\n",
      "  Downloading pycrayon-0.5.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests->PokerRL) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests->PokerRL) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests->PokerRL) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests->PokerRL) (1.26.7)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "2022/03/22 22:21:20 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "Requirement already satisfied: numpy>=1.10.4 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gym==0.10.9->PokerRL) (1.19.5)\n",
      "Requirement already satisfied: six in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gym==0.10.9->PokerRL) (1.16.0)\n",
      "Collecting pyglet>=1.2.0\n",
      "  Downloading pyglet-1.5.23-py3-none-any.whl (1.1 MB)\n",
      "Building wheels for collected packages: gym, pycrayon\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.10.9-py3-none-any.whl size=1587026 sha256=d48909716e27743b71fe6aadf02c5b093955d765f6197905e8c945d9622571d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/0f/5f/6f4bc0b2f3dcadc680adb6504e2807e863edbe207b52b6261e\n",
      "  Building wheel for pycrayon (setup.py): started\n",
      "  Building wheel for pycrayon (setup.py): finished with status 'done'\n",
      "  Created wheel for pycrayon: filename=pycrayon-0.5-py3-none-any.whl size=3337 sha256=c287b85ad8fb38bf5c4e1b76197844680b614135fa179dcc91bf2d4e4754c38c\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/60/e7/5538a163c30ab660ec2f9d05e31ce4fd5398f9971168c39c5e\n",
      "Successfully built gym pycrayon\n",
      "Installing collected packages: scipy, pyglet, gym, psutil, pycrayon, PokerRL\n",
      "Successfully installed PokerRL-0.0.3 gym-0.10.9 psutil-5.9.0 pycrayon-0.5 pyglet-1.5.23 scipy-1.5.4\n",
      "Collecting torch\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.10.2\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: six in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gdown) (1.16.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: requests[socks] in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gdown) (2.27.1)\n",
      "Collecting importlib-resources; python_version < \"3.7\"\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]->gdown) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]->gdown) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]->gdown) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm->gdown) (3.6.0)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=6a1f4167df247e95353776d0d3d413e4908b2e78cb6156986dc5e78afba331a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/f9/d3/4594b3b2fe2ee239d0c9eb861f468204652e0a1c6d03755d75\n",
      "Successfully built gdown\n",
      "Installing collected packages: importlib-resources, tqdm, soupsieve, beautifulsoup4, filelock, gdown\n",
      "Successfully installed beautifulsoup4-4.10.0 filelock-3.4.1 gdown-4.4.0 importlib-resources-5.4.0 soupsieve-2.3.1 tqdm-4.63.0\n",
      "Requirement already satisfied: click in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from click) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click) (3.6.0)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.23.1-py3-none-any.whl (15.6 MB)\n",
      "Requirement already satisfied: scipy in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (1.5.4)\n",
      "Collecting protobuf>=3.7.0\n",
      "  Downloading protobuf-3.19.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: click>=7.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (8.0.4)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.4.tar.gz (58 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (20.1.0)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: requests>=2.17.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (2.27.1)\n",
      "Requirement already satisfied: pytz in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (2022.1)\n",
      "Requirement already satisfied: numpy in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (6.0)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (4.8.3)\n",
      "Requirement already satisfied: docker>=4.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (5.0.3)\n",
      "Requirement already satisfied: Flask in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (1.0.3)\n",
      "Requirement already satisfied: packaging in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow) (21.3)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.32-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.19.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from pandas->mlflow) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: six>=1.10.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from alembic->mlflow) (5.4.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (50.3.0.post20201006)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gitpython>=2.1.0->mlflow) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests>=2.17.3->mlflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests>=2.17.3->mlflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests>=2.17.3->mlflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests>=2.17.3->mlflow) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.6.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from docker>=4.0.0->mlflow) (1.3.1)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from Flask->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from Flask->mlflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from packaging->mlflow) (3.0.7)\n",
      "Collecting greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))))\n",
      "  Downloading greenlet-1.1.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from Mako->alembic->mlflow) (2.0.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from Werkzeug>=0.14->Flask->mlflow) (0.8)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.4-py3-none-any.whl size=106877 sha256=8784cbdeb27bcc4e572bc08fe35fd0bf460eac1c9c0a424c4ca1f5b6baaebb4b\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/08/b0/8d0a22c4625004e1b16012ecfc1efcb515d54ba674dd94f29e\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: protobuf, pandas, databricks-cli, greenlet, sqlalchemy, Mako, alembic, smmap, gitdb, gitpython, sqlparse, entrypoints, querystring-parser, prometheus-client, prometheus-flask-exporter, mlflow\n",
      "Successfully installed Mako-1.1.6 alembic-1.7.7 databricks-cli-0.16.4 entrypoints-0.4 gitdb-4.0.9 gitpython-3.1.18 greenlet-1.1.2 mlflow-1.23.1 pandas-1.1.5 prometheus-client-0.13.1 prometheus-flask-exporter-0.19.0 protobuf-3.19.4 querystring-parser-1.2.4 smmap-5.0.0 sqlalchemy-1.4.32 sqlparse-0.4.2\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from tensorboardX) (3.19.4)\n",
      "Requirement already satisfied: six in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from tensorboardX) (1.16.0)\n",
      "Requirement already satisfied: numpy in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from tensorboardX) (1.19.5)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5\n",
      "Collecting azureml-mlflow\n",
      "  Downloading azureml_mlflow-1.39.0.post1-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: jsonpickle in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-mlflow) (2.1.0)\n",
      "Requirement already satisfied: azureml-core~=1.39.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-mlflow) (1.39.0.post1)\n",
      "Collecting mlflow-skinny\n",
      "  Downloading mlflow_skinny-1.23.1-py3-none-any.whl (3.2 MB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from jsonpickle->azureml-mlflow) (4.8.3)\n",
      "Requirement already satisfied: backports.tempfile in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.0)\n",
      "Requirement already satisfied: argcomplete<2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.12.3)\n",
      "Requirement already satisfied: SecretStorage<4.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (3.3.1)\n",
      "Requirement already satisfied: msal-extensions<0.4,>=0.3.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.3.1)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (9.3.0)\n",
      "Requirement already satisfied: pyopenssl<22.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (21.0.0)\n",
      "Requirement already satisfied: azure-mgmt-storage<20.0.0,>=16.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (19.1.0)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry<9.0.0,>=8.2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (8.2.0)\n",
      "Requirement already satisfied: azure-core<1.22 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.21.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (2.8.2)\n",
      "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.6.4)\n",
      "Requirement already satisfied: docker<6.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (5.0.3)\n",
      "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.61.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.15.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.17.0)\n",
      "Requirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (2.27.1)\n",
      "Requirement already satisfied: knack~=0.9.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.9.0)\n",
      "Requirement already satisfied: contextlib2<22.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (21.6.0)\n",
      "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.5.1)\n",
      "Requirement already satisfied: humanfriendly<11.0,>=4.7 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (10.0)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (2.3.0)\n",
      "Requirement already satisfied: jmespath<1.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.10.0)\n",
      "Requirement already satisfied: urllib3<=1.26.7,>=1.23 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.26.7)\n",
      "Requirement already satisfied: paramiko<3.0.0,>=2.0.8 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (2.10.3)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.1.28)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (36.0.2)\n",
      "Requirement already satisfied: pytz in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (2022.1)\n",
      "Requirement already satisfied: msrest<1.0.0,>=0.5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.6.21)\n",
      "Requirement already satisfied: packaging<22.0,>=20.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (21.3)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.61.1)\n",
      "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.2.7)\n",
      "Requirement already satisfied: pkginfo in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (1.8.2)\n",
      "Requirement already satisfied: azure-mgmt-resource<21.0.0,>=15.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (20.1.0)\n",
      "Requirement already satisfied: pathspec<1.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-mlflow) (0.9.0)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (3.1.18)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (3.19.4)\n",
      "Requirement already satisfied: cloudpickle in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (2.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (6.0)\n",
      "Requirement already satisfied: entrypoints in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (0.4)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (0.16.4)\n",
      "Requirement already satisfied: click>=7.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from mlflow-skinny->azureml-mlflow) (8.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->azureml-mlflow) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->azureml-mlflow) (4.1.1)\n",
      "Requirement already satisfied: backports.weakref in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.39.0->azureml-mlflow) (1.0.post1)\n",
      "Requirement already satisfied: jeepney>=0.6 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml-core~=1.39.0->azureml-mlflow) (0.7.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msal-extensions<0.4,>=0.3.0->azureml-core~=1.39.0->azureml-mlflow) (2.4.0)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azure-mgmt-keyvault<10.0.0,>=0.40.0->azureml-core~=1.39.0->azureml-mlflow) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from pyopenssl<22.0.0->azureml-core~=1.39.0->azureml-mlflow) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from docker<6.0.0->azureml-core~=1.39.0->azureml-mlflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-mlflow) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-mlflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-mlflow) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-mlflow) (1.7.1)\n",
      "Requirement already satisfied: pygments in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from knack~=0.9.0->azureml-core~=1.39.0->azureml-mlflow) (2.11.2)\n",
      "Requirement already satisfied: tabulate in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from knack~=0.9.0->azureml-core~=1.39.0->azureml-mlflow) (0.8.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.39.0->azureml-mlflow) (0.4.8)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.39.0->azureml-mlflow) (1.5.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.39.0->azureml-mlflow) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.39.0->azureml-mlflow) (1.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-mlflow) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-mlflow) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from packaging<22.0,>=20.0->azureml-core~=1.39.0->azureml-mlflow) (3.0.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gitpython>=2.1.0->mlflow-skinny->azureml-mlflow) (4.0.9)\n",
      "Requirement already satisfied: pycparser in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.39.0->azureml-mlflow) (2.21)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-mlflow) (3.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow-skinny->azureml-mlflow) (5.0.0)\n",
      "Installing collected packages: mlflow-skinny, azureml-mlflow\n",
      "Successfully installed azureml-mlflow-1.39.0.post1 mlflow-skinny-1.23.1\n",
      "Collecting azureml-tensorboard\n",
      "  Downloading azureml_tensorboard-1.39.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: azureml-core~=1.39.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-tensorboard) (1.39.0.post1)\n",
      "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.2.7)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2.3.0)\n",
      "Requirement already satisfied: contextlib2<22.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (21.6.0)\n",
      "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.6.4)\n",
      "Requirement already satisfied: pkginfo in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.8.2)\n",
      "Requirement already satisfied: paramiko<3.0.0,>=2.0.8 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2.10.3)\n",
      "Requirement already satisfied: azure-mgmt-resource<21.0.0,>=15.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (20.1.0)\n",
      "Requirement already satisfied: azure-mgmt-storage<20.0.0,>=16.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (19.1.0)\n",
      "Requirement already satisfied: knack~=0.9.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.9.0)\n",
      "Requirement already satisfied: packaging<22.0,>=20.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (21.3)\n",
      "Requirement already satisfied: jsonpickle<3.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2.1.0)\n",
      "Requirement already satisfied: msal-extensions<0.4,>=0.3.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.3.1)\n",
      "Requirement already satisfied: azure-core<1.22 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.21.1)\n",
      "Requirement already satisfied: pytz in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2022.1)\n",
      "Requirement already satisfied: argcomplete<2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.12.3)\n",
      "Requirement already satisfied: SecretStorage<4.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (3.3.1)\n",
      "Requirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2.27.1)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry<9.0.0,>=8.2.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (8.2.0)\n",
      "Requirement already satisfied: humanfriendly<11.0,>=4.7 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (10.0)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.1.28)\n",
      "Requirement already satisfied: msrest<1.0.0,>=0.5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.6.21)\n",
      "Requirement already satisfied: urllib3<=1.26.7,>=1.23 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.26.7)\n",
      "Requirement already satisfied: pyopenssl<22.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (21.0.0)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (9.3.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.61.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (2.8.2)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.61.1)\n",
      "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.5.1)\n",
      "Requirement already satisfied: backports.tempfile in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.15.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (1.17.0)\n",
      "Requirement already satisfied: pathspec<1.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.9.0)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (36.0.2)\n",
      "Requirement already satisfied: jmespath<1.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (0.10.0)\n",
      "Requirement already satisfied: docker<6.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azureml-core~=1.39.0->azureml-tensorboard) (5.0.3)\n",
      "Requirement already satisfied: six in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msrestazure<=0.6.4,>=0.4.33->azureml-core~=1.39.0->azureml-tensorboard) (1.16.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.39.0->azureml-tensorboard) (1.5.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml-core~=1.39.0->azureml-tensorboard) (3.2.0)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from azure-mgmt-resource<21.0.0,>=15.0.0->azureml-core~=1.39.0->azureml-tensorboard) (1.3.0)\n",
      "Requirement already satisfied: tabulate in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from knack~=0.9.0->azureml-core~=1.39.0->azureml-tensorboard) (0.8.9)\n",
      "Requirement already satisfied: pyyaml in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from knack~=0.9.0->azureml-core~=1.39.0->azureml-tensorboard) (6.0)\n",
      "Requirement already satisfied: pygments in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from knack~=0.9.0->azureml-core~=1.39.0->azureml-tensorboard) (2.11.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from packaging<22.0,>=20.0->azureml-core~=1.39.0->azureml-tensorboard) (3.0.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from jsonpickle<3.0.0->azureml-core~=1.39.0->azureml-tensorboard) (4.8.3)\n",
      "Requirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msal-extensions<0.4,>=0.3.0->azureml-core~=1.39.0->azureml-tensorboard) (2.4.0)\n",
      "Requirement already satisfied: jeepney>=0.6 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml-core~=1.39.0->azureml-tensorboard) (0.7.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-tensorboard) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-tensorboard) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.39.0->azureml-tensorboard) (1.7.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-tensorboard) (1.3.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-tensorboard) (0.6.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.39.0->azureml-tensorboard) (0.4.8)\n",
      "Requirement already satisfied: backports.weakref in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.39.0->azureml-tensorboard) (1.0.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.39.0->azureml-tensorboard) (1.15.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from docker<6.0.0->azureml-core~=1.39.0->azureml-tensorboard) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.39.0->azureml-tensorboard) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.39.0->azureml-tensorboard) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core~=1.39.0->azureml-tensorboard) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml-core~=1.39.0->azureml-tensorboard) (2.21)\n",
      "Installing collected packages: azureml-tensorboard\n",
      "Successfully installed azureml-tensorboard-1.39.0\n",
      "Finished setup script\n",
      "baselines/supervised_learning/modelling/train.py:9: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  import mlflow\n",
      "===== DATA =====\n",
      "DATA PATH: /mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024\n",
      "LIST FILES IN DATA DIR...\n",
      "['6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_0', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_1', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_10', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_100', '6max_0.25usd_0.50usd_pokerstars_eu.txt_100.amltmp', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_101', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_102', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_103', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_104', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_105', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_106', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_107', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_108', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_109', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_11', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_110', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_111', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_112', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_113', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_114', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_115', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_116', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_117', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_118', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_119', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_12', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_120', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_121', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_122', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_123', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_124', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_125', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_126', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_127', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_128', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_129', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_13', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_130', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_131', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_132', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_133', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_134', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_135', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_136', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_137', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_138', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_139', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_14', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_140', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_141', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_142', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_143', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_144', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_145', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_146', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_147', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_148', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_149', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_15', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_150', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_151', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_152', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_153', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_154', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_155', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_156', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_157', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_158', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_159', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_16', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_161', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_17', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_18', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_19', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_2', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_20', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_21', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_22', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_23', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_24', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_25', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_26', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_27', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_28', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_29', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_3', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_30', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_31', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_32', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_33', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_34', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_35', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_36', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_37', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_38', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_39', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_4', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_40', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_41', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_42', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_43', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_44', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_45', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_46', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_47', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_48', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_49', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_5', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_50', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_51', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_52', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_53', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_54', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_55', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_56', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_57', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_58', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_59', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_6', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_60', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_61', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_62', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_63', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_64', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_65', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_66', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_67', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_68', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_69', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_7', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_70', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_71', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_72', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_73', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_74', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_75', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_76', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_77', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_78', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_79', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_8', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_80', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_81', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_82', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_83', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_84', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_85', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_86', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_87', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_88', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_89', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_9', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_90', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_91', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_92', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_93', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_94', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_95', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_96', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_97', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_98', '6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_99']\n",
      "================\n",
      "['/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_0', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_1', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_10', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_100', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_101', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_102', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_103', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_104', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_105', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_106', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_107', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_108', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_109', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_11', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_110', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_111', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_112', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_113', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_114', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_115', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_116', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_117', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_118', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_119', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_12', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_120', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_121', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_122', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_123', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_124', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_125', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_126', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_127', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_128', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_129', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_13', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_130', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_131', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_132', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_133', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_134', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_135', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_136', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_137', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_138', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_139', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_14', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_140', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_141', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_142', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_143', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_144', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_145', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_146', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_147', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_148', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_149', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_15', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_150', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_151', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_152', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_153', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_154', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_155', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_156', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_157', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_158', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_159', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_16', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_161', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_17', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_18', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_19', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_2', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_20', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_21', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_22', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_23', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_24', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_25', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_26', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_27', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_28', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_29', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_3', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_30', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_31', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_32', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_33', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_34', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_35', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_36', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_37', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_38', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_39', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_4', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_40', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_41', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_42', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_43', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_44', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_45', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_46', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_47', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_48', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_49', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_5', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_50', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_51', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_52', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_53', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_54', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_55', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_56', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_57', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_58', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_59', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_6', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_60', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_61', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_62', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_63', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_64', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_65', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_66', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_67', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_68', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_69', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_7', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_70', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_71', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_72', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_73', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_74', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_75', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_76', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_77', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_78', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_79', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_8', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_80', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_81', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_82', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_83', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_84', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_85', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_86', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_87', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_88', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_89', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_9', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_90', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_91', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_92', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_93', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_94', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_95', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_96', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_97', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_98', '/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/input_071c5ada-c0db-46a4-98b3-1e169144c024/6MAX_0.25USD_0.50USD_Pokerstars_eu.txt_99']\n",
      "161 train files loaded\n",
      "Loading File 0/112 into memory...\n",
      "Loading File 1/112 into memory...\n",
      "Loading File 2/112 into memory...\n",
      "Loading File 3/112 into memory...\n",
      "Loading File 4/112 into memory...\n",
      "Loading File 5/112 into memory...\n",
      "Loading File 6/112 into memory...\n",
      "Loading File 7/112 into memory...\n",
      "Loading File 8/112 into memory...\n",
      "Loading File 9/112 into memory...\n",
      "Loading File 10/112 into memory...\n",
      "Loading File 11/112 into memory...\n",
      "Loading File 12/112 into memory...\n",
      "Loading File 13/112 into memory...\n",
      "Loading File 14/112 into memory...\n",
      "Loading File 15/112 into memory...\n",
      "Loading File 16/112 into memory...\n",
      "Loading File 17/112 into memory...\n",
      "Loading File 18/112 into memory...\n",
      "Loading File 19/112 into memory...\n",
      "Loading File 20/112 into memory...\n",
      "Loading File 21/112 into memory...\n",
      "Loading File 22/112 into memory...\n",
      "Loading File 23/112 into memory...\n",
      "Loading File 24/112 into memory...\n",
      "Loading File 25/112 into memory...\n",
      "Loading File 26/112 into memory...\n",
      "Loading File 27/112 into memory...\n",
      "Loading File 28/112 into memory...\n",
      "Loading File 29/112 into memory...\n",
      "Loading File 30/112 into memory...\n",
      "Loading File 31/112 into memory...\n",
      "Loading File 32/112 into memory...\n",
      "Loading File 33/112 into memory...\n",
      "Loading File 34/112 into memory...\n",
      "Loading File 35/112 into memory...\n",
      "Loading File 36/112 into memory...\n",
      "Loading File 37/112 into memory...\n",
      "Loading File 38/112 into memory...\n",
      "Loading File 39/112 into memory...\n",
      "Loading File 40/112 into memory...\n",
      "Loading File 41/112 into memory...\n",
      "Loading File 42/112 into memory...\n",
      "Loading File 43/112 into memory...\n",
      "Loading File 44/112 into memory...\n",
      "Loading File 45/112 into memory...\n",
      "Loading File 46/112 into memory...\n",
      "Loading File 47/112 into memory...\n",
      "Loading File 48/112 into memory...\n",
      "Loading File 49/112 into memory...\n",
      "Loading File 50/112 into memory...\n",
      "Loading File 51/112 into memory...\n",
      "Loading File 52/112 into memory...\n",
      "Loading File 53/112 into memory...\n",
      "Loading File 54/112 into memory...\n",
      "Loading File 55/112 into memory...\n",
      "Loading File 56/112 into memory...\n",
      "Loading File 57/112 into memory...\n",
      "Loading File 58/112 into memory...\n",
      "Loading File 59/112 into memory...\n",
      "Loading File 60/112 into memory...\n",
      "Loading File 61/112 into memory...\n",
      "Loading File 62/112 into memory...\n",
      "Loading File 63/112 into memory...\n",
      "Loading File 64/112 into memory...\n",
      "Loading File 65/112 into memory...\n",
      "Loading File 66/112 into memory...\n",
      "Loading File 67/112 into memory...\n",
      "Loading File 68/112 into memory...\n",
      "Loading File 69/112 into memory...\n",
      "Loading File 70/112 into memory...\n",
      "Loading File 71/112 into memory...\n",
      "Loading File 72/112 into memory...\n",
      "Loading File 73/112 into memory...\n",
      "Loading File 74/112 into memory...\n",
      "Loading File 75/112 into memory...\n",
      "Loading File 76/112 into memory...\n",
      "Loading File 77/112 into memory...\n",
      "Loading File 78/112 into memory...\n",
      "Loading File 79/112 into memory...\n",
      "Loading File 80/112 into memory...\n",
      "Loading File 81/112 into memory...\n",
      "Loading File 82/112 into memory...\n",
      "Loading File 83/112 into memory...\n",
      "Loading File 84/112 into memory...\n",
      "Loading File 85/112 into memory...\n",
      "Loading File 86/112 into memory...\n",
      "Loading File 87/112 into memory...\n",
      "Loading File 88/112 into memory...\n",
      "Loading File 89/112 into memory...\n",
      "Loading File 90/112 into memory...\n",
      "Loading File 91/112 into memory...\n",
      "Loading File 92/112 into memory...\n",
      "Loading File 93/112 into memory...\n",
      "Loading File 94/112 into memory...\n",
      "Loading File 95/112 into memory...\n",
      "Loading File 96/112 into memory...\n",
      "Loading File 97/112 into memory...\n",
      "Loading File 98/112 into memory...\n",
      "Loading File 99/112 into memory...\n",
      "Loading File 100/112 into memory...\n",
      "Loading File 101/112 into memory...\n",
      "Loading File 102/112 into memory...\n",
      "Loading File 103/112 into memory...\n",
      "Loading File 104/112 into memory...\n",
      "Loading File 105/112 into memory...\n",
      "Loading File 106/112 into memory...\n",
      "Loading File 107/112 into memory...\n",
      "Loading File 108/112 into memory...\n",
      "Loading File 109/112 into memory...\n",
      "Loading File 110/112 into memory...\n",
      "Loading File 111/112 into memory...\n",
      "Loading File 0/32 into memory...\n",
      "Loading File 1/32 into memory...\n",
      "Loading File 2/32 into memory...\n",
      "Loading File 3/32 into memory...\n",
      "Loading File 4/32 into memory...\n",
      "Loading File 5/32 into memory...\n",
      "Loading File 6/32 into memory...\n",
      "Loading File 7/32 into memory...\n",
      "Loading File 8/32 into memory...\n",
      "Loading File 9/32 into memory...\n",
      "Loading File 10/32 into memory...\n",
      "Loading File 11/32 into memory...\n",
      "Loading File 12/32 into memory...\n",
      "Loading File 13/32 into memory...\n",
      "Loading File 14/32 into memory...\n",
      "Loading File 15/32 into memory...\n",
      "Loading File 16/32 into memory...\n",
      "Loading File 17/32 into memory...\n",
      "Loading File 18/32 into memory...\n",
      "Loading File 19/32 into memory...\n",
      "Loading File 20/32 into memory...\n",
      "Loading File 21/32 into memory...\n",
      "Loading File 22/32 into memory...\n",
      "Loading File 23/32 into memory...\n",
      "Loading File 24/32 into memory...\n",
      "Loading File 25/32 into memory...\n",
      "Loading File 26/32 into memory...\n",
      "Loading File 27/32 into memory...\n",
      "Loading File 28/32 into memory...\n",
      "Loading File 29/32 into memory...\n",
      "Loading File 30/32 into memory...\n",
      "Loading File 31/32 into memory...\n",
      "Loading File 0/17 into memory...\n",
      "Loading File 1/17 into memory...\n",
      "Loading File 2/17 into memory...\n",
      "Loading File 3/17 into memory...\n",
      "Loading File 4/17 into memory...\n",
      "Loading File 5/17 into memory...\n",
      "Loading File 6/17 into memory...\n",
      "Loading File 7/17 into memory...\n",
      "Loading File 8/17 into memory...\n",
      "Loading File 9/17 into memory...\n",
      "Loading File 10/17 into memory...\n",
      "Loading File 11/17 into memory...\n",
      "Loading File 12/17 into memory...\n",
      "Loading File 13/17 into memory...\n",
      "Loading File 14/17 into memory...\n",
      "Loading File 15/17 into memory...\n",
      "Loading File 16/17 into memory...\n",
      "Number of files loaded = 161\n",
      "For a total number of 8049485 examples\n",
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code DUMJK989R to authenticate.\n",
      "Interactive authentication successfully completed.\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/master-sla-ws/azureml/supervised_baseline_training_1647987601_670c7cb8/wd/azureml/supervised_baseline_training_1647987601_670c7cb8/baselines/supervised_learning/modelling/model.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.layers[-1](x))\n",
      "Train Epoch: 1 [0/5599780 (0%)]\tLoss: 3.800493\n",
      "Train Epoch: 1 [5120/5599780 (0%)]\tLoss: 3.798423\n",
      "Train Epoch: 1 [10240/5599780 (0%)]\tLoss: 3.836248\n",
      "Train Epoch: 1 [15360/5599780 (0%)]\tLoss: 3.816413\n",
      "Train Epoch: 1 [20480/5599780 (0%)]\tLoss: 3.837667\n",
      "Train Epoch: 1 [25600/5599780 (0%)]\tLoss: 3.870268\n",
      "Train Epoch: 1 [30720/5599780 (1%)]\tLoss: 3.834530\n",
      "Train Epoch: 1 [35840/5599780 (1%)]\tLoss: 3.864962\n",
      "Train Epoch: 1 [40960/5599780 (1%)]\tLoss: 3.821922\n",
      "Train Epoch: 1 [46080/5599780 (1%)]\tLoss: 3.847485\n",
      "Train Epoch: 1 [51200/5599780 (1%)]\tLoss: 3.835478\n",
      "Train Epoch: 1 [56320/5599780 (1%)]\tLoss: 3.839537\n",
      "Train Epoch: 1 [61440/5599780 (1%)]\tLoss: 3.851071\n",
      "Train Epoch: 1 [66560/5599780 (1%)]\tLoss: 3.866813\n",
      "Train Epoch: 1 [71680/5599780 (1%)]\tLoss: 3.832437\n",
      "Train Epoch: 1 [76800/5599780 (1%)]\tLoss: 3.851394\n",
      "Train Epoch: 1 [81920/5599780 (1%)]\tLoss: 3.875066\n",
      "Train Epoch: 1 [87040/5599780 (2%)]\tLoss: 3.807149\n",
      "Train Epoch: 1 [92160/5599780 (2%)]\tLoss: 3.809333\n",
      "Train Epoch: 1 [97280/5599780 (2%)]\tLoss: 3.847492\n",
      "Train Epoch: 1 [102400/5599780 (2%)]\tLoss: 3.804417\n",
      "Train Epoch: 1 [107520/5599780 (2%)]\tLoss: 3.816235\n",
      "Train Epoch: 1 [112640/5599780 (2%)]\tLoss: 3.819344\n",
      "Train Epoch: 1 [117760/5599780 (2%)]\tLoss: 3.824425\n",
      "Train Epoch: 1 [122880/5599780 (2%)]\tLoss: 3.854634\n",
      "Train Epoch: 1 [128000/5599780 (2%)]\tLoss: 3.827044\n",
      "Train Epoch: 1 [133120/5599780 (2%)]\tLoss: 3.866943\n",
      "Train Epoch: 1 [138240/5599780 (2%)]\tLoss: 3.858086\n",
      "Train Epoch: 1 [143360/5599780 (3%)]\tLoss: 3.844682\n",
      "Train Epoch: 1 [148480/5599780 (3%)]\tLoss: 3.829857\n",
      "Train Epoch: 1 [153600/5599780 (3%)]\tLoss: 3.821950\n",
      "Train Epoch: 1 [158720/5599780 (3%)]\tLoss: 3.843571\n",
      "Train Epoch: 1 [163840/5599780 (3%)]\tLoss: 3.843659\n",
      "Train Epoch: 1 [168960/5599780 (3%)]\tLoss: 3.852700\n",
      "Train Epoch: 1 [174080/5599780 (3%)]\tLoss: 3.840917\n",
      "Train Epoch: 1 [179200/5599780 (3%)]\tLoss: 3.823112\n",
      "Train Epoch: 1 [184320/5599780 (3%)]\tLoss: 3.845430\n",
      "Train Epoch: 1 [189440/5599780 (3%)]\tLoss: 3.823992\n",
      "Train Epoch: 1 [194560/5599780 (3%)]\tLoss: 3.809542\n",
      "Train Epoch: 1 [199680/5599780 (4%)]\tLoss: 3.824446\n",
      "Train Epoch: 1 [204800/5599780 (4%)]\tLoss: 3.851016\n",
      "Train Epoch: 1 [209920/5599780 (4%)]\tLoss: 3.845488\n",
      "Train Epoch: 1 [215040/5599780 (4%)]\tLoss: 3.814264\n",
      "Train Epoch: 1 [220160/5599780 (4%)]\tLoss: 3.857333\n",
      "Train Epoch: 1 [225280/5599780 (4%)]\tLoss: 3.842993\n",
      "Train Epoch: 1 [230400/5599780 (4%)]\tLoss: 3.819073\n",
      "Train Epoch: 1 [235520/5599780 (4%)]\tLoss: 3.857189\n",
      "Train Epoch: 1 [240640/5599780 (4%)]\tLoss: 3.818174\n",
      "Train Epoch: 1 [245760/5599780 (4%)]\tLoss: 3.815140\n",
      "Train Epoch: 1 [250880/5599780 (4%)]\tLoss: 3.814218\n",
      "Train Epoch: 1 [256000/5599780 (5%)]\tLoss: 3.851146\n",
      "Train Epoch: 1 [261120/5599780 (5%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [266240/5599780 (5%)]\tLoss: 3.840331\n",
      "Train Epoch: 1 [271360/5599780 (5%)]\tLoss: 3.831818\n",
      "Train Epoch: 1 [276480/5599780 (5%)]\tLoss: 3.857373\n",
      "Train Epoch: 1 [281600/5599780 (5%)]\tLoss: 3.829709\n",
      "Train Epoch: 1 [286720/5599780 (5%)]\tLoss: 3.835523\n",
      "Train Epoch: 1 [291840/5599780 (5%)]\tLoss: 3.835623\n",
      "Train Epoch: 1 [296960/5599780 (5%)]\tLoss: 3.849957\n",
      "Train Epoch: 1 [302080/5599780 (5%)]\tLoss: 3.859205\n",
      "Train Epoch: 1 [307200/5599780 (5%)]\tLoss: 3.840713\n",
      "Train Epoch: 1 [312320/5599780 (6%)]\tLoss: 3.835244\n",
      "Train Epoch: 1 [317440/5599780 (6%)]\tLoss: 3.783923\n",
      "Train Epoch: 1 [322560/5599780 (6%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [327680/5599780 (6%)]\tLoss: 3.822480\n",
      "Train Epoch: 1 [332800/5599780 (6%)]\tLoss: 3.829947\n",
      "Train Epoch: 1 [337920/5599780 (6%)]\tLoss: 3.833501\n",
      "Train Epoch: 1 [343040/5599780 (6%)]\tLoss: 3.845366\n",
      "Train Epoch: 1 [348160/5599780 (6%)]\tLoss: 3.824226\n",
      "Train Epoch: 1 [353280/5599780 (6%)]\tLoss: 3.826641\n",
      "Train Epoch: 1 [358400/5599780 (6%)]\tLoss: 3.829966\n",
      "Train Epoch: 1 [363520/5599780 (6%)]\tLoss: 3.835483\n",
      "Train Epoch: 1 [368640/5599780 (7%)]\tLoss: 3.830353\n",
      "Train Epoch: 1 [373760/5599780 (7%)]\tLoss: 3.831821\n",
      "Train Epoch: 1 [378880/5599780 (7%)]\tLoss: 3.805914\n",
      "Train Epoch: 1 [384000/5599780 (7%)]\tLoss: 3.851447\n",
      "Train Epoch: 1 [389120/5599780 (7%)]\tLoss: 3.814098\n",
      "Train Epoch: 1 [394240/5599780 (7%)]\tLoss: 3.833709\n",
      "Train Epoch: 1 [399360/5599780 (7%)]\tLoss: 3.872216\n",
      "Train Epoch: 1 [404480/5599780 (7%)]\tLoss: 3.808961\n",
      "Train Epoch: 1 [409600/5599780 (7%)]\tLoss: 3.824011\n",
      "Train Epoch: 1 [414720/5599780 (7%)]\tLoss: 3.817852\n",
      "Train Epoch: 1 [419840/5599780 (7%)]\tLoss: 3.844805\n",
      "Train Epoch: 1 [424960/5599780 (8%)]\tLoss: 3.827299\n",
      "Train Epoch: 1 [430080/5599780 (8%)]\tLoss: 3.836820\n",
      "Train Epoch: 1 [435200/5599780 (8%)]\tLoss: 3.890424\n",
      "Train Epoch: 1 [440320/5599780 (8%)]\tLoss: 3.826566\n",
      "Train Epoch: 1 [445440/5599780 (8%)]\tLoss: 3.831805\n",
      "Train Epoch: 1 [450560/5599780 (8%)]\tLoss: 3.815724\n",
      "Train Epoch: 1 [455680/5599780 (8%)]\tLoss: 3.839602\n",
      "Train Epoch: 1 [460800/5599780 (8%)]\tLoss: 3.830151\n",
      "Train Epoch: 1 [465920/5599780 (8%)]\tLoss: 3.845927\n",
      "Train Epoch: 1 [471040/5599780 (8%)]\tLoss: 3.818766\n",
      "Train Epoch: 1 [476160/5599780 (9%)]\tLoss: 3.827904\n",
      "Train Epoch: 1 [481280/5599780 (9%)]\tLoss: 3.814232\n",
      "Train Epoch: 1 [486400/5599780 (9%)]\tLoss: 3.833767\n",
      "Train Epoch: 1 [491520/5599780 (9%)]\tLoss: 3.802516\n",
      "Train Epoch: 1 [496640/5599780 (9%)]\tLoss: 3.835689\n",
      "Train Epoch: 1 [501760/5599780 (9%)]\tLoss: 3.838938\n",
      "Train Epoch: 1 [506880/5599780 (9%)]\tLoss: 3.824246\n",
      "Train Epoch: 1 [512000/5599780 (9%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [517120/5599780 (9%)]\tLoss: 3.819709\n",
      "Train Epoch: 1 [522240/5599780 (9%)]\tLoss: 3.850321\n",
      "Train Epoch: 1 [527360/5599780 (9%)]\tLoss: 3.837691\n",
      "Train Epoch: 1 [532480/5599780 (10%)]\tLoss: 3.829923\n",
      "Train Epoch: 1 [537600/5599780 (10%)]\tLoss: 3.827940\n",
      "Train Epoch: 1 [542720/5599780 (10%)]\tLoss: 3.820131\n",
      "Train Epoch: 1 [547840/5599780 (10%)]\tLoss: 3.849396\n",
      "Train Epoch: 1 [552960/5599780 (10%)]\tLoss: 3.806279\n",
      "Train Epoch: 1 [558080/5599780 (10%)]\tLoss: 3.818177\n",
      "Train Epoch: 1 [563200/5599780 (10%)]\tLoss: 3.847343\n",
      "Train Epoch: 1 [568320/5599780 (10%)]\tLoss: 3.845493\n",
      "Train Epoch: 1 [573440/5599780 (10%)]\tLoss: 3.838007\n",
      "Train Epoch: 1 [578560/5599780 (10%)]\tLoss: 3.854989\n",
      "Train Epoch: 1 [583680/5599780 (10%)]\tLoss: 3.853351\n",
      "Train Epoch: 1 [588800/5599780 (11%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [593920/5599780 (11%)]\tLoss: 3.789242\n",
      "Train Epoch: 1 [599040/5599780 (11%)]\tLoss: 3.814404\n",
      "Train Epoch: 1 [604160/5599780 (11%)]\tLoss: 3.814211\n",
      "Train Epoch: 1 [609280/5599780 (11%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [614400/5599780 (11%)]\tLoss: 3.839641\n",
      "Train Epoch: 1 [619520/5599780 (11%)]\tLoss: 3.829860\n",
      "Train Epoch: 1 [624640/5599780 (11%)]\tLoss: 3.782983\n",
      "Train Epoch: 1 [629760/5599780 (11%)]\tLoss: 3.849440\n",
      "Train Epoch: 1 [634880/5599780 (11%)]\tLoss: 3.829858\n",
      "Train Epoch: 1 [640000/5599780 (11%)]\tLoss: 3.847883\n",
      "Train Epoch: 1 [645120/5599780 (12%)]\tLoss: 3.861899\n",
      "Train Epoch: 1 [650240/5599780 (12%)]\tLoss: 3.826430\n",
      "Train Epoch: 1 [655360/5599780 (12%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [660480/5599780 (12%)]\tLoss: 3.794947\n",
      "Train Epoch: 1 [665600/5599780 (12%)]\tLoss: 3.823960\n",
      "Train Epoch: 1 [670720/5599780 (12%)]\tLoss: 3.833956\n",
      "Train Epoch: 1 [675840/5599780 (12%)]\tLoss: 3.818163\n",
      "Train Epoch: 1 [680960/5599780 (12%)]\tLoss: 3.878666\n",
      "Train Epoch: 1 [686080/5599780 (12%)]\tLoss: 3.808427\n",
      "Train Epoch: 1 [691200/5599780 (12%)]\tLoss: 3.786918\n",
      "Train Epoch: 1 [696320/5599780 (12%)]\tLoss: 3.816081\n",
      "Train Epoch: 1 [701440/5599780 (13%)]\tLoss: 3.831862\n",
      "Train Epoch: 1 [706560/5599780 (13%)]\tLoss: 3.847462\n",
      "Train Epoch: 1 [711680/5599780 (13%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [716800/5599780 (13%)]\tLoss: 3.855300\n",
      "Train Epoch: 1 [721920/5599780 (13%)]\tLoss: 3.812334\n",
      "Train Epoch: 1 [727040/5599780 (13%)]\tLoss: 3.865008\n",
      "Train Epoch: 1 [732160/5599780 (13%)]\tLoss: 3.836014\n",
      "Train Epoch: 1 [737280/5599780 (13%)]\tLoss: 3.830210\n",
      "Train Epoch: 1 [742400/5599780 (13%)]\tLoss: 3.831017\n",
      "Train Epoch: 1 [747520/5599780 (13%)]\tLoss: 3.796217\n",
      "Train Epoch: 1 [752640/5599780 (13%)]\tLoss: 3.855318\n",
      "Train Epoch: 1 [757760/5599780 (14%)]\tLoss: 3.841665\n",
      "Train Epoch: 1 [762880/5599780 (14%)]\tLoss: 3.839691\n",
      "Train Epoch: 1 [768000/5599780 (14%)]\tLoss: 3.824023\n",
      "Train Epoch: 1 [773120/5599780 (14%)]\tLoss: 3.804471\n",
      "Train Epoch: 1 [778240/5599780 (14%)]\tLoss: 3.859173\n",
      "Train Epoch: 1 [783360/5599780 (14%)]\tLoss: 3.830972\n",
      "Train Epoch: 1 [788480/5599780 (14%)]\tLoss: 3.867027\n",
      "Train Epoch: 1 [793600/5599780 (14%)]\tLoss: 3.861449\n",
      "Train Epoch: 1 [798720/5599780 (14%)]\tLoss: 3.798538\n",
      "Train Epoch: 1 [803840/5599780 (14%)]\tLoss: 3.809823\n",
      "Train Epoch: 1 [808960/5599780 (14%)]\tLoss: 3.812337\n",
      "Train Epoch: 1 [814080/5599780 (15%)]\tLoss: 3.833839\n",
      "Train Epoch: 1 [819200/5599780 (15%)]\tLoss: 3.796539\n",
      "Train Epoch: 1 [824320/5599780 (15%)]\tLoss: 3.847490\n",
      "Train Epoch: 1 [829440/5599780 (15%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [834560/5599780 (15%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [839680/5599780 (15%)]\tLoss: 3.797655\n",
      "Train Epoch: 1 [844800/5599780 (15%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [849920/5599780 (15%)]\tLoss: 3.798625\n",
      "Train Epoch: 1 [855040/5599780 (15%)]\tLoss: 3.824034\n",
      "Train Epoch: 1 [860160/5599780 (15%)]\tLoss: 3.823848\n",
      "Train Epoch: 1 [865280/5599780 (15%)]\tLoss: 3.855242\n",
      "Train Epoch: 1 [870400/5599780 (16%)]\tLoss: 3.835718\n",
      "Train Epoch: 1 [875520/5599780 (16%)]\tLoss: 3.843484\n",
      "Train Epoch: 1 [880640/5599780 (16%)]\tLoss: 3.880331\n",
      "Train Epoch: 1 [885760/5599780 (16%)]\tLoss: 3.830498\n",
      "Train Epoch: 1 [890880/5599780 (16%)]\tLoss: 3.857254\n",
      "Train Epoch: 1 [896000/5599780 (16%)]\tLoss: 3.798603\n",
      "Train Epoch: 1 [901120/5599780 (16%)]\tLoss: 3.837669\n",
      "Train Epoch: 1 [906240/5599780 (16%)]\tLoss: 3.863056\n",
      "Train Epoch: 1 [911360/5599780 (16%)]\tLoss: 3.784987\n",
      "Train Epoch: 1 [916480/5599780 (16%)]\tLoss: 3.839386\n",
      "Train Epoch: 1 [921600/5599780 (16%)]\tLoss: 3.841590\n",
      "Train Epoch: 1 [926720/5599780 (17%)]\tLoss: 3.845534\n",
      "Train Epoch: 1 [931840/5599780 (17%)]\tLoss: 3.820090\n",
      "Train Epoch: 1 [936960/5599780 (17%)]\tLoss: 3.825508\n",
      "Train Epoch: 1 [942080/5599780 (17%)]\tLoss: 3.810323\n",
      "Train Epoch: 1 [947200/5599780 (17%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [952320/5599780 (17%)]\tLoss: 3.867019\n",
      "Train Epoch: 1 [957440/5599780 (17%)]\tLoss: 3.798921\n",
      "Train Epoch: 1 [962560/5599780 (17%)]\tLoss: 3.810378\n",
      "Train Epoch: 1 [967680/5599780 (17%)]\tLoss: 3.804448\n",
      "Train Epoch: 1 [972800/5599780 (17%)]\tLoss: 3.855171\n",
      "Train Epoch: 1 [977920/5599780 (17%)]\tLoss: 3.849292\n",
      "Train Epoch: 1 [983040/5599780 (18%)]\tLoss: 3.815996\n",
      "Train Epoch: 1 [988160/5599780 (18%)]\tLoss: 3.812351\n",
      "Train Epoch: 1 [993280/5599780 (18%)]\tLoss: 3.794285\n",
      "Train Epoch: 1 [998400/5599780 (18%)]\tLoss: 3.827840\n",
      "Train Epoch: 1 [1003520/5599780 (18%)]\tLoss: 3.829847\n",
      "Train Epoch: 1 [1008640/5599780 (18%)]\tLoss: 3.820745\n",
      "Train Epoch: 1 [1013760/5599780 (18%)]\tLoss: 3.812274\n",
      "Train Epoch: 1 [1018880/5599780 (18%)]\tLoss: 3.835716\n",
      "Train Epoch: 1 [1024000/5599780 (18%)]\tLoss: 3.821867\n",
      "Train Epoch: 1 [1029120/5599780 (18%)]\tLoss: 3.817279\n",
      "Train Epoch: 1 [1034240/5599780 (18%)]\tLoss: 3.820117\n",
      "Train Epoch: 1 [1039360/5599780 (19%)]\tLoss: 3.796707\n",
      "Train Epoch: 1 [1044480/5599780 (19%)]\tLoss: 3.814328\n",
      "Train Epoch: 1 [1049600/5599780 (19%)]\tLoss: 3.794754\n",
      "Train Epoch: 1 [1054720/5599780 (19%)]\tLoss: 3.804474\n",
      "Train Epoch: 1 [1059840/5599780 (19%)]\tLoss: 3.839616\n",
      "Train Epoch: 1 [1064960/5599780 (19%)]\tLoss: 3.823995\n",
      "Train Epoch: 1 [1070080/5599780 (19%)]\tLoss: 3.808528\n",
      "Train Epoch: 1 [1075200/5599780 (19%)]\tLoss: 3.794655\n",
      "Train Epoch: 1 [1080320/5599780 (19%)]\tLoss: 3.847071\n",
      "Train Epoch: 1 [1085440/5599780 (19%)]\tLoss: 3.824762\n",
      "Train Epoch: 1 [1090560/5599780 (19%)]\tLoss: 3.824050\n",
      "Train Epoch: 1 [1095680/5599780 (20%)]\tLoss: 3.806233\n",
      "Train Epoch: 1 [1100800/5599780 (20%)]\tLoss: 3.824272\n",
      "Train Epoch: 1 [1105920/5599780 (20%)]\tLoss: 3.810318\n",
      "Train Epoch: 1 [1111040/5599780 (20%)]\tLoss: 3.818469\n",
      "Train Epoch: 1 [1116160/5599780 (20%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [1121280/5599780 (20%)]\tLoss: 3.834773\n",
      "Train Epoch: 1 [1126400/5599780 (20%)]\tLoss: 3.814242\n",
      "Train Epoch: 1 [1131520/5599780 (20%)]\tLoss: 3.823989\n",
      "Train Epoch: 1 [1136640/5599780 (20%)]\tLoss: 3.843583\n",
      "Train Epoch: 1 [1141760/5599780 (20%)]\tLoss: 3.794665\n",
      "Train Epoch: 1 [1146880/5599780 (20%)]\tLoss: 3.834692\n",
      "Train Epoch: 1 [1152000/5599780 (21%)]\tLoss: 3.827608\n",
      "Train Epoch: 1 [1157120/5599780 (21%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [1162240/5599780 (21%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [1167360/5599780 (21%)]\tLoss: 3.818130\n",
      "Train Epoch: 1 [1172480/5599780 (21%)]\tLoss: 3.851394\n",
      "Train Epoch: 1 [1177600/5599780 (21%)]\tLoss: 3.830214\n",
      "Train Epoch: 1 [1182720/5599780 (21%)]\tLoss: 3.833885\n",
      "Train Epoch: 1 [1187840/5599780 (21%)]\tLoss: 3.829908\n",
      "Train Epoch: 1 [1192960/5599780 (21%)]\tLoss: 3.837698\n",
      "Train Epoch: 1 [1198080/5599780 (21%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [1203200/5599780 (21%)]\tLoss: 3.818129\n",
      "Train Epoch: 1 [1208320/5599780 (22%)]\tLoss: 3.790802\n",
      "Train Epoch: 1 [1213440/5599780 (22%)]\tLoss: 3.784988\n",
      "Train Epoch: 1 [1218560/5599780 (22%)]\tLoss: 3.866931\n",
      "Train Epoch: 1 [1223680/5599780 (22%)]\tLoss: 3.831806\n",
      "Train Epoch: 1 [1228800/5599780 (22%)]\tLoss: 3.846913\n",
      "Train Epoch: 1 [1233920/5599780 (22%)]\tLoss: 3.866650\n",
      "Train Epoch: 1 [1239040/5599780 (22%)]\tLoss: 3.821763\n",
      "Train Epoch: 1 [1244160/5599780 (22%)]\tLoss: 3.833709\n",
      "Train Epoch: 1 [1249280/5599780 (22%)]\tLoss: 3.829848\n",
      "Train Epoch: 1 [1254400/5599780 (22%)]\tLoss: 3.820267\n",
      "Train Epoch: 1 [1259520/5599780 (22%)]\tLoss: 3.833779\n",
      "Train Epoch: 1 [1264640/5599780 (23%)]\tLoss: 3.804958\n",
      "Train Epoch: 1 [1269760/5599780 (23%)]\tLoss: 3.849333\n",
      "Train Epoch: 1 [1274880/5599780 (23%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [1280000/5599780 (23%)]\tLoss: 3.818141\n",
      "Train Epoch: 1 [1285120/5599780 (23%)]\tLoss: 3.850233\n",
      "Train Epoch: 1 [1290240/5599780 (23%)]\tLoss: 3.819157\n",
      "Train Epoch: 1 [1295360/5599780 (23%)]\tLoss: 3.855240\n",
      "Train Epoch: 1 [1300480/5599780 (23%)]\tLoss: 3.826149\n",
      "Train Epoch: 1 [1305600/5599780 (23%)]\tLoss: 3.863109\n",
      "Train Epoch: 1 [1310720/5599780 (23%)]\tLoss: 3.833815\n",
      "Train Epoch: 1 [1315840/5599780 (23%)]\tLoss: 3.829912\n",
      "Train Epoch: 1 [1320960/5599780 (24%)]\tLoss: 3.847018\n",
      "Train Epoch: 1 [1326080/5599780 (24%)]\tLoss: 3.829887\n",
      "Train Epoch: 1 [1331200/5599780 (24%)]\tLoss: 3.865324\n",
      "Train Epoch: 1 [1336320/5599780 (24%)]\tLoss: 3.830561\n",
      "Train Epoch: 1 [1341440/5599780 (24%)]\tLoss: 3.843085\n",
      "Train Epoch: 1 [1346560/5599780 (24%)]\tLoss: 3.777844\n",
      "Train Epoch: 1 [1351680/5599780 (24%)]\tLoss: 3.810166\n",
      "Train Epoch: 1 [1356800/5599780 (24%)]\tLoss: 3.837722\n",
      "Train Epoch: 1 [1361920/5599780 (24%)]\tLoss: 3.833791\n",
      "Train Epoch: 1 [1367040/5599780 (24%)]\tLoss: 3.827717\n",
      "Train Epoch: 1 [1372160/5599780 (25%)]\tLoss: 3.817874\n",
      "Train Epoch: 1 [1377280/5599780 (25%)]\tLoss: 3.867019\n",
      "Train Epoch: 1 [1382400/5599780 (25%)]\tLoss: 3.800553\n",
      "Train Epoch: 1 [1387520/5599780 (25%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [1392640/5599780 (25%)]\tLoss: 3.839417\n",
      "Train Epoch: 1 [1397760/5599780 (25%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [1402880/5599780 (25%)]\tLoss: 3.820105\n",
      "Train Epoch: 1 [1408000/5599780 (25%)]\tLoss: 3.859207\n",
      "Train Epoch: 1 [1413120/5599780 (25%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [1418240/5599780 (25%)]\tLoss: 3.826166\n",
      "Train Epoch: 1 [1423360/5599780 (25%)]\tLoss: 3.820111\n",
      "Train Epoch: 1 [1428480/5599780 (26%)]\tLoss: 3.797518\n",
      "Train Epoch: 1 [1433600/5599780 (26%)]\tLoss: 3.833765\n",
      "Train Epoch: 1 [1438720/5599780 (26%)]\tLoss: 3.827900\n",
      "Train Epoch: 1 [1443840/5599780 (26%)]\tLoss: 3.833802\n",
      "Train Epoch: 1 [1448960/5599780 (26%)]\tLoss: 3.841626\n",
      "Train Epoch: 1 [1454080/5599780 (26%)]\tLoss: 3.819538\n",
      "Train Epoch: 1 [1459200/5599780 (26%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [1464320/5599780 (26%)]\tLoss: 3.839595\n",
      "Train Epoch: 1 [1469440/5599780 (26%)]\tLoss: 3.828105\n",
      "Train Epoch: 1 [1474560/5599780 (26%)]\tLoss: 3.851385\n",
      "Train Epoch: 1 [1479680/5599780 (26%)]\tLoss: 3.806459\n",
      "Train Epoch: 1 [1484800/5599780 (27%)]\tLoss: 3.847485\n",
      "Train Epoch: 1 [1489920/5599780 (27%)]\tLoss: 3.849378\n",
      "Train Epoch: 1 [1495040/5599780 (27%)]\tLoss: 3.825077\n",
      "Train Epoch: 1 [1500160/5599780 (27%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [1505280/5599780 (27%)]\tLoss: 3.853348\n",
      "Train Epoch: 1 [1510400/5599780 (27%)]\tLoss: 3.819744\n",
      "Train Epoch: 1 [1515520/5599780 (27%)]\tLoss: 3.855243\n",
      "Train Epoch: 1 [1520640/5599780 (27%)]\tLoss: 3.819950\n",
      "Train Epoch: 1 [1525760/5599780 (27%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [1530880/5599780 (27%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [1536000/5599780 (27%)]\tLoss: 3.839599\n",
      "Train Epoch: 1 [1541120/5599780 (28%)]\tLoss: 3.792801\n",
      "Train Epoch: 1 [1546240/5599780 (28%)]\tLoss: 3.828305\n",
      "Train Epoch: 1 [1551360/5599780 (28%)]\tLoss: 3.800535\n",
      "Train Epoch: 1 [1556480/5599780 (28%)]\tLoss: 3.870907\n",
      "Train Epoch: 1 [1561600/5599780 (28%)]\tLoss: 3.861104\n",
      "Train Epoch: 1 [1566720/5599780 (28%)]\tLoss: 3.845494\n",
      "Train Epoch: 1 [1571840/5599780 (28%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [1576960/5599780 (28%)]\tLoss: 3.831795\n",
      "Train Epoch: 1 [1582080/5599780 (28%)]\tLoss: 3.816242\n",
      "Train Epoch: 1 [1587200/5599780 (28%)]\tLoss: 3.843373\n",
      "Train Epoch: 1 [1592320/5599780 (28%)]\tLoss: 3.870926\n",
      "Train Epoch: 1 [1597440/5599780 (29%)]\tLoss: 3.786941\n",
      "Train Epoch: 1 [1602560/5599780 (29%)]\tLoss: 3.810365\n",
      "Train Epoch: 1 [1607680/5599780 (29%)]\tLoss: 3.821974\n",
      "Train Epoch: 1 [1612800/5599780 (29%)]\tLoss: 3.791071\n",
      "Train Epoch: 1 [1617920/5599780 (29%)]\tLoss: 3.806593\n",
      "Train Epoch: 1 [1623040/5599780 (29%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [1628160/5599780 (29%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [1633280/5599780 (29%)]\tLoss: 3.829897\n",
      "Train Epoch: 1 [1638400/5599780 (29%)]\tLoss: 3.866960\n",
      "Train Epoch: 1 [1643520/5599780 (29%)]\tLoss: 3.847495\n",
      "Train Epoch: 1 [1648640/5599780 (29%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [1653760/5599780 (30%)]\tLoss: 3.849814\n",
      "Train Epoch: 1 [1658880/5599780 (30%)]\tLoss: 3.814286\n",
      "Train Epoch: 1 [1664000/5599780 (30%)]\tLoss: 3.816121\n",
      "Train Epoch: 1 [1669120/5599780 (30%)]\tLoss: 3.835756\n",
      "Train Epoch: 1 [1674240/5599780 (30%)]\tLoss: 3.808405\n",
      "Train Epoch: 1 [1679360/5599780 (30%)]\tLoss: 3.820145\n",
      "Train Epoch: 1 [1684480/5599780 (30%)]\tLoss: 3.786941\n",
      "Train Epoch: 1 [1689600/5599780 (30%)]\tLoss: 3.800587\n",
      "Train Epoch: 1 [1694720/5599780 (30%)]\tLoss: 3.790828\n",
      "Train Epoch: 1 [1699840/5599780 (30%)]\tLoss: 3.829798\n",
      "Train Epoch: 1 [1704960/5599780 (30%)]\tLoss: 3.837709\n",
      "Train Epoch: 1 [1710080/5599780 (31%)]\tLoss: 3.824309\n",
      "Train Epoch: 1 [1715200/5599780 (31%)]\tLoss: 3.844706\n",
      "Train Epoch: 1 [1720320/5599780 (31%)]\tLoss: 3.822093\n",
      "Train Epoch: 1 [1725440/5599780 (31%)]\tLoss: 3.822097\n",
      "Train Epoch: 1 [1730560/5599780 (31%)]\tLoss: 3.806828\n",
      "Train Epoch: 1 [1735680/5599780 (31%)]\tLoss: 3.862772\n",
      "Train Epoch: 1 [1740800/5599780 (31%)]\tLoss: 3.833599\n",
      "Train Epoch: 1 [1745920/5599780 (31%)]\tLoss: 3.863052\n",
      "Train Epoch: 1 [1751040/5599780 (31%)]\tLoss: 3.833126\n",
      "Train Epoch: 1 [1756160/5599780 (31%)]\tLoss: 3.853235\n",
      "Train Epoch: 1 [1761280/5599780 (31%)]\tLoss: 3.827971\n",
      "Train Epoch: 1 [1766400/5599780 (32%)]\tLoss: 3.843913\n",
      "Train Epoch: 1 [1771520/5599780 (32%)]\tLoss: 3.827903\n",
      "Train Epoch: 1 [1776640/5599780 (32%)]\tLoss: 3.829858\n",
      "Train Epoch: 1 [1781760/5599780 (32%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [1786880/5599780 (32%)]\tLoss: 3.843432\n",
      "Train Epoch: 1 [1792000/5599780 (32%)]\tLoss: 3.816240\n",
      "Train Epoch: 1 [1797120/5599780 (32%)]\tLoss: 3.855241\n",
      "Train Epoch: 1 [1802240/5599780 (32%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [1807360/5599780 (32%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [1812480/5599780 (32%)]\tLoss: 3.833802\n",
      "Train Epoch: 1 [1817600/5599780 (32%)]\tLoss: 3.798660\n",
      "Train Epoch: 1 [1822720/5599780 (33%)]\tLoss: 3.865001\n",
      "Train Epoch: 1 [1827840/5599780 (33%)]\tLoss: 3.797121\n",
      "Train Epoch: 1 [1832960/5599780 (33%)]\tLoss: 3.849393\n",
      "Train Epoch: 1 [1838080/5599780 (33%)]\tLoss: 3.804519\n",
      "Train Epoch: 1 [1843200/5599780 (33%)]\tLoss: 3.833745\n",
      "Train Epoch: 1 [1848320/5599780 (33%)]\tLoss: 3.827870\n",
      "Train Epoch: 1 [1853440/5599780 (33%)]\tLoss: 3.816234\n",
      "Train Epoch: 1 [1858560/5599780 (33%)]\tLoss: 3.827888\n",
      "Train Epoch: 1 [1863680/5599780 (33%)]\tLoss: 3.823905\n",
      "Train Epoch: 1 [1868800/5599780 (33%)]\tLoss: 3.788898\n",
      "Train Epoch: 1 [1873920/5599780 (33%)]\tLoss: 3.828288\n",
      "Train Epoch: 1 [1879040/5599780 (34%)]\tLoss: 3.831794\n",
      "Train Epoch: 1 [1884160/5599780 (34%)]\tLoss: 3.857248\n",
      "Train Epoch: 1 [1889280/5599780 (34%)]\tLoss: 3.829869\n",
      "Train Epoch: 1 [1894400/5599780 (34%)]\tLoss: 3.841577\n",
      "Train Epoch: 1 [1899520/5599780 (34%)]\tLoss: 3.816250\n",
      "Train Epoch: 1 [1904640/5599780 (34%)]\tLoss: 3.818262\n",
      "Train Epoch: 1 [1909760/5599780 (34%)]\tLoss: 3.831832\n",
      "Train Epoch: 1 [1914880/5599780 (34%)]\tLoss: 3.804463\n",
      "Train Epoch: 1 [1920000/5599780 (34%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [1925120/5599780 (34%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [1930240/5599780 (34%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [1935360/5599780 (35%)]\tLoss: 3.796429\n",
      "Train Epoch: 1 [1940480/5599780 (35%)]\tLoss: 3.853344\n",
      "Train Epoch: 1 [1945600/5599780 (35%)]\tLoss: 3.855232\n",
      "Train Epoch: 1 [1950720/5599780 (35%)]\tLoss: 3.844805\n",
      "Train Epoch: 1 [1955840/5599780 (35%)]\tLoss: 3.800612\n",
      "Train Epoch: 1 [1960960/5599780 (35%)]\tLoss: 3.816239\n",
      "Train Epoch: 1 [1966080/5599780 (35%)]\tLoss: 3.798466\n",
      "Train Epoch: 1 [1971200/5599780 (35%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [1976320/5599780 (35%)]\tLoss: 3.837681\n",
      "Train Epoch: 1 [1981440/5599780 (35%)]\tLoss: 3.811935\n",
      "Train Epoch: 1 [1986560/5599780 (35%)]\tLoss: 3.862560\n",
      "Train Epoch: 1 [1991680/5599780 (36%)]\tLoss: 3.832000\n",
      "Train Epoch: 1 [1996800/5599780 (36%)]\tLoss: 3.870159\n",
      "Train Epoch: 1 [2001920/5599780 (36%)]\tLoss: 3.841562\n",
      "Train Epoch: 1 [2007040/5599780 (36%)]\tLoss: 3.800630\n",
      "Train Epoch: 1 [2012160/5599780 (36%)]\tLoss: 3.841605\n",
      "Train Epoch: 1 [2017280/5599780 (36%)]\tLoss: 3.841632\n",
      "Train Epoch: 1 [2022400/5599780 (36%)]\tLoss: 3.837718\n",
      "Train Epoch: 1 [2027520/5599780 (36%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [2032640/5599780 (36%)]\tLoss: 3.812511\n",
      "Train Epoch: 1 [2037760/5599780 (36%)]\tLoss: 3.849380\n",
      "Train Epoch: 1 [2042880/5599780 (36%)]\tLoss: 3.824043\n",
      "Train Epoch: 1 [2048000/5599780 (37%)]\tLoss: 3.853453\n",
      "Train Epoch: 1 [2053120/5599780 (37%)]\tLoss: 3.842979\n",
      "Train Epoch: 1 [2058240/5599780 (37%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [2063360/5599780 (37%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [2068480/5599780 (37%)]\tLoss: 3.833684\n",
      "Train Epoch: 1 [2073600/5599780 (37%)]\tLoss: 3.865066\n",
      "Train Epoch: 1 [2078720/5599780 (37%)]\tLoss: 3.816469\n",
      "Train Epoch: 1 [2083840/5599780 (37%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [2088960/5599780 (37%)]\tLoss: 3.820158\n",
      "Train Epoch: 1 [2094080/5599780 (37%)]\tLoss: 3.851347\n",
      "Train Epoch: 1 [2099200/5599780 (37%)]\tLoss: 3.843379\n",
      "Train Epoch: 1 [2104320/5599780 (38%)]\tLoss: 3.863125\n",
      "Train Epoch: 1 [2109440/5599780 (38%)]\tLoss: 3.814617\n",
      "Train Epoch: 1 [2114560/5599780 (38%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [2119680/5599780 (38%)]\tLoss: 3.847791\n",
      "Train Epoch: 1 [2124800/5599780 (38%)]\tLoss: 3.833852\n",
      "Train Epoch: 1 [2129920/5599780 (38%)]\tLoss: 3.847500\n",
      "Train Epoch: 1 [2135040/5599780 (38%)]\tLoss: 3.851325\n",
      "Train Epoch: 1 [2140160/5599780 (38%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [2145280/5599780 (38%)]\tLoss: 3.900205\n",
      "Train Epoch: 1 [2150400/5599780 (38%)]\tLoss: 3.802522\n",
      "Train Epoch: 1 [2155520/5599780 (38%)]\tLoss: 3.837701\n",
      "Train Epoch: 1 [2160640/5599780 (39%)]\tLoss: 3.882545\n",
      "Train Epoch: 1 [2165760/5599780 (39%)]\tLoss: 3.825996\n",
      "Train Epoch: 1 [2170880/5599780 (39%)]\tLoss: 3.896242\n",
      "Train Epoch: 1 [2176000/5599780 (39%)]\tLoss: 3.855300\n",
      "Train Epoch: 1 [2181120/5599780 (39%)]\tLoss: 3.845467\n",
      "Train Epoch: 1 [2186240/5599780 (39%)]\tLoss: 3.841627\n",
      "Train Epoch: 1 [2191360/5599780 (39%)]\tLoss: 3.845474\n",
      "Train Epoch: 1 [2196480/5599780 (39%)]\tLoss: 3.850055\n",
      "Train Epoch: 1 [2201600/5599780 (39%)]\tLoss: 3.810380\n",
      "Train Epoch: 1 [2206720/5599780 (39%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [2211840/5599780 (39%)]\tLoss: 3.874767\n",
      "Train Epoch: 1 [2216960/5599780 (40%)]\tLoss: 3.841401\n",
      "Train Epoch: 1 [2222080/5599780 (40%)]\tLoss: 3.816118\n",
      "Train Epoch: 1 [2227200/5599780 (40%)]\tLoss: 3.827108\n",
      "Train Epoch: 1 [2232320/5599780 (40%)]\tLoss: 3.812335\n",
      "Train Epoch: 1 [2237440/5599780 (40%)]\tLoss: 3.822091\n",
      "Train Epoch: 1 [2242560/5599780 (40%)]\tLoss: 3.825872\n",
      "Train Epoch: 1 [2247680/5599780 (40%)]\tLoss: 3.865066\n",
      "Train Epoch: 1 [2252800/5599780 (40%)]\tLoss: 3.857254\n",
      "Train Epoch: 1 [2257920/5599780 (40%)]\tLoss: 3.826304\n",
      "Train Epoch: 1 [2263040/5599780 (40%)]\tLoss: 3.797464\n",
      "Train Epoch: 1 [2268160/5599780 (41%)]\tLoss: 3.848182\n",
      "Train Epoch: 1 [2273280/5599780 (41%)]\tLoss: 3.847310\n",
      "Train Epoch: 1 [2278400/5599780 (41%)]\tLoss: 3.847459\n",
      "Train Epoch: 1 [2283520/5599780 (41%)]\tLoss: 3.841502\n",
      "Train Epoch: 1 [2288640/5599780 (41%)]\tLoss: 3.804519\n",
      "Train Epoch: 1 [2293760/5599780 (41%)]\tLoss: 3.841589\n",
      "Train Epoch: 1 [2298880/5599780 (41%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [2304000/5599780 (41%)]\tLoss: 3.842016\n",
      "Train Epoch: 1 [2309120/5599780 (41%)]\tLoss: 3.851140\n",
      "Train Epoch: 1 [2314240/5599780 (41%)]\tLoss: 3.824000\n",
      "Train Epoch: 1 [2319360/5599780 (41%)]\tLoss: 3.812260\n",
      "Train Epoch: 1 [2324480/5599780 (42%)]\tLoss: 3.818164\n",
      "Train Epoch: 1 [2329600/5599780 (42%)]\tLoss: 3.837718\n",
      "Train Epoch: 1 [2334720/5599780 (42%)]\tLoss: 3.859189\n",
      "Train Epoch: 1 [2339840/5599780 (42%)]\tLoss: 3.829913\n",
      "Train Epoch: 1 [2344960/5599780 (42%)]\tLoss: 3.865068\n",
      "Train Epoch: 1 [2350080/5599780 (42%)]\tLoss: 3.833735\n",
      "Train Epoch: 1 [2355200/5599780 (42%)]\tLoss: 3.843535\n",
      "Train Epoch: 1 [2360320/5599780 (42%)]\tLoss: 3.792801\n",
      "Train Epoch: 1 [2365440/5599780 (42%)]\tLoss: 3.835566\n",
      "Train Epoch: 1 [2370560/5599780 (42%)]\tLoss: 3.824003\n",
      "Train Epoch: 1 [2375680/5599780 (42%)]\tLoss: 3.818191\n",
      "Train Epoch: 1 [2380800/5599780 (43%)]\tLoss: 3.808426\n",
      "Train Epoch: 1 [2385920/5599780 (43%)]\tLoss: 3.820143\n",
      "Train Epoch: 1 [2391040/5599780 (43%)]\tLoss: 3.814281\n",
      "Train Epoch: 1 [2396160/5599780 (43%)]\tLoss: 3.854333\n",
      "Train Epoch: 1 [2401280/5599780 (43%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [2406400/5599780 (43%)]\tLoss: 3.886550\n",
      "Train Epoch: 1 [2411520/5599780 (43%)]\tLoss: 3.827927\n",
      "Train Epoch: 1 [2416640/5599780 (43%)]\tLoss: 3.847166\n",
      "Train Epoch: 1 [2421760/5599780 (43%)]\tLoss: 3.856395\n",
      "Train Epoch: 1 [2426880/5599780 (43%)]\tLoss: 3.847475\n",
      "Train Epoch: 1 [2432000/5599780 (43%)]\tLoss: 3.816195\n",
      "Train Epoch: 1 [2437120/5599780 (44%)]\tLoss: 3.810370\n",
      "Train Epoch: 1 [2442240/5599780 (44%)]\tLoss: 3.824025\n",
      "Train Epoch: 1 [2447360/5599780 (44%)]\tLoss: 3.882763\n",
      "Train Epoch: 1 [2452480/5599780 (44%)]\tLoss: 3.847434\n",
      "Train Epoch: 1 [2457600/5599780 (44%)]\tLoss: 3.830127\n",
      "Train Epoch: 1 [2462720/5599780 (44%)]\tLoss: 3.825568\n",
      "Train Epoch: 1 [2467840/5599780 (44%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [2472960/5599780 (44%)]\tLoss: 3.841630\n",
      "Train Epoch: 1 [2478080/5599780 (44%)]\tLoss: 3.835630\n",
      "Train Epoch: 1 [2483200/5599780 (44%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [2488320/5599780 (44%)]\tLoss: 3.841604\n",
      "Train Epoch: 1 [2493440/5599780 (45%)]\tLoss: 3.819622\n",
      "Train Epoch: 1 [2498560/5599780 (45%)]\tLoss: 3.823549\n",
      "Train Epoch: 1 [2503680/5599780 (45%)]\tLoss: 3.853413\n",
      "Train Epoch: 1 [2508800/5599780 (45%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [2513920/5599780 (45%)]\tLoss: 3.798419\n",
      "Train Epoch: 1 [2519040/5599780 (45%)]\tLoss: 3.782916\n",
      "Train Epoch: 1 [2524160/5599780 (45%)]\tLoss: 3.822159\n",
      "Train Epoch: 1 [2529280/5599780 (45%)]\tLoss: 3.865066\n",
      "Train Epoch: 1 [2534400/5599780 (45%)]\tLoss: 3.820002\n",
      "Train Epoch: 1 [2539520/5599780 (45%)]\tLoss: 3.829336\n",
      "Train Epoch: 1 [2544640/5599780 (45%)]\tLoss: 3.800610\n",
      "Train Epoch: 1 [2549760/5599780 (46%)]\tLoss: 3.831845\n",
      "Train Epoch: 1 [2554880/5599780 (46%)]\tLoss: 3.809758\n",
      "Train Epoch: 1 [2560000/5599780 (46%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [2565120/5599780 (46%)]\tLoss: 3.808425\n",
      "Train Epoch: 1 [2570240/5599780 (46%)]\tLoss: 3.816237\n",
      "Train Epoch: 1 [2575360/5599780 (46%)]\tLoss: 3.857179\n",
      "Train Epoch: 1 [2580480/5599780 (46%)]\tLoss: 3.806649\n",
      "Train Epoch: 1 [2585600/5599780 (46%)]\tLoss: 3.814245\n",
      "Train Epoch: 1 [2590720/5599780 (46%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [2595840/5599780 (46%)]\tLoss: 3.851394\n",
      "Train Epoch: 1 [2600960/5599780 (46%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [2606080/5599780 (47%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [2611200/5599780 (47%)]\tLoss: 3.823672\n",
      "Train Epoch: 1 [2616320/5599780 (47%)]\tLoss: 3.769202\n",
      "Train Epoch: 1 [2621440/5599780 (47%)]\tLoss: 3.822021\n",
      "Train Epoch: 1 [2626560/5599780 (47%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [2631680/5599780 (47%)]\tLoss: 3.859594\n",
      "Train Epoch: 1 [2636800/5599780 (47%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [2641920/5599780 (47%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [2647040/5599780 (47%)]\tLoss: 3.810309\n",
      "Train Epoch: 1 [2652160/5599780 (47%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [2657280/5599780 (47%)]\tLoss: 3.853007\n",
      "Train Epoch: 1 [2662400/5599780 (48%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [2667520/5599780 (48%)]\tLoss: 3.862195\n",
      "Train Epoch: 1 [2672640/5599780 (48%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [2677760/5599780 (48%)]\tLoss: 3.806472\n",
      "Train Epoch: 1 [2682880/5599780 (48%)]\tLoss: 3.871894\n",
      "Train Epoch: 1 [2688000/5599780 (48%)]\tLoss: 3.845449\n",
      "Train Epoch: 1 [2693120/5599780 (48%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [2698240/5599780 (48%)]\tLoss: 3.810325\n",
      "Train Epoch: 1 [2703360/5599780 (48%)]\tLoss: 3.851401\n",
      "Train Epoch: 1 [2708480/5599780 (48%)]\tLoss: 3.853336\n",
      "Train Epoch: 1 [2713600/5599780 (48%)]\tLoss: 3.823721\n",
      "Train Epoch: 1 [2718720/5599780 (49%)]\tLoss: 3.806362\n",
      "Train Epoch: 1 [2723840/5599780 (49%)]\tLoss: 3.886551\n",
      "Train Epoch: 1 [2728960/5599780 (49%)]\tLoss: 3.857325\n",
      "Train Epoch: 1 [2734080/5599780 (49%)]\tLoss: 3.810375\n",
      "Train Epoch: 1 [2739200/5599780 (49%)]\tLoss: 3.825794\n",
      "Train Epoch: 1 [2744320/5599780 (49%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [2749440/5599780 (49%)]\tLoss: 3.840985\n",
      "Train Epoch: 1 [2754560/5599780 (49%)]\tLoss: 3.851312\n",
      "Train Epoch: 1 [2759680/5599780 (49%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [2764800/5599780 (49%)]\tLoss: 3.808487\n",
      "Train Epoch: 1 [2769920/5599780 (49%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [2775040/5599780 (50%)]\tLoss: 3.827961\n",
      "Train Epoch: 1 [2780160/5599780 (50%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [2785280/5599780 (50%)]\tLoss: 3.835768\n",
      "Train Epoch: 1 [2790400/5599780 (50%)]\tLoss: 3.833226\n",
      "Train Epoch: 1 [2795520/5599780 (50%)]\tLoss: 3.835679\n",
      "Train Epoch: 1 [2800640/5599780 (50%)]\tLoss: 3.825977\n",
      "Train Epoch: 1 [2805760/5599780 (50%)]\tLoss: 3.876785\n",
      "Train Epoch: 1 [2810880/5599780 (50%)]\tLoss: 3.794754\n",
      "Train Epoch: 1 [2816000/5599780 (50%)]\tLoss: 3.833797\n",
      "Train Epoch: 1 [2821120/5599780 (50%)]\tLoss: 3.833817\n",
      "Train Epoch: 1 [2826240/5599780 (50%)]\tLoss: 3.810062\n",
      "Train Epoch: 1 [2831360/5599780 (51%)]\tLoss: 3.797877\n",
      "Train Epoch: 1 [2836480/5599780 (51%)]\tLoss: 3.804519\n",
      "Train Epoch: 1 [2841600/5599780 (51%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [2846720/5599780 (51%)]\tLoss: 3.870836\n",
      "Train Epoch: 1 [2851840/5599780 (51%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [2856960/5599780 (51%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [2862080/5599780 (51%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [2867200/5599780 (51%)]\tLoss: 3.843583\n",
      "Train Epoch: 1 [2872320/5599780 (51%)]\tLoss: 3.804453\n",
      "Train Epoch: 1 [2877440/5599780 (51%)]\tLoss: 3.876381\n",
      "Train Epoch: 1 [2882560/5599780 (51%)]\tLoss: 3.824049\n",
      "Train Epoch: 1 [2887680/5599780 (52%)]\tLoss: 3.790784\n",
      "Train Epoch: 1 [2892800/5599780 (52%)]\tLoss: 3.804519\n",
      "Train Epoch: 1 [2897920/5599780 (52%)]\tLoss: 3.821052\n",
      "Train Epoch: 1 [2903040/5599780 (52%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [2908160/5599780 (52%)]\tLoss: 3.853350\n",
      "Train Epoch: 1 [2913280/5599780 (52%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [2918400/5599780 (52%)]\tLoss: 3.807859\n",
      "Train Epoch: 1 [2923520/5599780 (52%)]\tLoss: 3.805514\n",
      "Train Epoch: 1 [2928640/5599780 (52%)]\tLoss: 3.808454\n",
      "Train Epoch: 1 [2933760/5599780 (52%)]\tLoss: 3.814024\n",
      "Train Epoch: 1 [2938880/5599780 (52%)]\tLoss: 3.861160\n",
      "Train Epoch: 1 [2944000/5599780 (53%)]\tLoss: 3.853245\n",
      "Train Epoch: 1 [2949120/5599780 (53%)]\tLoss: 3.837648\n",
      "Train Epoch: 1 [2954240/5599780 (53%)]\tLoss: 3.794608\n",
      "Train Epoch: 1 [2959360/5599780 (53%)]\tLoss: 3.806397\n",
      "Train Epoch: 1 [2964480/5599780 (53%)]\tLoss: 3.830114\n",
      "Train Epoch: 1 [2969600/5599780 (53%)]\tLoss: 3.822097\n",
      "Train Epoch: 1 [2974720/5599780 (53%)]\tLoss: 3.831799\n",
      "Train Epoch: 1 [2979840/5599780 (53%)]\tLoss: 3.833764\n",
      "Train Epoch: 1 [2984960/5599780 (53%)]\tLoss: 3.845524\n",
      "Train Epoch: 1 [2990080/5599780 (53%)]\tLoss: 3.847489\n",
      "Train Epoch: 1 [2995200/5599780 (53%)]\tLoss: 3.861160\n",
      "Train Epoch: 1 [3000320/5599780 (54%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [3005440/5599780 (54%)]\tLoss: 3.844337\n",
      "Train Epoch: 1 [3010560/5599780 (54%)]\tLoss: 3.814203\n",
      "Train Epoch: 1 [3015680/5599780 (54%)]\tLoss: 3.831846\n",
      "Train Epoch: 1 [3020800/5599780 (54%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [3025920/5599780 (54%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3031040/5599780 (54%)]\tLoss: 3.824125\n",
      "Train Epoch: 1 [3036160/5599780 (54%)]\tLoss: 3.796707\n",
      "Train Epoch: 1 [3041280/5599780 (54%)]\tLoss: 3.829836\n",
      "Train Epoch: 1 [3046400/5599780 (54%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [3051520/5599780 (54%)]\tLoss: 3.859205\n",
      "Train Epoch: 1 [3056640/5599780 (55%)]\tLoss: 3.827888\n",
      "Train Epoch: 1 [3061760/5599780 (55%)]\tLoss: 3.826009\n",
      "Train Epoch: 1 [3066880/5599780 (55%)]\tLoss: 3.788894\n",
      "Train Epoch: 1 [3072000/5599780 (55%)]\tLoss: 3.861080\n",
      "Train Epoch: 1 [3077120/5599780 (55%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [3082240/5599780 (55%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [3087360/5599780 (55%)]\tLoss: 3.859108\n",
      "Train Epoch: 1 [3092480/5599780 (55%)]\tLoss: 3.790877\n",
      "Train Epoch: 1 [3097600/5599780 (55%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [3102720/5599780 (55%)]\tLoss: 3.792590\n",
      "Train Epoch: 1 [3107840/5599780 (55%)]\tLoss: 3.825956\n",
      "Train Epoch: 1 [3112960/5599780 (56%)]\tLoss: 3.822084\n",
      "Train Epoch: 1 [3118080/5599780 (56%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3123200/5599780 (56%)]\tLoss: 3.852850\n",
      "Train Epoch: 1 [3128320/5599780 (56%)]\tLoss: 3.847154\n",
      "Train Epoch: 1 [3133440/5599780 (56%)]\tLoss: 3.845477\n",
      "Train Epoch: 1 [3138560/5599780 (56%)]\tLoss: 3.831858\n",
      "Train Epoch: 1 [3143680/5599780 (56%)]\tLoss: 3.823593\n",
      "Train Epoch: 1 [3148800/5599780 (56%)]\tLoss: 3.861168\n",
      "Train Epoch: 1 [3153920/5599780 (56%)]\tLoss: 3.849916\n",
      "Train Epoch: 1 [3159040/5599780 (56%)]\tLoss: 3.802566\n",
      "Train Epoch: 1 [3164160/5599780 (57%)]\tLoss: 3.835610\n",
      "Train Epoch: 1 [3169280/5599780 (57%)]\tLoss: 3.808426\n",
      "Train Epoch: 1 [3174400/5599780 (57%)]\tLoss: 3.827897\n",
      "Train Epoch: 1 [3179520/5599780 (57%)]\tLoss: 3.849368\n",
      "Train Epoch: 1 [3184640/5599780 (57%)]\tLoss: 3.827911\n",
      "Train Epoch: 1 [3189760/5599780 (57%)]\tLoss: 3.843483\n",
      "Train Epoch: 1 [3194880/5599780 (57%)]\tLoss: 3.808060\n",
      "Train Epoch: 1 [3200000/5599780 (57%)]\tLoss: 3.853520\n",
      "Train Epoch: 1 [3205120/5599780 (57%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [3210240/5599780 (57%)]\tLoss: 3.802566\n",
      "Train Epoch: 1 [3215360/5599780 (57%)]\tLoss: 3.814134\n",
      "Train Epoch: 1 [3220480/5599780 (58%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [3225600/5599780 (58%)]\tLoss: 3.795970\n",
      "Train Epoch: 1 [3230720/5599780 (58%)]\tLoss: 3.794152\n",
      "Train Epoch: 1 [3235840/5599780 (58%)]\tLoss: 3.835512\n",
      "Train Epoch: 1 [3240960/5599780 (58%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [3246080/5599780 (58%)]\tLoss: 3.859071\n",
      "Train Epoch: 1 [3251200/5599780 (58%)]\tLoss: 3.853460\n",
      "Train Epoch: 1 [3256320/5599780 (58%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [3261440/5599780 (58%)]\tLoss: 3.826182\n",
      "Train Epoch: 1 [3266560/5599780 (58%)]\tLoss: 3.824005\n",
      "Train Epoch: 1 [3271680/5599780 (58%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [3276800/5599780 (59%)]\tLoss: 3.818191\n",
      "Train Epoch: 1 [3281920/5599780 (59%)]\tLoss: 3.867019\n",
      "Train Epoch: 1 [3287040/5599780 (59%)]\tLoss: 3.853348\n",
      "Train Epoch: 1 [3292160/5599780 (59%)]\tLoss: 3.855302\n",
      "Train Epoch: 1 [3297280/5599780 (59%)]\tLoss: 3.839284\n",
      "Train Epoch: 1 [3302400/5599780 (59%)]\tLoss: 3.822370\n",
      "Train Epoch: 1 [3307520/5599780 (59%)]\tLoss: 3.839660\n",
      "Train Epoch: 1 [3312640/5599780 (59%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [3317760/5599780 (59%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [3322880/5599780 (59%)]\tLoss: 3.802566\n",
      "Train Epoch: 1 [3328000/5599780 (59%)]\tLoss: 3.806383\n",
      "Train Epoch: 1 [3333120/5599780 (60%)]\tLoss: 3.820136\n",
      "Train Epoch: 1 [3338240/5599780 (60%)]\tLoss: 3.826027\n",
      "Train Epoch: 1 [3343360/5599780 (60%)]\tLoss: 3.839674\n",
      "Train Epoch: 1 [3348480/5599780 (60%)]\tLoss: 3.849381\n",
      "Train Epoch: 1 [3353600/5599780 (60%)]\tLoss: 3.833757\n",
      "Train Epoch: 1 [3358720/5599780 (60%)]\tLoss: 3.837718\n",
      "Train Epoch: 1 [3363840/5599780 (60%)]\tLoss: 3.805878\n",
      "Train Epoch: 1 [3368960/5599780 (60%)]\tLoss: 3.823978\n",
      "Train Epoch: 1 [3374080/5599780 (60%)]\tLoss: 3.835340\n",
      "Train Epoch: 1 [3379200/5599780 (60%)]\tLoss: 3.839642\n",
      "Train Epoch: 1 [3384320/5599780 (60%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [3389440/5599780 (61%)]\tLoss: 3.872817\n",
      "Train Epoch: 1 [3394560/5599780 (61%)]\tLoss: 3.808394\n",
      "Train Epoch: 1 [3399680/5599780 (61%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [3404800/5599780 (61%)]\tLoss: 3.784999\n",
      "Train Epoch: 1 [3409920/5599780 (61%)]\tLoss: 3.836728\n",
      "Train Epoch: 1 [3415040/5599780 (61%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3420160/5599780 (61%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [3425280/5599780 (61%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [3430400/5599780 (61%)]\tLoss: 3.837636\n",
      "Train Epoch: 1 [3435520/5599780 (61%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [3440640/5599780 (61%)]\tLoss: 3.806472\n",
      "Train Epoch: 1 [3445760/5599780 (62%)]\tLoss: 3.851394\n",
      "Train Epoch: 1 [3450880/5599780 (62%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [3456000/5599780 (62%)]\tLoss: 3.767410\n",
      "Train Epoch: 1 [3461120/5599780 (62%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [3466240/5599780 (62%)]\tLoss: 3.841505\n",
      "Train Epoch: 1 [3471360/5599780 (62%)]\tLoss: 3.839687\n",
      "Train Epoch: 1 [3476480/5599780 (62%)]\tLoss: 3.857254\n",
      "Train Epoch: 1 [3481600/5599780 (62%)]\tLoss: 3.836916\n",
      "Train Epoch: 1 [3486720/5599780 (62%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [3491840/5599780 (62%)]\tLoss: 3.836759\n",
      "Train Epoch: 1 [3496960/5599780 (62%)]\tLoss: 3.878738\n",
      "Train Epoch: 1 [3502080/5599780 (63%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [3507200/5599780 (63%)]\tLoss: 3.827956\n",
      "Train Epoch: 1 [3512320/5599780 (63%)]\tLoss: 3.808425\n",
      "Train Epoch: 1 [3517440/5599780 (63%)]\tLoss: 3.805309\n",
      "Train Epoch: 1 [3522560/5599780 (63%)]\tLoss: 3.839674\n",
      "Train Epoch: 1 [3527680/5599780 (63%)]\tLoss: 3.814224\n",
      "Train Epoch: 1 [3532800/5599780 (63%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [3537920/5599780 (63%)]\tLoss: 3.802566\n",
      "Train Epoch: 1 [3543040/5599780 (63%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [3548160/5599780 (63%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [3553280/5599780 (63%)]\tLoss: 3.814311\n",
      "Train Epoch: 1 [3558400/5599780 (64%)]\tLoss: 3.853292\n",
      "Train Epoch: 1 [3563520/5599780 (64%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [3568640/5599780 (64%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [3573760/5599780 (64%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [3578880/5599780 (64%)]\tLoss: 3.833417\n",
      "Train Epoch: 1 [3584000/5599780 (64%)]\tLoss: 3.814150\n",
      "Train Epoch: 1 [3589120/5599780 (64%)]\tLoss: 3.821950\n",
      "Train Epoch: 1 [3594240/5599780 (64%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [3599360/5599780 (64%)]\tLoss: 3.837690\n",
      "Train Epoch: 1 [3604480/5599780 (64%)]\tLoss: 3.851714\n",
      "Train Epoch: 1 [3609600/5599780 (64%)]\tLoss: 3.835768\n",
      "Train Epoch: 1 [3614720/5599780 (65%)]\tLoss: 3.849159\n",
      "Train Epoch: 1 [3619840/5599780 (65%)]\tLoss: 3.826000\n",
      "Train Epoch: 1 [3624960/5599780 (65%)]\tLoss: 3.832094\n",
      "Train Epoch: 1 [3630080/5599780 (65%)]\tLoss: 3.843570\n",
      "Train Epoch: 1 [3635200/5599780 (65%)]\tLoss: 3.795122\n",
      "Train Epoch: 1 [3640320/5599780 (65%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [3645440/5599780 (65%)]\tLoss: 3.868966\n",
      "Train Epoch: 1 [3650560/5599780 (65%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [3655680/5599780 (65%)]\tLoss: 3.857213\n",
      "Train Epoch: 1 [3660800/5599780 (65%)]\tLoss: 3.828447\n",
      "Train Epoch: 1 [3665920/5599780 (65%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [3671040/5599780 (66%)]\tLoss: 3.853345\n",
      "Train Epoch: 1 [3676160/5599780 (66%)]\tLoss: 3.803067\n",
      "Train Epoch: 1 [3681280/5599780 (66%)]\tLoss: 3.847505\n",
      "Train Epoch: 1 [3686400/5599780 (66%)]\tLoss: 3.781082\n",
      "Train Epoch: 1 [3691520/5599780 (66%)]\tLoss: 3.820103\n",
      "Train Epoch: 1 [3696640/5599780 (66%)]\tLoss: 3.829831\n",
      "Train Epoch: 1 [3701760/5599780 (66%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [3706880/5599780 (66%)]\tLoss: 3.829908\n",
      "Train Epoch: 1 [3712000/5599780 (66%)]\tLoss: 3.847328\n",
      "Train Epoch: 1 [3717120/5599780 (66%)]\tLoss: 3.798660\n",
      "Train Epoch: 1 [3722240/5599780 (66%)]\tLoss: 3.839600\n",
      "Train Epoch: 1 [3727360/5599780 (67%)]\tLoss: 3.839601\n",
      "Train Epoch: 1 [3732480/5599780 (67%)]\tLoss: 3.820113\n",
      "Train Epoch: 1 [3737600/5599780 (67%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [3742720/5599780 (67%)]\tLoss: 3.808367\n",
      "Train Epoch: 1 [3747840/5599780 (67%)]\tLoss: 3.810379\n",
      "Train Epoch: 1 [3752960/5599780 (67%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [3758080/5599780 (67%)]\tLoss: 3.851552\n",
      "Train Epoch: 1 [3763200/5599780 (67%)]\tLoss: 3.800613\n",
      "Train Epoch: 1 [3768320/5599780 (67%)]\tLoss: 3.831912\n",
      "Train Epoch: 1 [3773440/5599780 (67%)]\tLoss: 3.859207\n",
      "Train Epoch: 1 [3778560/5599780 (67%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [3783680/5599780 (68%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [3788800/5599780 (68%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [3793920/5599780 (68%)]\tLoss: 3.881035\n",
      "Train Epoch: 1 [3799040/5599780 (68%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [3804160/5599780 (68%)]\tLoss: 3.830569\n",
      "Train Epoch: 1 [3809280/5599780 (68%)]\tLoss: 3.825942\n",
      "Train Epoch: 1 [3814400/5599780 (68%)]\tLoss: 3.810378\n",
      "Train Epoch: 1 [3819520/5599780 (68%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [3824640/5599780 (68%)]\tLoss: 3.876785\n",
      "Train Epoch: 1 [3829760/5599780 (68%)]\tLoss: 3.861160\n",
      "Train Epoch: 1 [3834880/5599780 (68%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [3840000/5599780 (69%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3845120/5599780 (69%)]\tLoss: 3.872879\n",
      "Train Epoch: 1 [3850240/5599780 (69%)]\tLoss: 3.849583\n",
      "Train Epoch: 1 [3855360/5599780 (69%)]\tLoss: 3.817267\n",
      "Train Epoch: 1 [3860480/5599780 (69%)]\tLoss: 3.856886\n",
      "Train Epoch: 1 [3865600/5599780 (69%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [3870720/5599780 (69%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3875840/5599780 (69%)]\tLoss: 3.847383\n",
      "Train Epoch: 1 [3880960/5599780 (69%)]\tLoss: 3.830926\n",
      "Train Epoch: 1 [3886080/5599780 (69%)]\tLoss: 3.853007\n",
      "Train Epoch: 1 [3891200/5599780 (69%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [3896320/5599780 (70%)]\tLoss: 3.790848\n",
      "Train Epoch: 1 [3901440/5599780 (70%)]\tLoss: 3.839597\n",
      "Train Epoch: 1 [3906560/5599780 (70%)]\tLoss: 3.831859\n",
      "Train Epoch: 1 [3911680/5599780 (70%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [3916800/5599780 (70%)]\tLoss: 3.835764\n",
      "Train Epoch: 1 [3921920/5599780 (70%)]\tLoss: 3.829866\n",
      "Train Epoch: 1 [3927040/5599780 (70%)]\tLoss: 3.814270\n",
      "Train Epoch: 1 [3932160/5599780 (70%)]\tLoss: 3.835770\n",
      "Train Epoch: 1 [3937280/5599780 (70%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [3942400/5599780 (70%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [3947520/5599780 (70%)]\tLoss: 3.820064\n",
      "Train Epoch: 1 [3952640/5599780 (71%)]\tLoss: 3.814098\n",
      "Train Epoch: 1 [3957760/5599780 (71%)]\tLoss: 3.799229\n",
      "Train Epoch: 1 [3962880/5599780 (71%)]\tLoss: 3.821245\n",
      "Train Epoch: 1 [3968000/5599780 (71%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [3973120/5599780 (71%)]\tLoss: 3.835792\n",
      "Train Epoch: 1 [3978240/5599780 (71%)]\tLoss: 3.827485\n",
      "Train Epoch: 1 [3983360/5599780 (71%)]\tLoss: 3.845459\n",
      "Train Epoch: 1 [3988480/5599780 (71%)]\tLoss: 3.810379\n",
      "Train Epoch: 1 [3993600/5599780 (71%)]\tLoss: 3.852038\n",
      "Train Epoch: 1 [3998720/5599780 (71%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [4003840/5599780 (71%)]\tLoss: 3.802734\n",
      "Train Epoch: 1 [4008960/5599780 (72%)]\tLoss: 3.783154\n",
      "Train Epoch: 1 [4014080/5599780 (72%)]\tLoss: 3.822154\n",
      "Train Epoch: 1 [4019200/5599780 (72%)]\tLoss: 3.812275\n",
      "Train Epoch: 1 [4024320/5599780 (72%)]\tLoss: 3.818191\n",
      "Train Epoch: 1 [4029440/5599780 (72%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [4034560/5599780 (72%)]\tLoss: 3.808423\n",
      "Train Epoch: 1 [4039680/5599780 (72%)]\tLoss: 3.814312\n",
      "Train Epoch: 1 [4044800/5599780 (72%)]\tLoss: 3.851390\n",
      "Train Epoch: 1 [4049920/5599780 (72%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [4055040/5599780 (72%)]\tLoss: 3.865066\n",
      "Train Epoch: 1 [4060160/5599780 (72%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [4065280/5599780 (73%)]\tLoss: 3.817128\n",
      "Train Epoch: 1 [4070400/5599780 (73%)]\tLoss: 3.851372\n",
      "Train Epoch: 1 [4075520/5599780 (73%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [4080640/5599780 (73%)]\tLoss: 3.863117\n",
      "Train Epoch: 1 [4085760/5599780 (73%)]\tLoss: 3.835740\n",
      "Train Epoch: 1 [4090880/5599780 (73%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [4096000/5599780 (73%)]\tLoss: 3.847393\n",
      "Train Epoch: 1 [4101120/5599780 (73%)]\tLoss: 3.853495\n",
      "Train Epoch: 1 [4106240/5599780 (73%)]\tLoss: 3.853247\n",
      "Train Epoch: 1 [4111360/5599780 (73%)]\tLoss: 3.824221\n",
      "Train Epoch: 1 [4116480/5599780 (74%)]\tLoss: 3.878738\n",
      "Train Epoch: 1 [4121600/5599780 (74%)]\tLoss: 3.814421\n",
      "Train Epoch: 1 [4126720/5599780 (74%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [4131840/5599780 (74%)]\tLoss: 3.784985\n",
      "Train Epoch: 1 [4136960/5599780 (74%)]\tLoss: 3.867019\n",
      "Train Epoch: 1 [4142080/5599780 (74%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [4147200/5599780 (74%)]\tLoss: 3.814321\n",
      "Train Epoch: 1 [4152320/5599780 (74%)]\tLoss: 3.849244\n",
      "Train Epoch: 1 [4157440/5599780 (74%)]\tLoss: 3.843195\n",
      "Train Epoch: 1 [4162560/5599780 (74%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [4167680/5599780 (74%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4172800/5599780 (75%)]\tLoss: 3.825905\n",
      "Train Epoch: 1 [4177920/5599780 (75%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4183040/5599780 (75%)]\tLoss: 3.833728\n",
      "Train Epoch: 1 [4188160/5599780 (75%)]\tLoss: 3.853231\n",
      "Train Epoch: 1 [4193280/5599780 (75%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [4198400/5599780 (75%)]\tLoss: 3.857254\n",
      "Train Epoch: 1 [4203520/5599780 (75%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4208640/5599780 (75%)]\tLoss: 3.847488\n",
      "Train Epoch: 1 [4213760/5599780 (75%)]\tLoss: 3.779040\n",
      "Train Epoch: 1 [4218880/5599780 (75%)]\tLoss: 3.802566\n",
      "Train Epoch: 1 [4224000/5599780 (75%)]\tLoss: 3.796667\n",
      "Train Epoch: 1 [4229120/5599780 (76%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4234240/5599780 (76%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4239360/5599780 (76%)]\tLoss: 3.808424\n",
      "Train Epoch: 1 [4244480/5599780 (76%)]\tLoss: 3.820127\n",
      "Train Epoch: 1 [4249600/5599780 (76%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [4254720/5599780 (76%)]\tLoss: 3.827757\n",
      "Train Epoch: 1 [4259840/5599780 (76%)]\tLoss: 3.818095\n",
      "Train Epoch: 1 [4264960/5599780 (76%)]\tLoss: 3.831758\n",
      "Train Epoch: 1 [4270080/5599780 (76%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [4275200/5599780 (76%)]\tLoss: 3.838884\n",
      "Train Epoch: 1 [4280320/5599780 (76%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [4285440/5599780 (77%)]\tLoss: 3.800613\n",
      "Train Epoch: 1 [4290560/5599780 (77%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4295680/5599780 (77%)]\tLoss: 3.822097\n",
      "Train Epoch: 1 [4300800/5599780 (77%)]\tLoss: 3.847617\n",
      "Train Epoch: 1 [4305920/5599780 (77%)]\tLoss: 3.818116\n",
      "Train Epoch: 1 [4311040/5599780 (77%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [4316160/5599780 (77%)]\tLoss: 3.822008\n",
      "Train Epoch: 1 [4321280/5599780 (77%)]\tLoss: 3.835744\n",
      "Train Epoch: 1 [4326400/5599780 (77%)]\tLoss: 3.837716\n",
      "Train Epoch: 1 [4331520/5599780 (77%)]\tLoss: 3.875026\n",
      "Train Epoch: 1 [4336640/5599780 (77%)]\tLoss: 3.843579\n",
      "Train Epoch: 1 [4341760/5599780 (78%)]\tLoss: 3.824051\n",
      "Train Epoch: 1 [4346880/5599780 (78%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4352000/5599780 (78%)]\tLoss: 3.817813\n",
      "Train Epoch: 1 [4357120/5599780 (78%)]\tLoss: 3.843494\n",
      "Train Epoch: 1 [4362240/5599780 (78%)]\tLoss: 3.800613\n",
      "Train Epoch: 1 [4367360/5599780 (78%)]\tLoss: 3.816109\n",
      "Train Epoch: 1 [4372480/5599780 (78%)]\tLoss: 3.877060\n",
      "Train Epoch: 1 [4377600/5599780 (78%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [4382720/5599780 (78%)]\tLoss: 3.829709\n",
      "Train Epoch: 1 [4387840/5599780 (78%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [4392960/5599780 (78%)]\tLoss: 3.835647\n",
      "Train Epoch: 1 [4398080/5599780 (79%)]\tLoss: 3.810700\n",
      "Train Epoch: 1 [4403200/5599780 (79%)]\tLoss: 3.861155\n",
      "Train Epoch: 1 [4408320/5599780 (79%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [4413440/5599780 (79%)]\tLoss: 3.872873\n",
      "Train Epoch: 1 [4418560/5599780 (79%)]\tLoss: 3.832162\n",
      "Train Epoch: 1 [4423680/5599780 (79%)]\tLoss: 3.812333\n",
      "Train Epoch: 1 [4428800/5599780 (79%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [4433920/5599780 (79%)]\tLoss: 3.829908\n",
      "Train Epoch: 1 [4439040/5599780 (79%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [4444160/5599780 (79%)]\tLoss: 3.870841\n",
      "Train Epoch: 1 [4449280/5599780 (79%)]\tLoss: 3.835605\n",
      "Train Epoch: 1 [4454400/5599780 (80%)]\tLoss: 3.810378\n",
      "Train Epoch: 1 [4459520/5599780 (80%)]\tLoss: 3.859208\n",
      "Train Epoch: 1 [4464640/5599780 (80%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [4469760/5599780 (80%)]\tLoss: 3.804507\n",
      "Train Epoch: 1 [4474880/5599780 (80%)]\tLoss: 3.841640\n",
      "Train Epoch: 1 [4480000/5599780 (80%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4485120/5599780 (80%)]\tLoss: 3.806398\n",
      "Train Epoch: 1 [4490240/5599780 (80%)]\tLoss: 3.796950\n",
      "Train Epoch: 1 [4495360/5599780 (80%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [4500480/5599780 (80%)]\tLoss: 3.837710\n",
      "Train Epoch: 1 [4505600/5599780 (80%)]\tLoss: 3.809612\n",
      "Train Epoch: 1 [4510720/5599780 (81%)]\tLoss: 3.833748\n",
      "Train Epoch: 1 [4515840/5599780 (81%)]\tLoss: 3.816139\n",
      "Train Epoch: 1 [4520960/5599780 (81%)]\tLoss: 3.863114\n",
      "Train Epoch: 1 [4526080/5599780 (81%)]\tLoss: 3.812225\n",
      "Train Epoch: 1 [4531200/5599780 (81%)]\tLoss: 3.804519\n",
      "Train Epoch: 1 [4536320/5599780 (81%)]\tLoss: 3.831864\n",
      "Train Epoch: 1 [4541440/5599780 (81%)]\tLoss: 3.822101\n",
      "Train Epoch: 1 [4546560/5599780 (81%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4551680/5599780 (81%)]\tLoss: 3.821977\n",
      "Train Epoch: 1 [4556800/5599780 (81%)]\tLoss: 3.798661\n",
      "Train Epoch: 1 [4561920/5599780 (81%)]\tLoss: 3.804310\n",
      "Train Epoch: 1 [4567040/5599780 (82%)]\tLoss: 3.847949\n",
      "Train Epoch: 1 [4572160/5599780 (82%)]\tLoss: 3.839679\n",
      "Train Epoch: 1 [4577280/5599780 (82%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [4582400/5599780 (82%)]\tLoss: 3.841023\n",
      "Train Epoch: 1 [4587520/5599780 (82%)]\tLoss: 3.821802\n",
      "Train Epoch: 1 [4592640/5599780 (82%)]\tLoss: 3.872879\n",
      "Train Epoch: 1 [4597760/5599780 (82%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [4602880/5599780 (82%)]\tLoss: 3.820116\n",
      "Train Epoch: 1 [4608000/5599780 (82%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [4613120/5599780 (82%)]\tLoss: 3.825999\n",
      "Train Epoch: 1 [4618240/5599780 (82%)]\tLoss: 3.856177\n",
      "Train Epoch: 1 [4623360/5599780 (83%)]\tLoss: 3.818051\n",
      "Train Epoch: 1 [4628480/5599780 (83%)]\tLoss: 3.839674\n",
      "Train Epoch: 1 [4633600/5599780 (83%)]\tLoss: 3.792798\n",
      "Train Epoch: 1 [4638720/5599780 (83%)]\tLoss: 3.855025\n",
      "Train Epoch: 1 [4643840/5599780 (83%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [4648960/5599780 (83%)]\tLoss: 3.818192\n",
      "Train Epoch: 1 [4654080/5599780 (83%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [4659200/5599780 (83%)]\tLoss: 3.857255\n",
      "Train Epoch: 1 [4664320/5599780 (83%)]\tLoss: 3.851376\n",
      "Train Epoch: 1 [4669440/5599780 (83%)]\tLoss: 3.837702\n",
      "Train Epoch: 1 [4674560/5599780 (83%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4679680/5599780 (84%)]\tLoss: 3.839622\n",
      "Train Epoch: 1 [4684800/5599780 (84%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [4689920/5599780 (84%)]\tLoss: 3.788894\n",
      "Train Epoch: 1 [4695040/5599780 (84%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [4700160/5599780 (84%)]\tLoss: 3.843531\n",
      "Train Epoch: 1 [4705280/5599780 (84%)]\tLoss: 3.788894\n",
      "Train Epoch: 1 [4710400/5599780 (84%)]\tLoss: 3.880691\n",
      "Train Epoch: 1 [4715520/5599780 (84%)]\tLoss: 3.847491\n",
      "Train Epoch: 1 [4720640/5599780 (84%)]\tLoss: 3.853295\n",
      "Train Epoch: 1 [4725760/5599780 (84%)]\tLoss: 3.854796\n",
      "Train Epoch: 1 [4730880/5599780 (84%)]\tLoss: 3.804569\n",
      "Train Epoch: 1 [4736000/5599780 (85%)]\tLoss: 3.786857\n",
      "Train Epoch: 1 [4741120/5599780 (85%)]\tLoss: 3.818191\n",
      "Train Epoch: 1 [4746240/5599780 (85%)]\tLoss: 3.853348\n",
      "Train Epoch: 1 [4751360/5599780 (85%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [4756480/5599780 (85%)]\tLoss: 3.833808\n",
      "Train Epoch: 1 [4761600/5599780 (85%)]\tLoss: 3.857128\n",
      "Train Epoch: 1 [4766720/5599780 (85%)]\tLoss: 3.863113\n",
      "Train Epoch: 1 [4771840/5599780 (85%)]\tLoss: 3.834028\n",
      "Train Epoch: 1 [4776960/5599780 (85%)]\tLoss: 3.859151\n",
      "Train Epoch: 1 [4782080/5599780 (85%)]\tLoss: 3.820174\n",
      "Train Epoch: 1 [4787200/5599780 (85%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [4792320/5599780 (86%)]\tLoss: 3.841323\n",
      "Train Epoch: 1 [4797440/5599780 (86%)]\tLoss: 3.843523\n",
      "Train Epoch: 1 [4802560/5599780 (86%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [4807680/5599780 (86%)]\tLoss: 3.824279\n",
      "Train Epoch: 1 [4812800/5599780 (86%)]\tLoss: 3.867019\n",
      "Train Epoch: 1 [4817920/5599780 (86%)]\tLoss: 3.835708\n",
      "Train Epoch: 1 [4823040/5599780 (86%)]\tLoss: 3.843581\n",
      "Train Epoch: 1 [4828160/5599780 (86%)]\tLoss: 3.827841\n",
      "Train Epoch: 1 [4833280/5599780 (86%)]\tLoss: 3.837742\n",
      "Train Epoch: 1 [4838400/5599780 (86%)]\tLoss: 3.835638\n",
      "Train Epoch: 1 [4843520/5599780 (86%)]\tLoss: 3.827962\n",
      "Train Epoch: 1 [4848640/5599780 (87%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [4853760/5599780 (87%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [4858880/5599780 (87%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [4864000/5599780 (87%)]\tLoss: 3.804517\n",
      "Train Epoch: 1 [4869120/5599780 (87%)]\tLoss: 3.831863\n",
      "Train Epoch: 1 [4874240/5599780 (87%)]\tLoss: 3.796592\n",
      "Train Epoch: 1 [4879360/5599780 (87%)]\tLoss: 3.855278\n",
      "Train Epoch: 1 [4884480/5599780 (87%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4889600/5599780 (87%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [4894720/5599780 (87%)]\tLoss: 3.828792\n",
      "Train Epoch: 1 [4899840/5599780 (87%)]\tLoss: 3.825039\n",
      "Train Epoch: 1 [4904960/5599780 (88%)]\tLoss: 3.853369\n",
      "Train Epoch: 1 [4910080/5599780 (88%)]\tLoss: 3.798660\n",
      "Train Epoch: 1 [4915200/5599780 (88%)]\tLoss: 3.816104\n",
      "Train Epoch: 1 [4920320/5599780 (88%)]\tLoss: 3.843494\n",
      "Train Epoch: 1 [4925440/5599780 (88%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [4930560/5599780 (88%)]\tLoss: 3.843556\n",
      "Train Epoch: 1 [4935680/5599780 (88%)]\tLoss: 3.820145\n",
      "Train Epoch: 1 [4940800/5599780 (88%)]\tLoss: 3.827958\n",
      "Train Epoch: 1 [4945920/5599780 (88%)]\tLoss: 3.835712\n",
      "Train Epoch: 1 [4951040/5599780 (88%)]\tLoss: 3.823323\n",
      "Train Epoch: 1 [4956160/5599780 (88%)]\tLoss: 3.857259\n",
      "Train Epoch: 1 [4961280/5599780 (89%)]\tLoss: 3.864995\n",
      "Train Epoch: 1 [4966400/5599780 (89%)]\tLoss: 3.857196\n",
      "Train Epoch: 1 [4971520/5599780 (89%)]\tLoss: 3.848073\n",
      "Train Epoch: 1 [4976640/5599780 (89%)]\tLoss: 3.861028\n",
      "Train Epoch: 1 [4981760/5599780 (89%)]\tLoss: 3.847487\n",
      "Train Epoch: 1 [4986880/5599780 (89%)]\tLoss: 3.867022\n",
      "Train Epoch: 1 [4992000/5599780 (89%)]\tLoss: 3.839975\n",
      "Train Epoch: 1 [4997120/5599780 (89%)]\tLoss: 3.851399\n",
      "Train Epoch: 1 [5002240/5599780 (89%)]\tLoss: 3.796707\n",
      "Train Epoch: 1 [5007360/5599780 (89%)]\tLoss: 3.818214\n",
      "Train Epoch: 1 [5012480/5599780 (90%)]\tLoss: 3.837711\n",
      "Train Epoch: 1 [5017600/5599780 (90%)]\tLoss: 3.837592\n",
      "Train Epoch: 1 [5022720/5599780 (90%)]\tLoss: 3.855301\n",
      "Train Epoch: 1 [5027840/5599780 (90%)]\tLoss: 3.837830\n",
      "Train Epoch: 1 [5032960/5599780 (90%)]\tLoss: 3.814603\n",
      "Train Epoch: 1 [5038080/5599780 (90%)]\tLoss: 3.788646\n",
      "Train Epoch: 1 [5043200/5599780 (90%)]\tLoss: 3.820145\n",
      "Train Epoch: 1 [5048320/5599780 (90%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [5053440/5599780 (90%)]\tLoss: 3.808427\n",
      "Train Epoch: 1 [5058560/5599780 (90%)]\tLoss: 3.792783\n",
      "Train Epoch: 1 [5063680/5599780 (90%)]\tLoss: 3.826397\n",
      "Train Epoch: 1 [5068800/5599780 (91%)]\tLoss: 3.851381\n",
      "Train Epoch: 1 [5073920/5599780 (91%)]\tLoss: 3.868926\n",
      "Train Epoch: 1 [5079040/5599780 (91%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [5084160/5599780 (91%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [5089280/5599780 (91%)]\tLoss: 3.837715\n",
      "Train Epoch: 1 [5094400/5599780 (91%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [5099520/5599780 (91%)]\tLoss: 3.814275\n",
      "Train Epoch: 1 [5104640/5599780 (91%)]\tLoss: 3.855439\n",
      "Train Epoch: 1 [5109760/5599780 (91%)]\tLoss: 3.806473\n",
      "Train Epoch: 1 [5114880/5599780 (91%)]\tLoss: 3.812332\n",
      "Train Epoch: 1 [5120000/5599780 (91%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [5125120/5599780 (92%)]\tLoss: 3.824047\n",
      "Train Epoch: 1 [5130240/5599780 (92%)]\tLoss: 3.800562\n",
      "Train Epoch: 1 [5135360/5599780 (92%)]\tLoss: 3.827957\n",
      "Train Epoch: 1 [5140480/5599780 (92%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [5145600/5599780 (92%)]\tLoss: 3.829909\n",
      "Train Epoch: 1 [5150720/5599780 (92%)]\tLoss: 3.820109\n",
      "Train Epoch: 1 [5155840/5599780 (92%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [5160960/5599780 (92%)]\tLoss: 3.835340\n",
      "Train Epoch: 1 [5166080/5599780 (92%)]\tLoss: 3.808300\n",
      "Train Epoch: 1 [5171200/5599780 (92%)]\tLoss: 3.808426\n",
      "Train Epoch: 1 [5176320/5599780 (92%)]\tLoss: 3.835769\n",
      "Train Epoch: 1 [5181440/5599780 (93%)]\tLoss: 3.874833\n",
      "Train Epoch: 1 [5186560/5599780 (93%)]\tLoss: 3.777071\n",
      "Train Epoch: 1 [5191680/5599780 (93%)]\tLoss: 3.816272\n",
      "Train Epoch: 1 [5196800/5599780 (93%)]\tLoss: 3.823949\n",
      "Train Epoch: 1 [5201920/5599780 (93%)]\tLoss: 3.822096\n",
      "Train Epoch: 1 [5207040/5599780 (93%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [5212160/5599780 (93%)]\tLoss: 3.824054\n",
      "Train Epoch: 1 [5217280/5599780 (93%)]\tLoss: 3.847971\n",
      "Train Epoch: 1 [5222400/5599780 (93%)]\tLoss: 3.800613\n",
      "Train Epoch: 1 [5227520/5599780 (93%)]\tLoss: 3.841626\n",
      "Train Epoch: 1 [5232640/5599780 (93%)]\tLoss: 3.825994\n",
      "Train Epoch: 1 [5237760/5599780 (94%)]\tLoss: 3.824055\n",
      "Train Epoch: 1 [5242880/5599780 (94%)]\tLoss: 3.814260\n",
      "Train Epoch: 1 [5248000/5599780 (94%)]\tLoss: 3.835761\n",
      "Train Epoch: 1 [5253120/5599780 (94%)]\tLoss: 3.811093\n",
      "Train Epoch: 1 [5258240/5599780 (94%)]\tLoss: 3.855299\n",
      "Train Epoch: 1 [5263360/5599780 (94%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [5268480/5599780 (94%)]\tLoss: 3.861158\n",
      "Train Epoch: 1 [5273600/5599780 (94%)]\tLoss: 3.823900\n",
      "Train Epoch: 1 [5278720/5599780 (94%)]\tLoss: 3.829910\n",
      "Train Epoch: 1 [5283840/5599780 (94%)]\tLoss: 3.841629\n",
      "Train Epoch: 1 [5288960/5599780 (94%)]\tLoss: 3.833807\n",
      "Train Epoch: 1 [5294080/5599780 (95%)]\tLoss: 3.835768\n",
      "Train Epoch: 1 [5299200/5599780 (95%)]\tLoss: 3.861036\n",
      "Train Epoch: 1 [5304320/5599780 (95%)]\tLoss: 3.796597\n",
      "Train Epoch: 1 [5309440/5599780 (95%)]\tLoss: 3.810269\n",
      "Train Epoch: 1 [5314560/5599780 (95%)]\tLoss: 3.853203\n",
      "Train Epoch: 1 [5319680/5599780 (95%)]\tLoss: 3.818191\n",
      "Train Epoch: 1 [5324800/5599780 (95%)]\tLoss: 3.859207\n",
      "Train Epoch: 1 [5329920/5599780 (95%)]\tLoss: 3.821673\n",
      "Train Epoch: 1 [5335040/5599780 (95%)]\tLoss: 3.841593\n",
      "Train Epoch: 1 [5340160/5599780 (95%)]\tLoss: 3.847488\n",
      "Train Epoch: 1 [5345280/5599780 (95%)]\tLoss: 3.822098\n",
      "Train Epoch: 1 [5350400/5599780 (96%)]\tLoss: 3.806151\n",
      "Train Epoch: 1 [5355520/5599780 (96%)]\tLoss: 3.823943\n",
      "Train Epoch: 1 [5360640/5599780 (96%)]\tLoss: 3.826154\n",
      "Train Epoch: 1 [5365760/5599780 (96%)]\tLoss: 3.884597\n",
      "Train Epoch: 1 [5370880/5599780 (96%)]\tLoss: 3.827771\n",
      "Train Epoch: 1 [5376000/5599780 (96%)]\tLoss: 3.800464\n",
      "Train Epoch: 1 [5381120/5599780 (96%)]\tLoss: 3.820145\n",
      "Train Epoch: 1 [5386240/5599780 (96%)]\tLoss: 3.827956\n",
      "Train Epoch: 1 [5391360/5599780 (96%)]\tLoss: 3.845673\n",
      "Train Epoch: 1 [5396480/5599780 (96%)]\tLoss: 3.818195\n",
      "Train Epoch: 1 [5401600/5599780 (96%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [5406720/5599780 (97%)]\tLoss: 3.820144\n",
      "Train Epoch: 1 [5411840/5599780 (97%)]\tLoss: 3.843582\n",
      "Train Epoch: 1 [5416960/5599780 (97%)]\tLoss: 3.849441\n",
      "Train Epoch: 1 [5422080/5599780 (97%)]\tLoss: 3.831415\n",
      "Train Epoch: 1 [5427200/5599780 (97%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [5432320/5599780 (97%)]\tLoss: 3.836016\n",
      "Train Epoch: 1 [5437440/5599780 (97%)]\tLoss: 3.845535\n",
      "Train Epoch: 1 [5442560/5599780 (97%)]\tLoss: 3.847488\n",
      "Train Epoch: 1 [5447680/5599780 (97%)]\tLoss: 3.859207\n",
      "Train Epoch: 1 [5452800/5599780 (97%)]\tLoss: 3.851375\n",
      "Train Epoch: 1 [5457920/5599780 (97%)]\tLoss: 3.845548\n",
      "Train Epoch: 1 [5463040/5599780 (98%)]\tLoss: 3.839676\n",
      "Train Epoch: 1 [5468160/5599780 (98%)]\tLoss: 3.800536\n",
      "Train Epoch: 1 [5473280/5599780 (98%)]\tLoss: 3.835727\n",
      "Train Epoch: 1 [5478400/5599780 (98%)]\tLoss: 3.800613\n",
      "Train Epoch: 1 [5483520/5599780 (98%)]\tLoss: 3.843576\n",
      "Train Epoch: 1 [5488640/5599780 (98%)]\tLoss: 3.826004\n",
      "Train Epoch: 1 [5493760/5599780 (98%)]\tLoss: 3.791770\n",
      "Train Epoch: 1 [5498880/5599780 (98%)]\tLoss: 3.814285\n",
      "Train Epoch: 1 [5504000/5599780 (98%)]\tLoss: 3.839675\n",
      "Train Epoch: 1 [5509120/5599780 (98%)]\tLoss: 3.859208\n",
      "Train Epoch: 1 [5514240/5599780 (98%)]\tLoss: 3.886576\n",
      "Train Epoch: 1 [5519360/5599780 (99%)]\tLoss: 3.824052\n",
      "Train Epoch: 1 [5524480/5599780 (99%)]\tLoss: 3.833815\n",
      "Train Epoch: 1 [5529600/5599780 (99%)]\tLoss: 3.843606\n",
      "Train Epoch: 1 [5534720/5599780 (99%)]\tLoss: 3.816238\n",
      "Train Epoch: 1 [5539840/5599780 (99%)]\tLoss: 3.808451\n",
      "Train Epoch: 1 [5544960/5599780 (99%)]\tLoss: 3.829882\n",
      "Train Epoch: 1 [5550080/5599780 (99%)]\tLoss: 3.792801\n",
      "Train Epoch: 1 [5555200/5599780 (99%)]\tLoss: 3.837723\n",
      "Train Epoch: 1 [5560320/5599780 (99%)]\tLoss: 3.834171\n",
      "Train Epoch: 1 [5565440/5599780 (99%)]\tLoss: 3.833816\n",
      "Train Epoch: 1 [5570560/5599780 (99%)]\tLoss: 3.833568\n",
      "Train Epoch: 1 [5575680/5599780 (100%)]\tLoss: 3.839623\n",
      "Train Epoch: 1 [5580800/5599780 (100%)]\tLoss: 3.845903\n",
      "Train Epoch: 1 [5585920/5599780 (100%)]\tLoss: 3.810379\n",
      "Train Epoch: 1 [5591040/5599780 (100%)]\tLoss: 3.847488\n",
      "Train Epoch: 1 [5596160/5599780 (100%)]\tLoss: 3.800596\n",
      "\n",
      "Test set: Average loss: 3.8280, Accuracy: 303653/849497 (36%)\n",
      "\n",
      "Registering model baseline_model\n",
      "Train Epoch: 2 [0/5599780 (0%)]\tLoss: 3.788888\n",
      "Train Epoch: 2 [5120/5599780 (0%)]\tLoss: 3.855186\n",
      "Train Epoch: 2 [10240/5599780 (0%)]\tLoss: 3.783035\n",
      "Train Epoch: 2 [15360/5599780 (0%)]\tLoss: 3.790882\n",
      "Train Epoch: 2 [20480/5599780 (0%)]\tLoss: 3.808347\n",
      "Train Epoch: 2 [25600/5599780 (0%)]\tLoss: 3.850767\n",
      "Train Epoch: 2 [30720/5599780 (1%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [35840/5599780 (1%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [40960/5599780 (1%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [46080/5599780 (1%)]\tLoss: 3.874642\n",
      "Train Epoch: 2 [51200/5599780 (1%)]\tLoss: 3.849442\n",
      "Train Epoch: 2 [56320/5599780 (1%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [61440/5599780 (1%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [66560/5599780 (1%)]\tLoss: 3.847471\n",
      "Train Epoch: 2 [71680/5599780 (1%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [76800/5599780 (1%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [81920/5599780 (1%)]\tLoss: 3.820138\n",
      "Train Epoch: 2 [87040/5599780 (2%)]\tLoss: 3.844055\n",
      "Train Epoch: 2 [92160/5599780 (2%)]\tLoss: 3.851393\n",
      "Train Epoch: 2 [97280/5599780 (2%)]\tLoss: 3.851287\n",
      "Train Epoch: 2 [102400/5599780 (2%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [107520/5599780 (2%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [112640/5599780 (2%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [117760/5599780 (2%)]\tLoss: 3.849392\n",
      "Train Epoch: 2 [122880/5599780 (2%)]\tLoss: 3.816103\n",
      "Train Epoch: 2 [128000/5599780 (2%)]\tLoss: 3.853296\n",
      "Train Epoch: 2 [133120/5599780 (2%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [138240/5599780 (2%)]\tLoss: 3.792801\n",
      "Train Epoch: 2 [143360/5599780 (3%)]\tLoss: 3.829848\n",
      "Train Epoch: 2 [148480/5599780 (3%)]\tLoss: 3.839549\n",
      "Train Epoch: 2 [153600/5599780 (3%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [158720/5599780 (3%)]\tLoss: 3.839666\n",
      "Train Epoch: 2 [163840/5599780 (3%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [168960/5599780 (3%)]\tLoss: 3.810376\n",
      "Train Epoch: 2 [174080/5599780 (3%)]\tLoss: 3.843581\n",
      "Train Epoch: 2 [179200/5599780 (3%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [184320/5599780 (3%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [189440/5599780 (3%)]\tLoss: 3.786915\n",
      "Train Epoch: 2 [194560/5599780 (3%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [199680/5599780 (4%)]\tLoss: 3.841640\n",
      "Train Epoch: 2 [204800/5599780 (4%)]\tLoss: 3.822078\n",
      "Train Epoch: 2 [209920/5599780 (4%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [215040/5599780 (4%)]\tLoss: 3.782931\n",
      "Train Epoch: 2 [220160/5599780 (4%)]\tLoss: 3.831806\n",
      "Train Epoch: 2 [225280/5599780 (4%)]\tLoss: 3.827921\n",
      "Train Epoch: 2 [230400/5599780 (4%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [235520/5599780 (4%)]\tLoss: 3.841628\n",
      "Train Epoch: 2 [240640/5599780 (4%)]\tLoss: 3.851432\n",
      "Train Epoch: 2 [245760/5599780 (4%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [250880/5599780 (4%)]\tLoss: 3.829760\n",
      "Train Epoch: 2 [256000/5599780 (5%)]\tLoss: 3.827955\n",
      "Train Epoch: 2 [261120/5599780 (5%)]\tLoss: 3.854258\n",
      "Train Epoch: 2 [266240/5599780 (5%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [271360/5599780 (5%)]\tLoss: 3.837672\n",
      "Train Epoch: 2 [276480/5599780 (5%)]\tLoss: 3.849332\n",
      "Train Epoch: 2 [281600/5599780 (5%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [286720/5599780 (5%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [291840/5599780 (5%)]\tLoss: 3.859065\n",
      "Train Epoch: 2 [296960/5599780 (5%)]\tLoss: 3.825980\n",
      "Train Epoch: 2 [302080/5599780 (5%)]\tLoss: 3.835713\n",
      "Train Epoch: 2 [307200/5599780 (5%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [312320/5599780 (6%)]\tLoss: 3.790934\n",
      "Train Epoch: 2 [317440/5599780 (6%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [322560/5599780 (6%)]\tLoss: 3.835618\n",
      "Train Epoch: 2 [327680/5599780 (6%)]\tLoss: 3.784988\n",
      "Train Epoch: 2 [332800/5599780 (6%)]\tLoss: 3.837709\n",
      "Train Epoch: 2 [337920/5599780 (6%)]\tLoss: 3.804416\n",
      "Train Epoch: 2 [343040/5599780 (6%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [348160/5599780 (6%)]\tLoss: 3.849442\n",
      "Train Epoch: 2 [353280/5599780 (6%)]\tLoss: 3.822081\n",
      "Train Epoch: 2 [358400/5599780 (6%)]\tLoss: 3.811703\n",
      "Train Epoch: 2 [363520/5599780 (6%)]\tLoss: 3.843581\n",
      "Train Epoch: 2 [368640/5599780 (7%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [373760/5599780 (7%)]\tLoss: 3.779082\n",
      "Train Epoch: 2 [378880/5599780 (7%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [384000/5599780 (7%)]\tLoss: 3.810338\n",
      "Train Epoch: 2 [389120/5599780 (7%)]\tLoss: 3.825999\n",
      "Train Epoch: 2 [394240/5599780 (7%)]\tLoss: 3.845533\n",
      "Train Epoch: 2 [399360/5599780 (7%)]\tLoss: 3.839607\n",
      "Train Epoch: 2 [404480/5599780 (7%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [409600/5599780 (7%)]\tLoss: 3.829905\n",
      "Train Epoch: 2 [414720/5599780 (7%)]\tLoss: 3.794754\n",
      "Train Epoch: 2 [419840/5599780 (7%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [424960/5599780 (8%)]\tLoss: 3.831062\n",
      "Train Epoch: 2 [430080/5599780 (8%)]\tLoss: 3.786927\n",
      "Train Epoch: 2 [435200/5599780 (8%)]\tLoss: 3.820174\n",
      "Train Epoch: 2 [440320/5599780 (8%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [445440/5599780 (8%)]\tLoss: 3.826474\n",
      "Train Epoch: 2 [450560/5599780 (8%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [455680/5599780 (8%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [460800/5599780 (8%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [465920/5599780 (8%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [471040/5599780 (8%)]\tLoss: 3.816240\n",
      "Train Epoch: 2 [476160/5599780 (9%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [481280/5599780 (9%)]\tLoss: 3.878682\n",
      "Train Epoch: 2 [486400/5599780 (9%)]\tLoss: 3.808430\n",
      "Train Epoch: 2 [491520/5599780 (9%)]\tLoss: 3.829911\n",
      "Train Epoch: 2 [496640/5599780 (9%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [501760/5599780 (9%)]\tLoss: 3.818156\n",
      "Train Epoch: 2 [506880/5599780 (9%)]\tLoss: 3.863117\n",
      "Train Epoch: 2 [512000/5599780 (9%)]\tLoss: 3.829845\n",
      "Train Epoch: 2 [517120/5599780 (9%)]\tLoss: 3.818953\n",
      "Train Epoch: 2 [522240/5599780 (9%)]\tLoss: 3.773269\n",
      "Train Epoch: 2 [527360/5599780 (9%)]\tLoss: 3.814280\n",
      "Train Epoch: 2 [532480/5599780 (10%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [537600/5599780 (10%)]\tLoss: 3.841641\n",
      "Train Epoch: 2 [542720/5599780 (10%)]\tLoss: 3.855179\n",
      "Train Epoch: 2 [547840/5599780 (10%)]\tLoss: 3.800613\n",
      "Train Epoch: 2 [552960/5599780 (10%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [558080/5599780 (10%)]\tLoss: 3.820142\n",
      "Train Epoch: 2 [563200/5599780 (10%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [568320/5599780 (10%)]\tLoss: 3.839605\n",
      "Train Epoch: 2 [573440/5599780 (10%)]\tLoss: 3.876783\n",
      "Train Epoch: 2 [578560/5599780 (10%)]\tLoss: 3.824048\n",
      "Train Epoch: 2 [583680/5599780 (10%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [588800/5599780 (11%)]\tLoss: 3.821555\n",
      "Train Epoch: 2 [593920/5599780 (11%)]\tLoss: 3.795055\n",
      "Train Epoch: 2 [599040/5599780 (11%)]\tLoss: 3.837701\n",
      "Train Epoch: 2 [604160/5599780 (11%)]\tLoss: 3.860946\n",
      "Train Epoch: 2 [609280/5599780 (11%)]\tLoss: 3.828168\n",
      "Train Epoch: 2 [614400/5599780 (11%)]\tLoss: 3.835759\n",
      "Train Epoch: 2 [619520/5599780 (11%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [624640/5599780 (11%)]\tLoss: 3.853344\n",
      "Train Epoch: 2 [629760/5599780 (11%)]\tLoss: 3.810268\n",
      "Train Epoch: 2 [634880/5599780 (11%)]\tLoss: 3.808422\n",
      "Train Epoch: 2 [640000/5599780 (11%)]\tLoss: 3.826003\n",
      "Train Epoch: 2 [645120/5599780 (12%)]\tLoss: 3.831822\n",
      "Train Epoch: 2 [650240/5599780 (12%)]\tLoss: 3.821512\n",
      "Train Epoch: 2 [655360/5599780 (12%)]\tLoss: 3.812173\n",
      "Train Epoch: 2 [660480/5599780 (12%)]\tLoss: 3.841629\n",
      "Train Epoch: 2 [665600/5599780 (12%)]\tLoss: 3.865172\n",
      "Train Epoch: 2 [670720/5599780 (12%)]\tLoss: 3.849307\n",
      "Train Epoch: 2 [675840/5599780 (12%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [680960/5599780 (12%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [686080/5599780 (12%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [691200/5599780 (12%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [696320/5599780 (12%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [701440/5599780 (13%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [706560/5599780 (13%)]\tLoss: 3.824070\n",
      "Train Epoch: 2 [711680/5599780 (13%)]\tLoss: 3.808425\n",
      "Train Epoch: 2 [716800/5599780 (13%)]\tLoss: 3.837716\n",
      "Train Epoch: 2 [721920/5599780 (13%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [727040/5599780 (13%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [732160/5599780 (13%)]\tLoss: 3.839674\n",
      "Train Epoch: 2 [737280/5599780 (13%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [742400/5599780 (13%)]\tLoss: 3.826005\n",
      "Train Epoch: 2 [747520/5599780 (13%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [752640/5599780 (13%)]\tLoss: 3.775187\n",
      "Train Epoch: 2 [757760/5599780 (14%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [762880/5599780 (14%)]\tLoss: 3.837707\n",
      "Train Epoch: 2 [768000/5599780 (14%)]\tLoss: 3.819956\n",
      "Train Epoch: 2 [773120/5599780 (14%)]\tLoss: 3.808429\n",
      "Train Epoch: 2 [778240/5599780 (14%)]\tLoss: 3.857257\n",
      "Train Epoch: 2 [783360/5599780 (14%)]\tLoss: 3.839581\n",
      "Train Epoch: 2 [788480/5599780 (14%)]\tLoss: 3.824020\n",
      "Train Epoch: 2 [793600/5599780 (14%)]\tLoss: 3.839675\n",
      "Train Epoch: 2 [798720/5599780 (14%)]\tLoss: 3.801589\n",
      "Train Epoch: 2 [803840/5599780 (14%)]\tLoss: 3.827823\n",
      "Train Epoch: 2 [808960/5599780 (14%)]\tLoss: 3.841629\n",
      "Train Epoch: 2 [814080/5599780 (15%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [819200/5599780 (15%)]\tLoss: 3.839680\n",
      "Train Epoch: 2 [824320/5599780 (15%)]\tLoss: 3.804506\n",
      "Train Epoch: 2 [829440/5599780 (15%)]\tLoss: 3.836548\n",
      "Train Epoch: 2 [834560/5599780 (15%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [839680/5599780 (15%)]\tLoss: 3.824062\n",
      "Train Epoch: 2 [844800/5599780 (15%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [849920/5599780 (15%)]\tLoss: 3.816010\n",
      "Train Epoch: 2 [855040/5599780 (15%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [860160/5599780 (15%)]\tLoss: 3.785196\n",
      "Train Epoch: 2 [865280/5599780 (15%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [870400/5599780 (16%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [875520/5599780 (16%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [880640/5599780 (16%)]\tLoss: 3.851618\n",
      "Train Epoch: 2 [885760/5599780 (16%)]\tLoss: 3.845534\n",
      "Train Epoch: 2 [890880/5599780 (16%)]\tLoss: 3.837722\n",
      "Train Epoch: 2 [896000/5599780 (16%)]\tLoss: 3.783035\n",
      "Train Epoch: 2 [901120/5599780 (16%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [906240/5599780 (16%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [911360/5599780 (16%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [916480/5599780 (16%)]\tLoss: 3.841509\n",
      "Train Epoch: 2 [921600/5599780 (16%)]\tLoss: 3.804528\n",
      "Train Epoch: 2 [926720/5599780 (17%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [931840/5599780 (17%)]\tLoss: 3.788894\n",
      "Train Epoch: 2 [936960/5599780 (17%)]\tLoss: 3.822099\n",
      "Train Epoch: 2 [942080/5599780 (17%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [947200/5599780 (17%)]\tLoss: 3.855304\n",
      "Train Epoch: 2 [952320/5599780 (17%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [957440/5599780 (17%)]\tLoss: 3.806327\n",
      "Train Epoch: 2 [962560/5599780 (17%)]\tLoss: 3.829891\n",
      "Train Epoch: 2 [967680/5599780 (17%)]\tLoss: 3.839564\n",
      "Train Epoch: 2 [972800/5599780 (17%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [977920/5599780 (17%)]\tLoss: 3.827933\n",
      "Train Epoch: 2 [983040/5599780 (18%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [988160/5599780 (18%)]\tLoss: 3.869037\n",
      "Train Epoch: 2 [993280/5599780 (18%)]\tLoss: 3.839692\n",
      "Train Epoch: 2 [998400/5599780 (18%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [1003520/5599780 (18%)]\tLoss: 3.812256\n",
      "Train Epoch: 2 [1008640/5599780 (18%)]\tLoss: 3.853153\n",
      "Train Epoch: 2 [1013760/5599780 (18%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [1018880/5599780 (18%)]\tLoss: 3.865066\n",
      "Train Epoch: 2 [1024000/5599780 (18%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [1029120/5599780 (18%)]\tLoss: 3.827904\n",
      "Train Epoch: 2 [1034240/5599780 (18%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [1039360/5599780 (19%)]\tLoss: 3.837818\n",
      "Train Epoch: 2 [1044480/5599780 (19%)]\tLoss: 3.847486\n",
      "Train Epoch: 2 [1049600/5599780 (19%)]\tLoss: 3.810243\n",
      "Train Epoch: 2 [1054720/5599780 (19%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [1059840/5599780 (19%)]\tLoss: 3.826006\n",
      "Train Epoch: 2 [1064960/5599780 (19%)]\tLoss: 3.835848\n",
      "Train Epoch: 2 [1070080/5599780 (19%)]\tLoss: 3.789314\n",
      "Train Epoch: 2 [1075200/5599780 (19%)]\tLoss: 3.822093\n",
      "Train Epoch: 2 [1080320/5599780 (19%)]\tLoss: 3.845539\n",
      "Train Epoch: 2 [1085440/5599780 (19%)]\tLoss: 3.783018\n",
      "Train Epoch: 2 [1090560/5599780 (19%)]\tLoss: 3.859231\n",
      "Train Epoch: 2 [1095680/5599780 (20%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [1100800/5599780 (20%)]\tLoss: 3.865061\n",
      "Train Epoch: 2 [1105920/5599780 (20%)]\tLoss: 3.841467\n",
      "Train Epoch: 2 [1111040/5599780 (20%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [1116160/5599780 (20%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [1121280/5599780 (20%)]\tLoss: 3.867088\n",
      "Train Epoch: 2 [1126400/5599780 (20%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [1131520/5599780 (20%)]\tLoss: 3.798660\n",
      "Train Epoch: 2 [1136640/5599780 (20%)]\tLoss: 3.839793\n",
      "Train Epoch: 2 [1141760/5599780 (20%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [1146880/5599780 (20%)]\tLoss: 3.816263\n",
      "Train Epoch: 2 [1152000/5599780 (21%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1157120/5599780 (21%)]\tLoss: 3.886796\n",
      "Train Epoch: 2 [1162240/5599780 (21%)]\tLoss: 3.847582\n",
      "Train Epoch: 2 [1167360/5599780 (21%)]\tLoss: 3.831857\n",
      "Train Epoch: 2 [1172480/5599780 (21%)]\tLoss: 3.824057\n",
      "Train Epoch: 2 [1177600/5599780 (21%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [1182720/5599780 (21%)]\tLoss: 3.841629\n",
      "Train Epoch: 2 [1187840/5599780 (21%)]\tLoss: 3.796703\n",
      "Train Epoch: 2 [1192960/5599780 (21%)]\tLoss: 3.845536\n",
      "Train Epoch: 2 [1198080/5599780 (21%)]\tLoss: 3.825860\n",
      "Train Epoch: 2 [1203200/5599780 (21%)]\tLoss: 3.783035\n",
      "Train Epoch: 2 [1208320/5599780 (22%)]\tLoss: 3.822097\n",
      "Train Epoch: 2 [1213440/5599780 (22%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [1218560/5599780 (22%)]\tLoss: 3.812266\n",
      "Train Epoch: 2 [1223680/5599780 (22%)]\tLoss: 3.851083\n",
      "Train Epoch: 2 [1228800/5599780 (22%)]\tLoss: 3.861251\n",
      "Train Epoch: 2 [1233920/5599780 (22%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [1239040/5599780 (22%)]\tLoss: 3.867697\n",
      "Train Epoch: 2 [1244160/5599780 (22%)]\tLoss: 3.825992\n",
      "Train Epoch: 2 [1249280/5599780 (22%)]\tLoss: 3.840824\n",
      "Train Epoch: 2 [1254400/5599780 (22%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [1259520/5599780 (22%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [1264640/5599780 (23%)]\tLoss: 3.820908\n",
      "Train Epoch: 2 [1269760/5599780 (23%)]\tLoss: 3.794754\n",
      "Train Epoch: 2 [1274880/5599780 (23%)]\tLoss: 3.829729\n",
      "Train Epoch: 2 [1280000/5599780 (23%)]\tLoss: 3.831801\n",
      "Train Epoch: 2 [1285120/5599780 (23%)]\tLoss: 3.815938\n",
      "Train Epoch: 2 [1290240/5599780 (23%)]\tLoss: 3.857253\n",
      "Train Epoch: 2 [1295360/5599780 (23%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [1300480/5599780 (23%)]\tLoss: 3.812327\n",
      "Train Epoch: 2 [1305600/5599780 (23%)]\tLoss: 3.835768\n",
      "Train Epoch: 2 [1310720/5599780 (23%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [1315840/5599780 (23%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [1320960/5599780 (24%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [1326080/5599780 (24%)]\tLoss: 3.874845\n",
      "Train Epoch: 2 [1331200/5599780 (24%)]\tLoss: 3.865066\n",
      "Train Epoch: 2 [1336320/5599780 (24%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [1341440/5599780 (24%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [1346560/5599780 (24%)]\tLoss: 3.833788\n",
      "Train Epoch: 2 [1351680/5599780 (24%)]\tLoss: 3.831898\n",
      "Train Epoch: 2 [1356800/5599780 (24%)]\tLoss: 3.869390\n",
      "Train Epoch: 2 [1361920/5599780 (24%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [1367040/5599780 (24%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [1372160/5599780 (25%)]\tLoss: 3.843614\n",
      "Train Epoch: 2 [1377280/5599780 (25%)]\tLoss: 3.806456\n",
      "Train Epoch: 2 [1382400/5599780 (25%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [1387520/5599780 (25%)]\tLoss: 3.868973\n",
      "Train Epoch: 2 [1392640/5599780 (25%)]\tLoss: 3.851923\n",
      "Train Epoch: 2 [1397760/5599780 (25%)]\tLoss: 3.841627\n",
      "Train Epoch: 2 [1402880/5599780 (25%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [1408000/5599780 (25%)]\tLoss: 3.799029\n",
      "Train Epoch: 2 [1413120/5599780 (25%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [1418240/5599780 (25%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [1423360/5599780 (25%)]\tLoss: 3.779122\n",
      "Train Epoch: 2 [1428480/5599780 (26%)]\tLoss: 3.845630\n",
      "Train Epoch: 2 [1433600/5599780 (26%)]\tLoss: 3.843564\n",
      "Train Epoch: 2 [1438720/5599780 (26%)]\tLoss: 3.827954\n",
      "Train Epoch: 2 [1443840/5599780 (26%)]\tLoss: 3.805676\n",
      "Train Epoch: 2 [1448960/5599780 (26%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [1454080/5599780 (26%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [1459200/5599780 (26%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [1464320/5599780 (26%)]\tLoss: 3.798660\n",
      "Train Epoch: 2 [1469440/5599780 (26%)]\tLoss: 3.839674\n",
      "Train Epoch: 2 [1474560/5599780 (26%)]\tLoss: 3.810601\n",
      "Train Epoch: 2 [1479680/5599780 (26%)]\tLoss: 3.845338\n",
      "Train Epoch: 2 [1484800/5599780 (27%)]\tLoss: 3.867019\n",
      "Train Epoch: 2 [1489920/5599780 (27%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1495040/5599780 (27%)]\tLoss: 3.790919\n",
      "Train Epoch: 2 [1500160/5599780 (27%)]\tLoss: 3.845534\n",
      "Train Epoch: 2 [1505280/5599780 (27%)]\tLoss: 3.839006\n",
      "Train Epoch: 2 [1510400/5599780 (27%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [1515520/5599780 (27%)]\tLoss: 3.825995\n",
      "Train Epoch: 2 [1520640/5599780 (27%)]\tLoss: 3.818144\n",
      "Train Epoch: 2 [1525760/5599780 (27%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [1530880/5599780 (27%)]\tLoss: 3.859198\n",
      "Train Epoch: 2 [1536000/5599780 (27%)]\tLoss: 3.839623\n",
      "Train Epoch: 2 [1541120/5599780 (28%)]\tLoss: 3.814305\n",
      "Train Epoch: 2 [1546240/5599780 (28%)]\tLoss: 3.796512\n",
      "Train Epoch: 2 [1551360/5599780 (28%)]\tLoss: 3.792645\n",
      "Train Epoch: 2 [1556480/5599780 (28%)]\tLoss: 3.837696\n",
      "Train Epoch: 2 [1561600/5599780 (28%)]\tLoss: 3.798660\n",
      "Train Epoch: 2 [1566720/5599780 (28%)]\tLoss: 3.827834\n",
      "Train Epoch: 2 [1571840/5599780 (28%)]\tLoss: 3.839675\n",
      "Train Epoch: 2 [1576960/5599780 (28%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [1582080/5599780 (28%)]\tLoss: 3.835635\n",
      "Train Epoch: 2 [1587200/5599780 (28%)]\tLoss: 3.835635\n",
      "Train Epoch: 2 [1592320/5599780 (28%)]\tLoss: 3.841629\n",
      "Train Epoch: 2 [1597440/5599780 (29%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [1602560/5599780 (29%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [1607680/5599780 (29%)]\tLoss: 3.822733\n",
      "Train Epoch: 2 [1612800/5599780 (29%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [1617920/5599780 (29%)]\tLoss: 3.814188\n",
      "Train Epoch: 2 [1623040/5599780 (29%)]\tLoss: 3.814165\n",
      "Train Epoch: 2 [1628160/5599780 (29%)]\tLoss: 3.820187\n",
      "Train Epoch: 2 [1633280/5599780 (29%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [1638400/5599780 (29%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [1643520/5599780 (29%)]\tLoss: 3.814153\n",
      "Train Epoch: 2 [1648640/5599780 (29%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [1653760/5599780 (30%)]\tLoss: 3.845586\n",
      "Train Epoch: 2 [1658880/5599780 (30%)]\tLoss: 3.823878\n",
      "Train Epoch: 2 [1664000/5599780 (30%)]\tLoss: 3.876793\n",
      "Train Epoch: 2 [1669120/5599780 (30%)]\tLoss: 3.792789\n",
      "Train Epoch: 2 [1674240/5599780 (30%)]\tLoss: 3.814284\n",
      "Train Epoch: 2 [1679360/5599780 (30%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [1684480/5599780 (30%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [1689600/5599780 (30%)]\tLoss: 3.826026\n",
      "Train Epoch: 2 [1694720/5599780 (30%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [1699840/5599780 (30%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [1704960/5599780 (30%)]\tLoss: 3.818171\n",
      "Train Epoch: 2 [1710080/5599780 (31%)]\tLoss: 3.841623\n",
      "Train Epoch: 2 [1715200/5599780 (31%)]\tLoss: 3.786873\n",
      "Train Epoch: 2 [1720320/5599780 (31%)]\tLoss: 3.855320\n",
      "Train Epoch: 2 [1725440/5599780 (31%)]\tLoss: 3.847284\n",
      "Train Epoch: 2 [1730560/5599780 (31%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [1735680/5599780 (31%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [1740800/5599780 (31%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [1745920/5599780 (31%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [1751040/5599780 (31%)]\tLoss: 3.794753\n",
      "Train Epoch: 2 [1756160/5599780 (31%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [1761280/5599780 (31%)]\tLoss: 3.796703\n",
      "Train Epoch: 2 [1766400/5599780 (32%)]\tLoss: 3.857110\n",
      "Train Epoch: 2 [1771520/5599780 (32%)]\tLoss: 3.835776\n",
      "Train Epoch: 2 [1776640/5599780 (32%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [1781760/5599780 (32%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [1786880/5599780 (32%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [1792000/5599780 (32%)]\tLoss: 3.788894\n",
      "Train Epoch: 2 [1797120/5599780 (32%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [1802240/5599780 (32%)]\tLoss: 3.831717\n",
      "Train Epoch: 2 [1807360/5599780 (32%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [1812480/5599780 (32%)]\tLoss: 3.859210\n",
      "Train Epoch: 2 [1817600/5599780 (32%)]\tLoss: 3.863113\n",
      "Train Epoch: 2 [1822720/5599780 (33%)]\tLoss: 3.849415\n",
      "Train Epoch: 2 [1827840/5599780 (33%)]\tLoss: 3.855294\n",
      "Train Epoch: 2 [1832960/5599780 (33%)]\tLoss: 3.792589\n",
      "Train Epoch: 2 [1838080/5599780 (33%)]\tLoss: 3.832815\n",
      "Train Epoch: 2 [1843200/5599780 (33%)]\tLoss: 3.790847\n",
      "Train Epoch: 2 [1848320/5599780 (33%)]\tLoss: 3.845532\n",
      "Train Epoch: 2 [1853440/5599780 (33%)]\tLoss: 3.859210\n",
      "Train Epoch: 2 [1858560/5599780 (33%)]\tLoss: 3.857250\n",
      "Train Epoch: 2 [1863680/5599780 (33%)]\tLoss: 3.870921\n",
      "Train Epoch: 2 [1868800/5599780 (33%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [1873920/5599780 (33%)]\tLoss: 3.829967\n",
      "Train Epoch: 2 [1879040/5599780 (34%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [1884160/5599780 (34%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [1889280/5599780 (34%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [1894400/5599780 (34%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [1899520/5599780 (34%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1904640/5599780 (34%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [1909760/5599780 (34%)]\tLoss: 3.892408\n",
      "Train Epoch: 2 [1914880/5599780 (34%)]\tLoss: 3.826013\n",
      "Train Epoch: 2 [1920000/5599780 (34%)]\tLoss: 3.835788\n",
      "Train Epoch: 2 [1925120/5599780 (34%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [1930240/5599780 (34%)]\tLoss: 3.841409\n",
      "Train Epoch: 2 [1935360/5599780 (35%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1940480/5599780 (35%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1945600/5599780 (35%)]\tLoss: 3.864879\n",
      "Train Epoch: 2 [1950720/5599780 (35%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [1955840/5599780 (35%)]\tLoss: 3.820007\n",
      "Train Epoch: 2 [1960960/5599780 (35%)]\tLoss: 3.837677\n",
      "Train Epoch: 2 [1966080/5599780 (35%)]\tLoss: 3.792600\n",
      "Train Epoch: 2 [1971200/5599780 (35%)]\tLoss: 3.835736\n",
      "Train Epoch: 2 [1976320/5599780 (35%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [1981440/5599780 (35%)]\tLoss: 3.812339\n",
      "Train Epoch: 2 [1986560/5599780 (35%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [1991680/5599780 (36%)]\tLoss: 3.832832\n",
      "Train Epoch: 2 [1996800/5599780 (36%)]\tLoss: 3.829865\n",
      "Train Epoch: 2 [2001920/5599780 (36%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [2007040/5599780 (36%)]\tLoss: 3.847336\n",
      "Train Epoch: 2 [2012160/5599780 (36%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [2017280/5599780 (36%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [2022400/5599780 (36%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [2027520/5599780 (36%)]\tLoss: 3.846582\n",
      "Train Epoch: 2 [2032640/5599780 (36%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [2037760/5599780 (36%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [2042880/5599780 (36%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [2048000/5599780 (37%)]\tLoss: 3.859150\n",
      "Train Epoch: 2 [2053120/5599780 (37%)]\tLoss: 3.831866\n",
      "Train Epoch: 2 [2058240/5599780 (37%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [2063360/5599780 (37%)]\tLoss: 3.818196\n",
      "Train Epoch: 2 [2068480/5599780 (37%)]\tLoss: 3.882805\n",
      "Train Epoch: 2 [2073600/5599780 (37%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [2078720/5599780 (37%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [2083840/5599780 (37%)]\tLoss: 3.872886\n",
      "Train Epoch: 2 [2088960/5599780 (37%)]\tLoss: 3.827940\n",
      "Train Epoch: 2 [2094080/5599780 (37%)]\tLoss: 3.837495\n",
      "Train Epoch: 2 [2099200/5599780 (37%)]\tLoss: 3.829738\n",
      "Train Epoch: 2 [2104320/5599780 (38%)]\tLoss: 3.826013\n",
      "Train Epoch: 2 [2109440/5599780 (38%)]\tLoss: 3.845532\n",
      "Train Epoch: 2 [2114560/5599780 (38%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [2119680/5599780 (38%)]\tLoss: 3.827763\n",
      "Train Epoch: 2 [2124800/5599780 (38%)]\tLoss: 3.804514\n",
      "Train Epoch: 2 [2129920/5599780 (38%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [2135040/5599780 (38%)]\tLoss: 3.829838\n",
      "Train Epoch: 2 [2140160/5599780 (38%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [2145280/5599780 (38%)]\tLoss: 3.870926\n",
      "Train Epoch: 2 [2150400/5599780 (38%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [2155520/5599780 (38%)]\tLoss: 3.839465\n",
      "Train Epoch: 2 [2160640/5599780 (39%)]\tLoss: 3.832639\n",
      "Train Epoch: 2 [2165760/5599780 (39%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [2170880/5599780 (39%)]\tLoss: 3.847473\n",
      "Train Epoch: 2 [2176000/5599780 (39%)]\tLoss: 3.847439\n",
      "Train Epoch: 2 [2181120/5599780 (39%)]\tLoss: 3.822099\n",
      "Train Epoch: 2 [2186240/5599780 (39%)]\tLoss: 3.851393\n",
      "Train Epoch: 2 [2191360/5599780 (39%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [2196480/5599780 (39%)]\tLoss: 3.829909\n",
      "Train Epoch: 2 [2201600/5599780 (39%)]\tLoss: 3.890195\n",
      "Train Epoch: 2 [2206720/5599780 (39%)]\tLoss: 3.837500\n",
      "Train Epoch: 2 [2211840/5599780 (39%)]\tLoss: 3.792800\n",
      "Train Epoch: 2 [2216960/5599780 (40%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [2222080/5599780 (40%)]\tLoss: 3.833817\n",
      "Train Epoch: 2 [2227200/5599780 (40%)]\tLoss: 3.847485\n",
      "Train Epoch: 2 [2232320/5599780 (40%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [2237440/5599780 (40%)]\tLoss: 3.773325\n",
      "Train Epoch: 2 [2242560/5599780 (40%)]\tLoss: 3.804297\n",
      "Train Epoch: 2 [2247680/5599780 (40%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [2252800/5599780 (40%)]\tLoss: 3.793032\n",
      "Train Epoch: 2 [2257920/5599780 (40%)]\tLoss: 3.847399\n",
      "Train Epoch: 2 [2263040/5599780 (40%)]\tLoss: 3.839675\n",
      "Train Epoch: 2 [2268160/5599780 (41%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [2273280/5599780 (41%)]\tLoss: 3.810237\n",
      "Train Epoch: 2 [2278400/5599780 (41%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [2283520/5599780 (41%)]\tLoss: 3.857253\n",
      "Train Epoch: 2 [2288640/5599780 (41%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [2293760/5599780 (41%)]\tLoss: 3.835792\n",
      "Train Epoch: 2 [2298880/5599780 (41%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [2304000/5599780 (41%)]\tLoss: 3.794754\n",
      "Train Epoch: 2 [2309120/5599780 (41%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [2314240/5599780 (41%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [2319360/5599780 (41%)]\tLoss: 3.890527\n",
      "Train Epoch: 2 [2324480/5599780 (42%)]\tLoss: 3.831999\n",
      "Train Epoch: 2 [2329600/5599780 (42%)]\tLoss: 3.845533\n",
      "Train Epoch: 2 [2334720/5599780 (42%)]\tLoss: 3.820128\n",
      "Train Epoch: 2 [2339840/5599780 (42%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [2344960/5599780 (42%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2350080/5599780 (42%)]\tLoss: 3.843580\n",
      "Train Epoch: 2 [2355200/5599780 (42%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [2360320/5599780 (42%)]\tLoss: 3.831857\n",
      "Train Epoch: 2 [2365440/5599780 (42%)]\tLoss: 3.825694\n",
      "Train Epoch: 2 [2370560/5599780 (42%)]\tLoss: 3.822097\n",
      "Train Epoch: 2 [2375680/5599780 (42%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [2380800/5599780 (43%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [2385920/5599780 (43%)]\tLoss: 3.807065\n",
      "Train Epoch: 2 [2391040/5599780 (43%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [2396160/5599780 (43%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [2401280/5599780 (43%)]\tLoss: 3.812157\n",
      "Train Epoch: 2 [2406400/5599780 (43%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [2411520/5599780 (43%)]\tLoss: 3.837398\n",
      "Train Epoch: 2 [2416640/5599780 (43%)]\tLoss: 3.859206\n",
      "Train Epoch: 2 [2421760/5599780 (43%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [2426880/5599780 (43%)]\tLoss: 3.835781\n",
      "Train Epoch: 2 [2432000/5599780 (43%)]\tLoss: 3.830276\n",
      "Train Epoch: 2 [2437120/5599780 (44%)]\tLoss: 3.815733\n",
      "Train Epoch: 2 [2442240/5599780 (44%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [2447360/5599780 (44%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [2452480/5599780 (44%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [2457600/5599780 (44%)]\tLoss: 3.861175\n",
      "Train Epoch: 2 [2462720/5599780 (44%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [2467840/5599780 (44%)]\tLoss: 3.865068\n",
      "Train Epoch: 2 [2472960/5599780 (44%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [2478080/5599780 (44%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [2483200/5599780 (44%)]\tLoss: 3.786942\n",
      "Train Epoch: 2 [2488320/5599780 (44%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [2493440/5599780 (45%)]\tLoss: 3.851048\n",
      "Train Epoch: 2 [2498560/5599780 (45%)]\tLoss: 3.822107\n",
      "Train Epoch: 2 [2503680/5599780 (45%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [2508800/5599780 (45%)]\tLoss: 3.872879\n",
      "Train Epoch: 2 [2513920/5599780 (45%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [2519040/5599780 (45%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [2524160/5599780 (45%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [2529280/5599780 (45%)]\tLoss: 3.781082\n",
      "Train Epoch: 2 [2534400/5599780 (45%)]\tLoss: 3.849228\n",
      "Train Epoch: 2 [2539520/5599780 (45%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [2544640/5599780 (45%)]\tLoss: 3.823301\n",
      "Train Epoch: 2 [2549760/5599780 (46%)]\tLoss: 3.823530\n",
      "Train Epoch: 2 [2554880/5599780 (46%)]\tLoss: 3.853333\n",
      "Train Epoch: 2 [2560000/5599780 (46%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2565120/5599780 (46%)]\tLoss: 3.867408\n",
      "Train Epoch: 2 [2570240/5599780 (46%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [2575360/5599780 (46%)]\tLoss: 3.819919\n",
      "Train Epoch: 2 [2580480/5599780 (46%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [2585600/5599780 (46%)]\tLoss: 3.853358\n",
      "Train Epoch: 2 [2590720/5599780 (46%)]\tLoss: 3.886528\n",
      "Train Epoch: 2 [2595840/5599780 (46%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [2600960/5599780 (46%)]\tLoss: 3.808715\n",
      "Train Epoch: 2 [2606080/5599780 (47%)]\tLoss: 3.851163\n",
      "Train Epoch: 2 [2611200/5599780 (47%)]\tLoss: 3.812331\n",
      "Train Epoch: 2 [2616320/5599780 (47%)]\tLoss: 3.874832\n",
      "Train Epoch: 2 [2621440/5599780 (47%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [2626560/5599780 (47%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [2631680/5599780 (47%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2636800/5599780 (47%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2641920/5599780 (47%)]\tLoss: 3.784988\n",
      "Train Epoch: 2 [2647040/5599780 (47%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [2652160/5599780 (47%)]\tLoss: 3.847468\n",
      "Train Epoch: 2 [2657280/5599780 (47%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [2662400/5599780 (48%)]\tLoss: 3.867016\n",
      "Train Epoch: 2 [2667520/5599780 (48%)]\tLoss: 3.827973\n",
      "Train Epoch: 2 [2672640/5599780 (48%)]\tLoss: 3.867019\n",
      "Train Epoch: 2 [2677760/5599780 (48%)]\tLoss: 3.868973\n",
      "Train Epoch: 2 [2682880/5599780 (48%)]\tLoss: 3.808423\n",
      "Train Epoch: 2 [2688000/5599780 (48%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [2693120/5599780 (48%)]\tLoss: 3.880704\n",
      "Train Epoch: 2 [2698240/5599780 (48%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [2703360/5599780 (48%)]\tLoss: 3.794866\n",
      "Train Epoch: 2 [2708480/5599780 (48%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [2713600/5599780 (48%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [2718720/5599780 (49%)]\tLoss: 3.800565\n",
      "Train Epoch: 2 [2723840/5599780 (49%)]\tLoss: 3.812329\n",
      "Train Epoch: 2 [2728960/5599780 (49%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [2734080/5599780 (49%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [2739200/5599780 (49%)]\tLoss: 3.859139\n",
      "Train Epoch: 2 [2744320/5599780 (49%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [2749440/5599780 (49%)]\tLoss: 3.804520\n",
      "Train Epoch: 2 [2754560/5599780 (49%)]\tLoss: 3.818178\n",
      "Train Epoch: 2 [2759680/5599780 (49%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [2764800/5599780 (49%)]\tLoss: 3.863113\n",
      "Train Epoch: 2 [2769920/5599780 (49%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [2775040/5599780 (50%)]\tLoss: 3.825778\n",
      "Train Epoch: 2 [2780160/5599780 (50%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [2785280/5599780 (50%)]\tLoss: 3.849472\n",
      "Train Epoch: 2 [2790400/5599780 (50%)]\tLoss: 3.837677\n",
      "Train Epoch: 2 [2795520/5599780 (50%)]\tLoss: 3.806468\n",
      "Train Epoch: 2 [2800640/5599780 (50%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [2805760/5599780 (50%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [2810880/5599780 (50%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [2816000/5599780 (50%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [2821120/5599780 (50%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2826240/5599780 (50%)]\tLoss: 3.847485\n",
      "Train Epoch: 2 [2831360/5599780 (51%)]\tLoss: 3.872799\n",
      "Train Epoch: 2 [2836480/5599780 (51%)]\tLoss: 3.824050\n",
      "Train Epoch: 2 [2841600/5599780 (51%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [2846720/5599780 (51%)]\tLoss: 3.829804\n",
      "Train Epoch: 2 [2851840/5599780 (51%)]\tLoss: 3.843567\n",
      "Train Epoch: 2 [2856960/5599780 (51%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [2862080/5599780 (51%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [2867200/5599780 (51%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [2872320/5599780 (51%)]\tLoss: 3.880676\n",
      "Train Epoch: 2 [2877440/5599780 (51%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [2882560/5599780 (51%)]\tLoss: 3.825780\n",
      "Train Epoch: 2 [2887680/5599780 (52%)]\tLoss: 3.867019\n",
      "Train Epoch: 2 [2892800/5599780 (52%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [2897920/5599780 (52%)]\tLoss: 3.810354\n",
      "Train Epoch: 2 [2903040/5599780 (52%)]\tLoss: 3.808432\n",
      "Train Epoch: 2 [2908160/5599780 (52%)]\tLoss: 3.822132\n",
      "Train Epoch: 2 [2913280/5599780 (52%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [2918400/5599780 (52%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [2923520/5599780 (52%)]\tLoss: 3.865066\n",
      "Train Epoch: 2 [2928640/5599780 (52%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [2933760/5599780 (52%)]\tLoss: 3.892410\n",
      "Train Epoch: 2 [2938880/5599780 (52%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [2944000/5599780 (53%)]\tLoss: 3.800785\n",
      "Train Epoch: 2 [2949120/5599780 (53%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [2954240/5599780 (53%)]\tLoss: 3.870856\n",
      "Train Epoch: 2 [2959360/5599780 (53%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [2964480/5599780 (53%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [2969600/5599780 (53%)]\tLoss: 3.814274\n",
      "Train Epoch: 2 [2974720/5599780 (53%)]\tLoss: 3.855307\n",
      "Train Epoch: 2 [2979840/5599780 (53%)]\tLoss: 3.872879\n",
      "Train Epoch: 2 [2984960/5599780 (53%)]\tLoss: 3.814265\n",
      "Train Epoch: 2 [2990080/5599780 (53%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [2995200/5599780 (53%)]\tLoss: 3.813999\n",
      "Train Epoch: 2 [3000320/5599780 (54%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [3005440/5599780 (54%)]\tLoss: 3.792801\n",
      "Train Epoch: 2 [3010560/5599780 (54%)]\tLoss: 3.801084\n",
      "Train Epoch: 2 [3015680/5599780 (54%)]\tLoss: 3.863111\n",
      "Train Epoch: 2 [3020800/5599780 (54%)]\tLoss: 3.841616\n",
      "Train Epoch: 2 [3025920/5599780 (54%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3031040/5599780 (54%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [3036160/5599780 (54%)]\tLoss: 3.835522\n",
      "Train Epoch: 2 [3041280/5599780 (54%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3046400/5599780 (54%)]\tLoss: 3.824074\n",
      "Train Epoch: 2 [3051520/5599780 (54%)]\tLoss: 3.821863\n",
      "Train Epoch: 2 [3056640/5599780 (55%)]\tLoss: 3.827939\n",
      "Train Epoch: 2 [3061760/5599780 (55%)]\tLoss: 3.790650\n",
      "Train Epoch: 2 [3066880/5599780 (55%)]\tLoss: 3.831852\n",
      "Train Epoch: 2 [3072000/5599780 (55%)]\tLoss: 3.837724\n",
      "Train Epoch: 2 [3077120/5599780 (55%)]\tLoss: 3.843575\n",
      "Train Epoch: 2 [3082240/5599780 (55%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [3087360/5599780 (55%)]\tLoss: 3.786940\n",
      "Train Epoch: 2 [3092480/5599780 (55%)]\tLoss: 3.861155\n",
      "Train Epoch: 2 [3097600/5599780 (55%)]\tLoss: 3.790848\n",
      "Train Epoch: 2 [3102720/5599780 (55%)]\tLoss: 3.843561\n",
      "Train Epoch: 2 [3107840/5599780 (55%)]\tLoss: 3.814252\n",
      "Train Epoch: 2 [3112960/5599780 (56%)]\tLoss: 3.841628\n",
      "Train Epoch: 2 [3118080/5599780 (56%)]\tLoss: 3.886550\n",
      "Train Epoch: 2 [3123200/5599780 (56%)]\tLoss: 3.823802\n",
      "Train Epoch: 2 [3128320/5599780 (56%)]\tLoss: 3.824046\n",
      "Train Epoch: 2 [3133440/5599780 (56%)]\tLoss: 3.863113\n",
      "Train Epoch: 2 [3138560/5599780 (56%)]\tLoss: 3.837700\n",
      "Train Epoch: 2 [3143680/5599780 (56%)]\tLoss: 3.840389\n",
      "Train Epoch: 2 [3148800/5599780 (56%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3153920/5599780 (56%)]\tLoss: 3.825919\n",
      "Train Epoch: 2 [3159040/5599780 (56%)]\tLoss: 3.777176\n",
      "Train Epoch: 2 [3164160/5599780 (57%)]\tLoss: 3.816236\n",
      "Train Epoch: 2 [3169280/5599780 (57%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3174400/5599780 (57%)]\tLoss: 3.815987\n",
      "Train Epoch: 2 [3179520/5599780 (57%)]\tLoss: 3.839489\n",
      "Train Epoch: 2 [3184640/5599780 (57%)]\tLoss: 3.831859\n",
      "Train Epoch: 2 [3189760/5599780 (57%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [3194880/5599780 (57%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [3200000/5599780 (57%)]\tLoss: 3.794754\n",
      "Train Epoch: 2 [3205120/5599780 (57%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [3210240/5599780 (57%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3215360/5599780 (57%)]\tLoss: 3.841624\n",
      "Train Epoch: 2 [3220480/5599780 (58%)]\tLoss: 3.825724\n",
      "Train Epoch: 2 [3225600/5599780 (58%)]\tLoss: 3.812567\n",
      "Train Epoch: 2 [3230720/5599780 (58%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [3235840/5599780 (58%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [3240960/5599780 (58%)]\tLoss: 3.826005\n",
      "Train Epoch: 2 [3246080/5599780 (58%)]\tLoss: 3.824072\n",
      "Train Epoch: 2 [3251200/5599780 (58%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [3256320/5599780 (58%)]\tLoss: 3.851374\n",
      "Train Epoch: 2 [3261440/5599780 (58%)]\tLoss: 3.865029\n",
      "Train Epoch: 2 [3266560/5599780 (58%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [3271680/5599780 (58%)]\tLoss: 3.804520\n",
      "Train Epoch: 2 [3276800/5599780 (59%)]\tLoss: 3.810429\n",
      "Train Epoch: 2 [3281920/5599780 (59%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [3287040/5599780 (59%)]\tLoss: 3.855840\n",
      "Train Epoch: 2 [3292160/5599780 (59%)]\tLoss: 3.841576\n",
      "Train Epoch: 2 [3297280/5599780 (59%)]\tLoss: 3.853347\n",
      "Train Epoch: 2 [3302400/5599780 (59%)]\tLoss: 3.818178\n",
      "Train Epoch: 2 [3307520/5599780 (59%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [3312640/5599780 (59%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [3317760/5599780 (59%)]\tLoss: 3.851393\n",
      "Train Epoch: 2 [3322880/5599780 (59%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [3328000/5599780 (59%)]\tLoss: 3.838839\n",
      "Train Epoch: 2 [3333120/5599780 (60%)]\tLoss: 3.841627\n",
      "Train Epoch: 2 [3338240/5599780 (60%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3343360/5599780 (60%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3348480/5599780 (60%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [3353600/5599780 (60%)]\tLoss: 3.779818\n",
      "Train Epoch: 2 [3358720/5599780 (60%)]\tLoss: 3.802505\n",
      "Train Epoch: 2 [3363840/5599780 (60%)]\tLoss: 3.786941\n",
      "Train Epoch: 2 [3368960/5599780 (60%)]\tLoss: 3.845541\n",
      "Train Epoch: 2 [3374080/5599780 (60%)]\tLoss: 3.837965\n",
      "Train Epoch: 2 [3379200/5599780 (60%)]\tLoss: 3.827895\n",
      "Train Epoch: 2 [3384320/5599780 (60%)]\tLoss: 3.857252\n",
      "Train Epoch: 2 [3389440/5599780 (61%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [3394560/5599780 (61%)]\tLoss: 3.822095\n",
      "Train Epoch: 2 [3399680/5599780 (61%)]\tLoss: 3.829911\n",
      "Train Epoch: 2 [3404800/5599780 (61%)]\tLoss: 3.831861\n",
      "Train Epoch: 2 [3409920/5599780 (61%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3415040/5599780 (61%)]\tLoss: 3.812369\n",
      "Train Epoch: 2 [3420160/5599780 (61%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [3425280/5599780 (61%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [3430400/5599780 (61%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [3435520/5599780 (61%)]\tLoss: 3.859189\n",
      "Train Epoch: 2 [3440640/5599780 (61%)]\tLoss: 3.796443\n",
      "Train Epoch: 2 [3445760/5599780 (62%)]\tLoss: 3.851413\n",
      "Train Epoch: 2 [3450880/5599780 (62%)]\tLoss: 3.798606\n",
      "Train Epoch: 2 [3456000/5599780 (62%)]\tLoss: 3.845534\n",
      "Train Epoch: 2 [3461120/5599780 (62%)]\tLoss: 3.868798\n",
      "Train Epoch: 2 [3466240/5599780 (62%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3471360/5599780 (62%)]\tLoss: 3.867018\n",
      "Train Epoch: 2 [3476480/5599780 (62%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3481600/5599780 (62%)]\tLoss: 3.820136\n",
      "Train Epoch: 2 [3486720/5599780 (62%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [3491840/5599780 (62%)]\tLoss: 3.841629\n",
      "Train Epoch: 2 [3496960/5599780 (62%)]\tLoss: 3.841554\n",
      "Train Epoch: 2 [3502080/5599780 (63%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [3507200/5599780 (63%)]\tLoss: 3.808413\n",
      "Train Epoch: 2 [3512320/5599780 (63%)]\tLoss: 3.818150\n",
      "Train Epoch: 2 [3517440/5599780 (63%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [3522560/5599780 (63%)]\tLoss: 3.847487\n",
      "Train Epoch: 2 [3527680/5599780 (63%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [3532800/5599780 (63%)]\tLoss: 3.865051\n",
      "Train Epoch: 2 [3537920/5599780 (63%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [3543040/5599780 (63%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [3548160/5599780 (63%)]\tLoss: 3.867018\n",
      "Train Epoch: 2 [3553280/5599780 (63%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [3558400/5599780 (64%)]\tLoss: 3.830098\n",
      "Train Epoch: 2 [3563520/5599780 (64%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [3568640/5599780 (64%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [3573760/5599780 (64%)]\tLoss: 3.867041\n",
      "Train Epoch: 2 [3578880/5599780 (64%)]\tLoss: 3.841466\n",
      "Train Epoch: 2 [3584000/5599780 (64%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3589120/5599780 (64%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [3594240/5599780 (64%)]\tLoss: 3.818207\n",
      "Train Epoch: 2 [3599360/5599780 (64%)]\tLoss: 3.821877\n",
      "Train Epoch: 2 [3604480/5599780 (64%)]\tLoss: 3.835992\n",
      "Train Epoch: 2 [3609600/5599780 (64%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [3614720/5599780 (65%)]\tLoss: 3.838162\n",
      "Train Epoch: 2 [3619840/5599780 (65%)]\tLoss: 3.841718\n",
      "Train Epoch: 2 [3624960/5599780 (65%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [3630080/5599780 (65%)]\tLoss: 3.812331\n",
      "Train Epoch: 2 [3635200/5599780 (65%)]\tLoss: 3.812331\n",
      "Train Epoch: 2 [3640320/5599780 (65%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [3645440/5599780 (65%)]\tLoss: 3.847489\n",
      "Train Epoch: 2 [3650560/5599780 (65%)]\tLoss: 3.817962\n",
      "Train Epoch: 2 [3655680/5599780 (65%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [3660800/5599780 (65%)]\tLoss: 3.841544\n",
      "Train Epoch: 2 [3665920/5599780 (65%)]\tLoss: 3.804517\n",
      "Train Epoch: 2 [3671040/5599780 (66%)]\tLoss: 3.835778\n",
      "Train Epoch: 2 [3676160/5599780 (66%)]\tLoss: 3.872879\n",
      "Train Epoch: 2 [3681280/5599780 (66%)]\tLoss: 3.835863\n",
      "Train Epoch: 2 [3686400/5599780 (66%)]\tLoss: 3.816240\n",
      "Train Epoch: 2 [3691520/5599780 (66%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [3696640/5599780 (66%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [3701760/5599780 (66%)]\tLoss: 3.820110\n",
      "Train Epoch: 2 [3706880/5599780 (66%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [3712000/5599780 (66%)]\tLoss: 3.835771\n",
      "Train Epoch: 2 [3717120/5599780 (66%)]\tLoss: 3.833813\n",
      "Train Epoch: 2 [3722240/5599780 (66%)]\tLoss: 3.865066\n",
      "Train Epoch: 2 [3727360/5599780 (67%)]\tLoss: 3.843579\n",
      "Train Epoch: 2 [3732480/5599780 (67%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [3737600/5599780 (67%)]\tLoss: 3.859200\n",
      "Train Epoch: 2 [3742720/5599780 (67%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [3747840/5599780 (67%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [3752960/5599780 (67%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [3758080/5599780 (67%)]\tLoss: 3.855029\n",
      "Train Epoch: 2 [3763200/5599780 (67%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3768320/5599780 (67%)]\tLoss: 3.839673\n",
      "Train Epoch: 2 [3773440/5599780 (67%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [3778560/5599780 (67%)]\tLoss: 3.812332\n",
      "Train Epoch: 2 [3783680/5599780 (68%)]\tLoss: 3.859185\n",
      "Train Epoch: 2 [3788800/5599780 (68%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [3793920/5599780 (68%)]\tLoss: 3.857262\n",
      "Train Epoch: 2 [3799040/5599780 (68%)]\tLoss: 3.830842\n",
      "Train Epoch: 2 [3804160/5599780 (68%)]\tLoss: 3.792595\n",
      "Train Epoch: 2 [3809280/5599780 (68%)]\tLoss: 3.798660\n",
      "Train Epoch: 2 [3814400/5599780 (68%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [3819520/5599780 (68%)]\tLoss: 3.812363\n",
      "Train Epoch: 2 [3824640/5599780 (68%)]\tLoss: 3.843624\n",
      "Train Epoch: 2 [3829760/5599780 (68%)]\tLoss: 3.849445\n",
      "Train Epoch: 2 [3834880/5599780 (68%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [3840000/5599780 (69%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [3845120/5599780 (69%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [3850240/5599780 (69%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [3855360/5599780 (69%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [3860480/5599780 (69%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [3865600/5599780 (69%)]\tLoss: 3.829812\n",
      "Train Epoch: 2 [3870720/5599780 (69%)]\tLoss: 3.771322\n",
      "Train Epoch: 2 [3875840/5599780 (69%)]\tLoss: 3.826268\n",
      "Train Epoch: 2 [3880960/5599780 (69%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [3886080/5599780 (69%)]\tLoss: 3.868973\n",
      "Train Epoch: 2 [3891200/5599780 (69%)]\tLoss: 3.777176\n",
      "Train Epoch: 2 [3896320/5599780 (70%)]\tLoss: 3.813068\n",
      "Train Epoch: 2 [3901440/5599780 (70%)]\tLoss: 3.792801\n",
      "Train Epoch: 2 [3906560/5599780 (70%)]\tLoss: 3.792801\n",
      "Train Epoch: 2 [3911680/5599780 (70%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [3916800/5599780 (70%)]\tLoss: 3.804403\n",
      "Train Epoch: 2 [3921920/5599780 (70%)]\tLoss: 3.837652\n",
      "Train Epoch: 2 [3927040/5599780 (70%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [3932160/5599780 (70%)]\tLoss: 3.855245\n",
      "Train Epoch: 2 [3937280/5599780 (70%)]\tLoss: 3.833532\n",
      "Train Epoch: 2 [3942400/5599780 (70%)]\tLoss: 3.835769\n",
      "Train Epoch: 2 [3947520/5599780 (70%)]\tLoss: 3.823490\n",
      "Train Epoch: 2 [3952640/5599780 (71%)]\tLoss: 3.822057\n",
      "Train Epoch: 2 [3957760/5599780 (71%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [3962880/5599780 (71%)]\tLoss: 3.812333\n",
      "Train Epoch: 2 [3968000/5599780 (71%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [3973120/5599780 (71%)]\tLoss: 3.847862\n",
      "Train Epoch: 2 [3978240/5599780 (71%)]\tLoss: 3.851107\n",
      "Train Epoch: 2 [3983360/5599780 (71%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [3988480/5599780 (71%)]\tLoss: 3.826037\n",
      "Train Epoch: 2 [3993600/5599780 (71%)]\tLoss: 3.791288\n",
      "Train Epoch: 2 [3998720/5599780 (71%)]\tLoss: 3.839675\n",
      "Train Epoch: 2 [4003840/5599780 (71%)]\tLoss: 3.827956\n",
      "Train Epoch: 2 [4008960/5599780 (72%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [4014080/5599780 (72%)]\tLoss: 3.835566\n",
      "Train Epoch: 2 [4019200/5599780 (72%)]\tLoss: 3.804456\n",
      "Train Epoch: 2 [4024320/5599780 (72%)]\tLoss: 3.810497\n",
      "Train Epoch: 2 [4029440/5599780 (72%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [4034560/5599780 (72%)]\tLoss: 3.841487\n",
      "Train Epoch: 2 [4039680/5599780 (72%)]\tLoss: 3.820143\n",
      "Train Epoch: 2 [4044800/5599780 (72%)]\tLoss: 3.847488\n",
      "Train Epoch: 2 [4049920/5599780 (72%)]\tLoss: 3.859456\n",
      "Train Epoch: 2 [4055040/5599780 (72%)]\tLoss: 3.818156\n",
      "Train Epoch: 2 [4060160/5599780 (72%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [4065280/5599780 (73%)]\tLoss: 3.859436\n",
      "Train Epoch: 2 [4070400/5599780 (73%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [4075520/5599780 (73%)]\tLoss: 3.798661\n",
      "Train Epoch: 2 [4080640/5599780 (73%)]\tLoss: 3.812334\n",
      "Train Epoch: 2 [4085760/5599780 (73%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [4090880/5599780 (73%)]\tLoss: 3.843386\n",
      "Train Epoch: 2 [4096000/5599780 (73%)]\tLoss: 3.829886\n",
      "Train Epoch: 2 [4101120/5599780 (73%)]\tLoss: 3.867017\n",
      "Train Epoch: 2 [4106240/5599780 (73%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [4111360/5599780 (73%)]\tLoss: 3.821912\n",
      "Train Epoch: 2 [4116480/5599780 (74%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [4121600/5599780 (74%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4126720/5599780 (74%)]\tLoss: 3.798663\n",
      "Train Epoch: 2 [4131840/5599780 (74%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4136960/5599780 (74%)]\tLoss: 3.855299\n",
      "Train Epoch: 2 [4142080/5599780 (74%)]\tLoss: 3.876566\n",
      "Train Epoch: 2 [4147200/5599780 (74%)]\tLoss: 3.856962\n",
      "Train Epoch: 2 [4152320/5599780 (74%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [4157440/5599780 (74%)]\tLoss: 3.808358\n",
      "Train Epoch: 2 [4162560/5599780 (74%)]\tLoss: 3.843583\n",
      "Train Epoch: 2 [4167680/5599780 (74%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [4172800/5599780 (75%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [4177920/5599780 (75%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [4183040/5599780 (75%)]\tLoss: 3.856966\n",
      "Train Epoch: 2 [4188160/5599780 (75%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [4193280/5599780 (75%)]\tLoss: 3.839688\n",
      "Train Epoch: 2 [4198400/5599780 (75%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [4203520/5599780 (75%)]\tLoss: 3.832902\n",
      "Train Epoch: 2 [4208640/5599780 (75%)]\tLoss: 3.847427\n",
      "Train Epoch: 2 [4213760/5599780 (75%)]\tLoss: 3.827962\n",
      "Train Epoch: 2 [4218880/5599780 (75%)]\tLoss: 3.849441\n",
      "Train Epoch: 2 [4224000/5599780 (75%)]\tLoss: 3.818179\n",
      "Train Epoch: 2 [4229120/5599780 (76%)]\tLoss: 3.820142\n",
      "Train Epoch: 2 [4234240/5599780 (76%)]\tLoss: 3.771753\n",
      "Train Epoch: 2 [4239360/5599780 (76%)]\tLoss: 3.837722\n",
      "Train Epoch: 2 [4244480/5599780 (76%)]\tLoss: 3.831889\n",
      "Train Epoch: 2 [4249600/5599780 (76%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [4254720/5599780 (76%)]\tLoss: 3.851207\n",
      "Train Epoch: 2 [4259840/5599780 (76%)]\tLoss: 3.820824\n",
      "Train Epoch: 2 [4264960/5599780 (76%)]\tLoss: 3.831862\n",
      "Train Epoch: 2 [4270080/5599780 (76%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [4275200/5599780 (76%)]\tLoss: 3.841604\n",
      "Train Epoch: 2 [4280320/5599780 (76%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [4285440/5599780 (77%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [4290560/5599780 (77%)]\tLoss: 3.796707\n",
      "Train Epoch: 2 [4295680/5599780 (77%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [4300800/5599780 (77%)]\tLoss: 3.865066\n",
      "Train Epoch: 2 [4305920/5599780 (77%)]\tLoss: 3.805684\n",
      "Train Epoch: 2 [4311040/5599780 (77%)]\tLoss: 3.839833\n",
      "Train Epoch: 2 [4316160/5599780 (77%)]\tLoss: 3.771311\n",
      "Train Epoch: 2 [4321280/5599780 (77%)]\tLoss: 3.819752\n",
      "Train Epoch: 2 [4326400/5599780 (77%)]\tLoss: 3.783037\n",
      "Train Epoch: 2 [4331520/5599780 (77%)]\tLoss: 3.823777\n",
      "Train Epoch: 2 [4336640/5599780 (77%)]\tLoss: 3.842347\n",
      "Train Epoch: 2 [4341760/5599780 (78%)]\tLoss: 3.806473\n",
      "Train Epoch: 2 [4346880/5599780 (78%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [4352000/5599780 (78%)]\tLoss: 3.821930\n",
      "Train Epoch: 2 [4357120/5599780 (78%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [4362240/5599780 (78%)]\tLoss: 3.841594\n",
      "Train Epoch: 2 [4367360/5599780 (78%)]\tLoss: 3.822098\n",
      "Train Epoch: 2 [4372480/5599780 (78%)]\tLoss: 3.813989\n",
      "Train Epoch: 2 [4377600/5599780 (78%)]\tLoss: 3.853400\n",
      "Train Epoch: 2 [4382720/5599780 (78%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [4387840/5599780 (78%)]\tLoss: 3.839674\n",
      "Train Epoch: 2 [4392960/5599780 (78%)]\tLoss: 3.784987\n",
      "Train Epoch: 2 [4398080/5599780 (79%)]\tLoss: 3.820241\n",
      "Train Epoch: 2 [4403200/5599780 (79%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [4408320/5599780 (79%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [4413440/5599780 (79%)]\tLoss: 3.792798\n",
      "Train Epoch: 2 [4418560/5599780 (79%)]\tLoss: 3.826003\n",
      "Train Epoch: 2 [4423680/5599780 (79%)]\tLoss: 3.839668\n",
      "Train Epoch: 2 [4428800/5599780 (79%)]\tLoss: 3.841469\n",
      "Train Epoch: 2 [4433920/5599780 (79%)]\tLoss: 3.812331\n",
      "Train Epoch: 2 [4439040/5599780 (79%)]\tLoss: 3.812357\n",
      "Train Epoch: 2 [4444160/5599780 (79%)]\tLoss: 3.859191\n",
      "Train Epoch: 2 [4449280/5599780 (79%)]\tLoss: 3.827863\n",
      "Train Epoch: 2 [4454400/5599780 (80%)]\tLoss: 3.822099\n",
      "Train Epoch: 2 [4459520/5599780 (80%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4464640/5599780 (80%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [4469760/5599780 (80%)]\tLoss: 3.808455\n",
      "Train Epoch: 2 [4474880/5599780 (80%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [4480000/5599780 (80%)]\tLoss: 3.853366\n",
      "Train Epoch: 2 [4485120/5599780 (80%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [4490240/5599780 (80%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [4495360/5599780 (80%)]\tLoss: 3.792434\n",
      "Train Epoch: 2 [4500480/5599780 (80%)]\tLoss: 3.843647\n",
      "Train Epoch: 2 [4505600/5599780 (80%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [4510720/5599780 (81%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [4515840/5599780 (81%)]\tLoss: 3.835737\n",
      "Train Epoch: 2 [4520960/5599780 (81%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [4526080/5599780 (81%)]\tLoss: 3.837723\n",
      "Train Epoch: 2 [4531200/5599780 (81%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [4536320/5599780 (81%)]\tLoss: 3.839577\n",
      "Train Epoch: 2 [4541440/5599780 (81%)]\tLoss: 3.823697\n",
      "Train Epoch: 2 [4546560/5599780 (81%)]\tLoss: 3.818189\n",
      "Train Epoch: 2 [4551680/5599780 (81%)]\tLoss: 3.763080\n",
      "Train Epoch: 2 [4556800/5599780 (81%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [4561920/5599780 (81%)]\tLoss: 3.814305\n",
      "Train Epoch: 2 [4567040/5599780 (82%)]\tLoss: 3.835765\n",
      "Train Epoch: 2 [4572160/5599780 (82%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [4577280/5599780 (82%)]\tLoss: 3.825994\n",
      "Train Epoch: 2 [4582400/5599780 (82%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [4587520/5599780 (82%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [4592640/5599780 (82%)]\tLoss: 3.863213\n",
      "Train Epoch: 2 [4597760/5599780 (82%)]\tLoss: 3.835776\n",
      "Train Epoch: 2 [4602880/5599780 (82%)]\tLoss: 3.816232\n",
      "Train Epoch: 2 [4608000/5599780 (82%)]\tLoss: 3.776059\n",
      "Train Epoch: 2 [4613120/5599780 (82%)]\tLoss: 3.829993\n",
      "Train Epoch: 2 [4618240/5599780 (82%)]\tLoss: 3.851174\n",
      "Train Epoch: 2 [4623360/5599780 (83%)]\tLoss: 3.835613\n",
      "Train Epoch: 2 [4628480/5599780 (83%)]\tLoss: 3.865129\n",
      "Train Epoch: 2 [4633600/5599780 (83%)]\tLoss: 3.875957\n",
      "Train Epoch: 2 [4638720/5599780 (83%)]\tLoss: 3.851402\n",
      "Train Epoch: 2 [4643840/5599780 (83%)]\tLoss: 3.825927\n",
      "Train Epoch: 2 [4648960/5599780 (83%)]\tLoss: 3.818439\n",
      "Train Epoch: 2 [4654080/5599780 (83%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [4659200/5599780 (83%)]\tLoss: 3.822118\n",
      "Train Epoch: 2 [4664320/5599780 (83%)]\tLoss: 3.794754\n",
      "Train Epoch: 2 [4669440/5599780 (83%)]\tLoss: 3.796704\n",
      "Train Epoch: 2 [4674560/5599780 (83%)]\tLoss: 3.867019\n",
      "Train Epoch: 2 [4679680/5599780 (84%)]\tLoss: 3.855278\n",
      "Train Epoch: 2 [4684800/5599780 (84%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [4689920/5599780 (84%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [4695040/5599780 (84%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [4700160/5599780 (84%)]\tLoss: 3.831945\n",
      "Train Epoch: 2 [4705280/5599780 (84%)]\tLoss: 3.829752\n",
      "Train Epoch: 2 [4710400/5599780 (84%)]\tLoss: 3.808426\n",
      "Train Epoch: 2 [4715520/5599780 (84%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4720640/5599780 (84%)]\tLoss: 3.855304\n",
      "Train Epoch: 2 [4725760/5599780 (84%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [4730880/5599780 (84%)]\tLoss: 3.853172\n",
      "Train Epoch: 2 [4736000/5599780 (85%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [4741120/5599780 (85%)]\tLoss: 3.804526\n",
      "Train Epoch: 2 [4746240/5599780 (85%)]\tLoss: 3.802566\n",
      "Train Epoch: 2 [4751360/5599780 (85%)]\tLoss: 3.808457\n",
      "Train Epoch: 2 [4756480/5599780 (85%)]\tLoss: 3.794747\n",
      "Train Epoch: 2 [4761600/5599780 (85%)]\tLoss: 3.796808\n",
      "Train Epoch: 2 [4766720/5599780 (85%)]\tLoss: 3.839646\n",
      "Train Epoch: 2 [4771840/5599780 (85%)]\tLoss: 3.851396\n",
      "Train Epoch: 2 [4776960/5599780 (85%)]\tLoss: 3.831852\n",
      "Train Epoch: 2 [4782080/5599780 (85%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [4787200/5599780 (85%)]\tLoss: 3.843583\n",
      "Train Epoch: 2 [4792320/5599780 (86%)]\tLoss: 3.800613\n",
      "Train Epoch: 2 [4797440/5599780 (86%)]\tLoss: 3.859163\n",
      "Train Epoch: 2 [4802560/5599780 (86%)]\tLoss: 3.833740\n",
      "Train Epoch: 2 [4807680/5599780 (86%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [4812800/5599780 (86%)]\tLoss: 3.829909\n",
      "Train Epoch: 2 [4817920/5599780 (86%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [4823040/5599780 (86%)]\tLoss: 3.820175\n",
      "Train Epoch: 2 [4828160/5599780 (86%)]\tLoss: 3.837722\n",
      "Train Epoch: 2 [4833280/5599780 (86%)]\tLoss: 3.845324\n",
      "Train Epoch: 2 [4838400/5599780 (86%)]\tLoss: 3.835809\n",
      "Train Epoch: 2 [4843520/5599780 (86%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [4848640/5599780 (87%)]\tLoss: 3.837682\n",
      "Train Epoch: 2 [4853760/5599780 (87%)]\tLoss: 3.827957\n",
      "Train Epoch: 2 [4858880/5599780 (87%)]\tLoss: 3.824050\n",
      "Train Epoch: 2 [4864000/5599780 (87%)]\tLoss: 3.837526\n",
      "Train Epoch: 2 [4869120/5599780 (87%)]\tLoss: 3.853066\n",
      "Train Epoch: 2 [4874240/5599780 (87%)]\tLoss: 3.816238\n",
      "Train Epoch: 2 [4879360/5599780 (87%)]\tLoss: 3.845534\n",
      "Train Epoch: 2 [4884480/5599780 (87%)]\tLoss: 3.806465\n",
      "Train Epoch: 2 [4889600/5599780 (87%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [4894720/5599780 (87%)]\tLoss: 3.786931\n",
      "Train Epoch: 2 [4899840/5599780 (87%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [4904960/5599780 (88%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [4910080/5599780 (88%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4915200/5599780 (88%)]\tLoss: 3.886550\n",
      "Train Epoch: 2 [4920320/5599780 (88%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [4925440/5599780 (88%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [4930560/5599780 (88%)]\tLoss: 3.855295\n",
      "Train Epoch: 2 [4935680/5599780 (88%)]\tLoss: 3.810403\n",
      "Train Epoch: 2 [4940800/5599780 (88%)]\tLoss: 3.830039\n",
      "Train Epoch: 2 [4945920/5599780 (88%)]\tLoss: 3.827955\n",
      "Train Epoch: 2 [4951040/5599780 (88%)]\tLoss: 3.827960\n",
      "Train Epoch: 2 [4956160/5599780 (88%)]\tLoss: 3.841647\n",
      "Train Epoch: 2 [4961280/5599780 (89%)]\tLoss: 3.814290\n",
      "Train Epoch: 2 [4966400/5599780 (89%)]\tLoss: 3.861101\n",
      "Train Epoch: 2 [4971520/5599780 (89%)]\tLoss: 3.790846\n",
      "Train Epoch: 2 [4976640/5599780 (89%)]\tLoss: 3.851396\n",
      "Train Epoch: 2 [4981760/5599780 (89%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [4986880/5599780 (89%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [4992000/5599780 (89%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [4997120/5599780 (89%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [5002240/5599780 (89%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [5007360/5599780 (89%)]\tLoss: 3.816862\n",
      "Train Epoch: 2 [5012480/5599780 (90%)]\tLoss: 3.829866\n",
      "Train Epoch: 2 [5017600/5599780 (90%)]\tLoss: 3.824039\n",
      "Train Epoch: 2 [5022720/5599780 (90%)]\tLoss: 3.788671\n",
      "Train Epoch: 2 [5027840/5599780 (90%)]\tLoss: 3.884598\n",
      "Train Epoch: 2 [5032960/5599780 (90%)]\tLoss: 3.857254\n",
      "Train Epoch: 2 [5038080/5599780 (90%)]\tLoss: 3.818191\n",
      "Train Epoch: 2 [5043200/5599780 (90%)]\tLoss: 3.829706\n",
      "Train Epoch: 2 [5048320/5599780 (90%)]\tLoss: 3.820147\n",
      "Train Epoch: 2 [5053440/5599780 (90%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [5058560/5599780 (90%)]\tLoss: 3.833817\n",
      "Train Epoch: 2 [5063680/5599780 (90%)]\tLoss: 3.798666\n",
      "Train Epoch: 2 [5068800/5599780 (91%)]\tLoss: 3.829907\n",
      "Train Epoch: 2 [5073920/5599780 (91%)]\tLoss: 3.839676\n",
      "Train Epoch: 2 [5079040/5599780 (91%)]\tLoss: 3.837722\n",
      "Train Epoch: 2 [5084160/5599780 (91%)]\tLoss: 3.835765\n",
      "Train Epoch: 2 [5089280/5599780 (91%)]\tLoss: 3.833817\n",
      "Train Epoch: 2 [5094400/5599780 (91%)]\tLoss: 3.810379\n",
      "Train Epoch: 2 [5099520/5599780 (91%)]\tLoss: 3.812331\n",
      "Train Epoch: 2 [5104640/5599780 (91%)]\tLoss: 3.812221\n",
      "Train Epoch: 2 [5109760/5599780 (91%)]\tLoss: 3.806476\n",
      "Train Epoch: 2 [5114880/5599780 (91%)]\tLoss: 3.855299\n",
      "Train Epoch: 2 [5120000/5599780 (91%)]\tLoss: 3.851395\n",
      "Train Epoch: 2 [5125120/5599780 (92%)]\tLoss: 3.804519\n",
      "Train Epoch: 2 [5130240/5599780 (92%)]\tLoss: 3.847486\n",
      "Train Epoch: 2 [5135360/5599780 (92%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [5140480/5599780 (92%)]\tLoss: 3.860866\n",
      "Train Epoch: 2 [5145600/5599780 (92%)]\tLoss: 3.851398\n",
      "Train Epoch: 2 [5150720/5599780 (92%)]\tLoss: 3.807572\n",
      "Train Epoch: 2 [5155840/5599780 (92%)]\tLoss: 3.827993\n",
      "Train Epoch: 2 [5160960/5599780 (92%)]\tLoss: 3.823979\n",
      "Train Epoch: 2 [5166080/5599780 (92%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [5171200/5599780 (92%)]\tLoss: 3.825978\n",
      "Train Epoch: 2 [5176320/5599780 (92%)]\tLoss: 3.814185\n",
      "Train Epoch: 2 [5181440/5599780 (93%)]\tLoss: 3.820136\n",
      "Train Epoch: 2 [5186560/5599780 (93%)]\tLoss: 3.853348\n",
      "Train Epoch: 2 [5191680/5599780 (93%)]\tLoss: 3.837711\n",
      "Train Epoch: 2 [5196800/5599780 (93%)]\tLoss: 3.812402\n",
      "Train Epoch: 2 [5201920/5599780 (93%)]\tLoss: 3.822097\n",
      "Train Epoch: 2 [5207040/5599780 (93%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [5212160/5599780 (93%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [5217280/5599780 (93%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [5222400/5599780 (93%)]\tLoss: 3.836084\n",
      "Train Epoch: 2 [5227520/5599780 (93%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [5232640/5599780 (93%)]\tLoss: 3.845513\n",
      "Train Epoch: 2 [5237760/5599780 (94%)]\tLoss: 3.855300\n",
      "Train Epoch: 2 [5242880/5599780 (94%)]\tLoss: 3.812284\n",
      "Train Epoch: 2 [5248000/5599780 (94%)]\tLoss: 3.801973\n",
      "Train Epoch: 2 [5253120/5599780 (94%)]\tLoss: 3.855301\n",
      "Train Epoch: 2 [5258240/5599780 (94%)]\tLoss: 3.849415\n",
      "Train Epoch: 2 [5263360/5599780 (94%)]\tLoss: 3.861336\n",
      "Train Epoch: 2 [5268480/5599780 (94%)]\tLoss: 3.820102\n",
      "Train Epoch: 2 [5273600/5599780 (94%)]\tLoss: 3.839672\n",
      "Train Epoch: 2 [5278720/5599780 (94%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [5283840/5599780 (94%)]\tLoss: 3.847177\n",
      "Train Epoch: 2 [5288960/5599780 (94%)]\tLoss: 3.841624\n",
      "Train Epoch: 2 [5294080/5599780 (95%)]\tLoss: 3.790847\n",
      "Train Epoch: 2 [5299200/5599780 (95%)]\tLoss: 3.872879\n",
      "Train Epoch: 2 [5304320/5599780 (95%)]\tLoss: 3.808034\n",
      "Train Epoch: 2 [5309440/5599780 (95%)]\tLoss: 3.786941\n",
      "Train Epoch: 2 [5314560/5599780 (95%)]\tLoss: 3.831167\n",
      "Train Epoch: 2 [5319680/5599780 (95%)]\tLoss: 3.786941\n",
      "Train Epoch: 2 [5324800/5599780 (95%)]\tLoss: 3.829910\n",
      "Train Epoch: 2 [5329920/5599780 (95%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [5335040/5599780 (95%)]\tLoss: 3.824051\n",
      "Train Epoch: 2 [5340160/5599780 (95%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [5345280/5599780 (95%)]\tLoss: 3.849648\n",
      "Train Epoch: 2 [5350400/5599780 (96%)]\tLoss: 3.854623\n",
      "Train Epoch: 2 [5355520/5599780 (96%)]\tLoss: 3.831863\n",
      "Train Epoch: 2 [5360640/5599780 (96%)]\tLoss: 3.857251\n",
      "Train Epoch: 2 [5365760/5599780 (96%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [5370880/5599780 (96%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [5376000/5599780 (96%)]\tLoss: 3.844438\n",
      "Train Epoch: 2 [5381120/5599780 (96%)]\tLoss: 3.810460\n",
      "Train Epoch: 2 [5386240/5599780 (96%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [5391360/5599780 (96%)]\tLoss: 3.855290\n",
      "Train Epoch: 2 [5396480/5599780 (96%)]\tLoss: 3.880692\n",
      "Train Epoch: 2 [5401600/5599780 (96%)]\tLoss: 3.829904\n",
      "Train Epoch: 2 [5406720/5599780 (97%)]\tLoss: 3.833816\n",
      "Train Epoch: 2 [5411840/5599780 (97%)]\tLoss: 3.820144\n",
      "Train Epoch: 2 [5416960/5599780 (97%)]\tLoss: 3.831864\n",
      "Train Epoch: 2 [5422080/5599780 (97%)]\tLoss: 3.845535\n",
      "Train Epoch: 2 [5427200/5599780 (97%)]\tLoss: 3.829598\n",
      "Train Epoch: 2 [5432320/5599780 (97%)]\tLoss: 3.820160\n",
      "Train Epoch: 2 [5437440/5599780 (97%)]\tLoss: 3.819412\n",
      "Train Epoch: 2 [5442560/5599780 (97%)]\tLoss: 3.818173\n",
      "Train Epoch: 2 [5447680/5599780 (97%)]\tLoss: 3.822099\n",
      "Train Epoch: 2 [5452800/5599780 (97%)]\tLoss: 3.861160\n",
      "Train Epoch: 2 [5457920/5599780 (97%)]\tLoss: 3.859202\n",
      "Train Epoch: 2 [5463040/5599780 (98%)]\tLoss: 3.812329\n",
      "Train Epoch: 2 [5468160/5599780 (98%)]\tLoss: 3.814285\n",
      "Train Epoch: 2 [5473280/5599780 (98%)]\tLoss: 3.839622\n",
      "Train Epoch: 2 [5478400/5599780 (98%)]\tLoss: 3.790848\n",
      "Train Epoch: 2 [5483520/5599780 (98%)]\tLoss: 3.818190\n",
      "Train Epoch: 2 [5488640/5599780 (98%)]\tLoss: 3.826016\n",
      "Train Epoch: 2 [5493760/5599780 (98%)]\tLoss: 3.856863\n",
      "Train Epoch: 2 [5498880/5599780 (98%)]\tLoss: 3.855160\n",
      "Train Epoch: 2 [5504000/5599780 (98%)]\tLoss: 3.847193\n",
      "Train Epoch: 2 [5509120/5599780 (98%)]\tLoss: 3.865063\n",
      "Train Epoch: 2 [5514240/5599780 (98%)]\tLoss: 3.843582\n",
      "Train Epoch: 2 [5519360/5599780 (99%)]\tLoss: 3.845323\n",
      "Train Epoch: 2 [5524480/5599780 (99%)]\tLoss: 3.826064\n",
      "Train Epoch: 2 [5529600/5599780 (99%)]\tLoss: 3.808425\n",
      "Train Epoch: 2 [5534720/5599780 (99%)]\tLoss: 3.826004\n",
      "Train Epoch: 2 [5539840/5599780 (99%)]\tLoss: 3.798658\n",
      "Train Epoch: 2 [5544960/5599780 (99%)]\tLoss: 3.870925\n",
      "Train Epoch: 2 [5550080/5599780 (99%)]\tLoss: 3.826007\n",
      "Train Epoch: 2 [5555200/5599780 (99%)]\tLoss: 3.825820\n",
      "Train Epoch: 2 [5560320/5599780 (99%)]\tLoss: 3.851394\n",
      "Train Epoch: 2 [5565440/5599780 (99%)]\tLoss: 3.859207\n",
      "Train Epoch: 2 [5570560/5599780 (99%)]\tLoss: 3.825985\n",
      "Train Epoch: 2 [5575680/5599780 (100%)]\tLoss: 3.836714\n",
      "Train Epoch: 2 [5580800/5599780 (100%)]\tLoss: 3.800629\n",
      "Train Epoch: 2 [5585920/5599780 (100%)]\tLoss: 3.904105\n",
      "Train Epoch: 2 [5591040/5599780 (100%)]\tLoss: 3.882644\n",
      "Train Epoch: 2 [5596160/5599780 (100%)]\tLoss: 3.814293\n",
      "\n",
      "Test set: Average loss: 3.8279, Accuracy: 303653/849497 (36%)\n",
      "\n",
      "Registering model baseline_model\n",
      "Train Epoch: 3 [0/5599780 (0%)]\tLoss: 3.857242\n",
      "Train Epoch: 3 [5120/5599780 (0%)]\tLoss: 3.800613\n",
      "Train Epoch: 3 [10240/5599780 (0%)]\tLoss: 3.792801\n",
      "Train Epoch: 3 [15360/5599780 (0%)]\tLoss: 3.821715\n",
      "Train Epoch: 3 [20480/5599780 (0%)]\tLoss: 3.872879\n",
      "Train Epoch: 3 [25600/5599780 (0%)]\tLoss: 3.864923\n",
      "Train Epoch: 3 [30720/5599780 (1%)]\tLoss: 3.872531\n",
      "Train Epoch: 3 [35840/5599780 (1%)]\tLoss: 3.823272\n",
      "Train Epoch: 3 [40960/5599780 (1%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [46080/5599780 (1%)]\tLoss: 3.845535\n",
      "Train Epoch: 3 [51200/5599780 (1%)]\tLoss: 3.802338\n",
      "Train Epoch: 3 [56320/5599780 (1%)]\tLoss: 3.843574\n",
      "Train Epoch: 3 [61440/5599780 (1%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [66560/5599780 (1%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [71680/5599780 (1%)]\tLoss: 3.837722\n",
      "Train Epoch: 3 [76800/5599780 (1%)]\tLoss: 3.859209\n",
      "Train Epoch: 3 [81920/5599780 (1%)]\tLoss: 3.859207\n",
      "Train Epoch: 3 [87040/5599780 (2%)]\tLoss: 3.837724\n",
      "Train Epoch: 3 [92160/5599780 (2%)]\tLoss: 3.820144\n",
      "Train Epoch: 3 [97280/5599780 (2%)]\tLoss: 3.836528\n",
      "Train Epoch: 3 [102400/5599780 (2%)]\tLoss: 3.873139\n",
      "Train Epoch: 3 [107520/5599780 (2%)]\tLoss: 3.802562\n",
      "Train Epoch: 3 [112640/5599780 (2%)]\tLoss: 3.878853\n",
      "Train Epoch: 3 [117760/5599780 (2%)]\tLoss: 3.812629\n",
      "Train Epoch: 3 [122880/5599780 (2%)]\tLoss: 3.818189\n",
      "Train Epoch: 3 [128000/5599780 (2%)]\tLoss: 3.814285\n",
      "Train Epoch: 3 [133120/5599780 (2%)]\tLoss: 3.831858\n",
      "Train Epoch: 3 [138240/5599780 (2%)]\tLoss: 3.818313\n",
      "Train Epoch: 3 [143360/5599780 (3%)]\tLoss: 3.802558\n",
      "Train Epoch: 3 [148480/5599780 (3%)]\tLoss: 3.861158\n",
      "Train Epoch: 3 [153600/5599780 (3%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [158720/5599780 (3%)]\tLoss: 3.784968\n",
      "Train Epoch: 3 [163840/5599780 (3%)]\tLoss: 3.824055\n",
      "Train Epoch: 3 [168960/5599780 (3%)]\tLoss: 3.796707\n",
      "Train Epoch: 3 [174080/5599780 (3%)]\tLoss: 3.820159\n",
      "Train Epoch: 3 [179200/5599780 (3%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [184320/5599780 (3%)]\tLoss: 3.817975\n",
      "Train Epoch: 3 [189440/5599780 (3%)]\tLoss: 3.843582\n",
      "Train Epoch: 3 [194560/5599780 (3%)]\tLoss: 3.837198\n",
      "Train Epoch: 3 [199680/5599780 (4%)]\tLoss: 3.808426\n",
      "Train Epoch: 3 [204800/5599780 (4%)]\tLoss: 3.810380\n",
      "Train Epoch: 3 [209920/5599780 (4%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [215040/5599780 (4%)]\tLoss: 3.812330\n",
      "Train Epoch: 3 [220160/5599780 (4%)]\tLoss: 3.857254\n",
      "Train Epoch: 3 [225280/5599780 (4%)]\tLoss: 3.850925\n",
      "Train Epoch: 3 [230400/5599780 (4%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [235520/5599780 (4%)]\tLoss: 3.853369\n",
      "Train Epoch: 3 [240640/5599780 (4%)]\tLoss: 3.818184\n",
      "Train Epoch: 3 [245760/5599780 (4%)]\tLoss: 3.822098\n",
      "Train Epoch: 3 [250880/5599780 (4%)]\tLoss: 3.847488\n",
      "Train Epoch: 3 [256000/5599780 (5%)]\tLoss: 3.861098\n",
      "Train Epoch: 3 [261120/5599780 (5%)]\tLoss: 3.874832\n",
      "Train Epoch: 3 [266240/5599780 (5%)]\tLoss: 3.835765\n",
      "Train Epoch: 3 [271360/5599780 (5%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [276480/5599780 (5%)]\tLoss: 3.864722\n",
      "Train Epoch: 3 [281600/5599780 (5%)]\tLoss: 3.818191\n",
      "Train Epoch: 3 [286720/5599780 (5%)]\tLoss: 3.800616\n",
      "Train Epoch: 3 [291840/5599780 (5%)]\tLoss: 3.801573\n",
      "Train Epoch: 3 [296960/5599780 (5%)]\tLoss: 3.812131\n",
      "Train Epoch: 3 [302080/5599780 (5%)]\tLoss: 3.820144\n",
      "Train Epoch: 3 [307200/5599780 (5%)]\tLoss: 3.829910\n",
      "Train Epoch: 3 [312320/5599780 (6%)]\tLoss: 3.829910\n",
      "Train Epoch: 3 [317440/5599780 (6%)]\tLoss: 3.798500\n",
      "Train Epoch: 3 [322560/5599780 (6%)]\tLoss: 3.812332\n",
      "Train Epoch: 3 [327680/5599780 (6%)]\tLoss: 3.837950\n",
      "Train Epoch: 3 [332800/5599780 (6%)]\tLoss: 3.843582\n",
      "Train Epoch: 3 [337920/5599780 (6%)]\tLoss: 3.847502\n",
      "Train Epoch: 3 [343040/5599780 (6%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [348160/5599780 (6%)]\tLoss: 3.818429\n",
      "Train Epoch: 3 [353280/5599780 (6%)]\tLoss: 3.831865\n",
      "Train Epoch: 3 [358400/5599780 (6%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [363520/5599780 (6%)]\tLoss: 3.832025\n",
      "Train Epoch: 3 [368640/5599780 (7%)]\tLoss: 3.808554\n",
      "Train Epoch: 3 [373760/5599780 (7%)]\tLoss: 3.784988\n",
      "Train Epoch: 3 [378880/5599780 (7%)]\tLoss: 3.799453\n",
      "Train Epoch: 3 [384000/5599780 (7%)]\tLoss: 3.798659\n",
      "Train Epoch: 3 [389120/5599780 (7%)]\tLoss: 3.843617\n",
      "Train Epoch: 3 [394240/5599780 (7%)]\tLoss: 3.827929\n",
      "Train Epoch: 3 [399360/5599780 (7%)]\tLoss: 3.890457\n",
      "Train Epoch: 3 [404480/5599780 (7%)]\tLoss: 3.884596\n",
      "Train Epoch: 3 [409600/5599780 (7%)]\tLoss: 3.822096\n",
      "Train Epoch: 3 [414720/5599780 (7%)]\tLoss: 3.826004\n",
      "Train Epoch: 3 [419840/5599780 (7%)]\tLoss: 3.859094\n",
      "Train Epoch: 3 [424960/5599780 (8%)]\tLoss: 3.859206\n",
      "Train Epoch: 3 [430080/5599780 (8%)]\tLoss: 3.827947\n",
      "Train Epoch: 3 [435200/5599780 (8%)]\tLoss: 3.853547\n",
      "Train Epoch: 3 [440320/5599780 (8%)]\tLoss: 3.810420\n",
      "Train Epoch: 3 [445440/5599780 (8%)]\tLoss: 3.823648\n",
      "Train Epoch: 3 [450560/5599780 (8%)]\tLoss: 3.802189\n",
      "Train Epoch: 3 [455680/5599780 (8%)]\tLoss: 3.843485\n",
      "Train Epoch: 3 [460800/5599780 (8%)]\tLoss: 3.824051\n",
      "Train Epoch: 3 [465920/5599780 (8%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [471040/5599780 (8%)]\tLoss: 3.851394\n",
      "Train Epoch: 3 [476160/5599780 (9%)]\tLoss: 3.804513\n",
      "Train Epoch: 3 [481280/5599780 (9%)]\tLoss: 3.815849\n",
      "Train Epoch: 3 [486400/5599780 (9%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [491520/5599780 (9%)]\tLoss: 3.788894\n",
      "Train Epoch: 3 [496640/5599780 (9%)]\tLoss: 3.792801\n",
      "Train Epoch: 3 [501760/5599780 (9%)]\tLoss: 3.876785\n",
      "Train Epoch: 3 [506880/5599780 (9%)]\tLoss: 3.845535\n",
      "Train Epoch: 3 [512000/5599780 (9%)]\tLoss: 3.841688\n",
      "Train Epoch: 3 [517120/5599780 (9%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [522240/5599780 (9%)]\tLoss: 3.843606\n",
      "Train Epoch: 3 [527360/5599780 (9%)]\tLoss: 3.868943\n",
      "Train Epoch: 3 [532480/5599780 (10%)]\tLoss: 3.794754\n",
      "Train Epoch: 3 [537600/5599780 (10%)]\tLoss: 3.804527\n",
      "Train Epoch: 3 [542720/5599780 (10%)]\tLoss: 3.857254\n",
      "Train Epoch: 3 [547840/5599780 (10%)]\tLoss: 3.808319\n",
      "Train Epoch: 3 [552960/5599780 (10%)]\tLoss: 3.827957\n",
      "Train Epoch: 3 [558080/5599780 (10%)]\tLoss: 3.841627\n",
      "Train Epoch: 3 [563200/5599780 (10%)]\tLoss: 3.841629\n",
      "Train Epoch: 3 [568320/5599780 (10%)]\tLoss: 3.827589\n",
      "Train Epoch: 3 [573440/5599780 (10%)]\tLoss: 3.809899\n",
      "Train Epoch: 3 [578560/5599780 (10%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [583680/5599780 (10%)]\tLoss: 3.843582\n",
      "Train Epoch: 3 [588800/5599780 (11%)]\tLoss: 3.800614\n",
      "Train Epoch: 3 [593920/5599780 (11%)]\tLoss: 3.870920\n",
      "Train Epoch: 3 [599040/5599780 (11%)]\tLoss: 3.824126\n",
      "Train Epoch: 3 [604160/5599780 (11%)]\tLoss: 3.810379\n",
      "Train Epoch: 3 [609280/5599780 (11%)]\tLoss: 3.812314\n",
      "Train Epoch: 3 [614400/5599780 (11%)]\tLoss: 3.808424\n",
      "Train Epoch: 3 [619520/5599780 (11%)]\tLoss: 3.826004\n",
      "Train Epoch: 3 [624640/5599780 (11%)]\tLoss: 3.814235\n",
      "Train Epoch: 3 [629760/5599780 (11%)]\tLoss: 3.818191\n",
      "Train Epoch: 3 [634880/5599780 (11%)]\tLoss: 3.865067\n",
      "Train Epoch: 3 [640000/5599780 (11%)]\tLoss: 3.827533\n",
      "Train Epoch: 3 [645120/5599780 (12%)]\tLoss: 3.845559\n",
      "Train Epoch: 3 [650240/5599780 (12%)]\tLoss: 3.814280\n",
      "Train Epoch: 3 [655360/5599780 (12%)]\tLoss: 3.831706\n",
      "Train Epoch: 3 [660480/5599780 (12%)]\tLoss: 3.839675\n",
      "Train Epoch: 3 [665600/5599780 (12%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [670720/5599780 (12%)]\tLoss: 3.827901\n",
      "Train Epoch: 3 [675840/5599780 (12%)]\tLoss: 3.833556\n",
      "Train Epoch: 3 [680960/5599780 (12%)]\tLoss: 3.818009\n",
      "Train Epoch: 3 [686080/5599780 (12%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [691200/5599780 (12%)]\tLoss: 3.802566\n",
      "Train Epoch: 3 [696320/5599780 (12%)]\tLoss: 3.822098\n",
      "Train Epoch: 3 [701440/5599780 (13%)]\tLoss: 3.837659\n",
      "Train Epoch: 3 [706560/5599780 (13%)]\tLoss: 3.852639\n",
      "Train Epoch: 3 [711680/5599780 (13%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [716800/5599780 (13%)]\tLoss: 3.845530\n",
      "Train Epoch: 3 [721920/5599780 (13%)]\tLoss: 3.857123\n",
      "Train Epoch: 3 [727040/5599780 (13%)]\tLoss: 3.808399\n",
      "Train Epoch: 3 [732160/5599780 (13%)]\tLoss: 3.790848\n",
      "Train Epoch: 3 [737280/5599780 (13%)]\tLoss: 3.816234\n",
      "Train Epoch: 3 [742400/5599780 (13%)]\tLoss: 3.840078\n",
      "Train Epoch: 3 [747520/5599780 (13%)]\tLoss: 3.845563\n",
      "Train Epoch: 3 [752640/5599780 (13%)]\tLoss: 3.851394\n",
      "Train Epoch: 3 [757760/5599780 (14%)]\tLoss: 3.806823\n",
      "Train Epoch: 3 [762880/5599780 (14%)]\tLoss: 3.845470\n",
      "Train Epoch: 3 [768000/5599780 (14%)]\tLoss: 3.863195\n",
      "Train Epoch: 3 [773120/5599780 (14%)]\tLoss: 3.859165\n",
      "Train Epoch: 3 [778240/5599780 (14%)]\tLoss: 3.824066\n",
      "Train Epoch: 3 [783360/5599780 (14%)]\tLoss: 3.820179\n",
      "Train Epoch: 3 [788480/5599780 (14%)]\tLoss: 3.847488\n",
      "Train Epoch: 3 [793600/5599780 (14%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [798720/5599780 (14%)]\tLoss: 3.872877\n",
      "Train Epoch: 3 [803840/5599780 (14%)]\tLoss: 3.863055\n",
      "Train Epoch: 3 [808960/5599780 (14%)]\tLoss: 3.843582\n",
      "Train Epoch: 3 [814080/5599780 (15%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [819200/5599780 (15%)]\tLoss: 3.859118\n",
      "Train Epoch: 3 [824320/5599780 (15%)]\tLoss: 3.824051\n",
      "Train Epoch: 3 [829440/5599780 (15%)]\tLoss: 3.866992\n",
      "Train Epoch: 3 [834560/5599780 (15%)]\tLoss: 3.864874\n",
      "Train Epoch: 3 [839680/5599780 (15%)]\tLoss: 3.808113\n",
      "Train Epoch: 3 [844800/5599780 (15%)]\tLoss: 3.831540\n",
      "Train Epoch: 3 [849920/5599780 (15%)]\tLoss: 3.792822\n",
      "Train Epoch: 3 [855040/5599780 (15%)]\tLoss: 3.826204\n",
      "Train Epoch: 3 [860160/5599780 (15%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [865280/5599780 (15%)]\tLoss: 3.820186\n",
      "Train Epoch: 3 [870400/5599780 (16%)]\tLoss: 3.805629\n",
      "Train Epoch: 3 [875520/5599780 (16%)]\tLoss: 3.825739\n",
      "Train Epoch: 3 [880640/5599780 (16%)]\tLoss: 3.814071\n",
      "Train Epoch: 3 [885760/5599780 (16%)]\tLoss: 3.859234\n",
      "Train Epoch: 3 [890880/5599780 (16%)]\tLoss: 3.820132\n",
      "Train Epoch: 3 [896000/5599780 (16%)]\tLoss: 3.843454\n",
      "Train Epoch: 3 [901120/5599780 (16%)]\tLoss: 3.836455\n",
      "Train Epoch: 3 [906240/5599780 (16%)]\tLoss: 3.818140\n",
      "Train Epoch: 3 [911360/5599780 (16%)]\tLoss: 3.853328\n",
      "Train Epoch: 3 [916480/5599780 (16%)]\tLoss: 3.822896\n",
      "Train Epoch: 3 [921600/5599780 (16%)]\tLoss: 3.855299\n",
      "Train Epoch: 3 [926720/5599780 (17%)]\tLoss: 3.814830\n",
      "Train Epoch: 3 [931840/5599780 (17%)]\tLoss: 3.824175\n",
      "Train Epoch: 3 [936960/5599780 (17%)]\tLoss: 3.796708\n",
      "Train Epoch: 3 [942080/5599780 (17%)]\tLoss: 3.819332\n",
      "Train Epoch: 3 [947200/5599780 (17%)]\tLoss: 3.837313\n",
      "Train Epoch: 3 [952320/5599780 (17%)]\tLoss: 3.775182\n",
      "Train Epoch: 3 [957440/5599780 (17%)]\tLoss: 3.792801\n",
      "Train Epoch: 3 [962560/5599780 (17%)]\tLoss: 3.817971\n",
      "Train Epoch: 3 [967680/5599780 (17%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [972800/5599780 (17%)]\tLoss: 3.818193\n",
      "Train Epoch: 3 [977920/5599780 (17%)]\tLoss: 3.831907\n",
      "Train Epoch: 3 [983040/5599780 (18%)]\tLoss: 3.845484\n",
      "Train Epoch: 3 [988160/5599780 (18%)]\tLoss: 3.843528\n",
      "Train Epoch: 3 [993280/5599780 (18%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [998400/5599780 (18%)]\tLoss: 3.837826\n",
      "Train Epoch: 3 [1003520/5599780 (18%)]\tLoss: 3.851394\n",
      "Train Epoch: 3 [1008640/5599780 (18%)]\tLoss: 3.841629\n",
      "Train Epoch: 3 [1013760/5599780 (18%)]\tLoss: 3.829109\n",
      "Train Epoch: 3 [1018880/5599780 (18%)]\tLoss: 3.858991\n",
      "Train Epoch: 3 [1024000/5599780 (18%)]\tLoss: 3.818226\n",
      "Train Epoch: 3 [1029120/5599780 (18%)]\tLoss: 3.828918\n",
      "Train Epoch: 3 [1034240/5599780 (18%)]\tLoss: 3.822098\n",
      "Train Epoch: 3 [1039360/5599780 (19%)]\tLoss: 3.824037\n",
      "Train Epoch: 3 [1044480/5599780 (19%)]\tLoss: 3.865065\n",
      "Train Epoch: 3 [1049600/5599780 (19%)]\tLoss: 3.816087\n",
      "Train Epoch: 3 [1054720/5599780 (19%)]\tLoss: 3.827959\n",
      "Train Epoch: 3 [1059840/5599780 (19%)]\tLoss: 3.839625\n",
      "Train Epoch: 3 [1064960/5599780 (19%)]\tLoss: 3.831525\n",
      "Train Epoch: 3 [1070080/5599780 (19%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [1075200/5599780 (19%)]\tLoss: 3.804519\n",
      "Train Epoch: 3 [1080320/5599780 (19%)]\tLoss: 3.849766\n",
      "Train Epoch: 3 [1085440/5599780 (19%)]\tLoss: 3.814360\n",
      "Train Epoch: 3 [1090560/5599780 (19%)]\tLoss: 3.825824\n",
      "Train Epoch: 3 [1095680/5599780 (20%)]\tLoss: 3.794754\n",
      "Train Epoch: 3 [1100800/5599780 (20%)]\tLoss: 3.792870\n",
      "Train Epoch: 3 [1105920/5599780 (20%)]\tLoss: 3.825923\n",
      "Train Epoch: 3 [1111040/5599780 (20%)]\tLoss: 3.794736\n",
      "Train Epoch: 3 [1116160/5599780 (20%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [1121280/5599780 (20%)]\tLoss: 3.786930\n",
      "Train Epoch: 3 [1126400/5599780 (20%)]\tLoss: 3.827959\n",
      "Train Epoch: 3 [1131520/5599780 (20%)]\tLoss: 3.822191\n",
      "Train Epoch: 3 [1136640/5599780 (20%)]\tLoss: 3.818011\n",
      "Train Epoch: 3 [1141760/5599780 (20%)]\tLoss: 3.816242\n",
      "Train Epoch: 3 [1146880/5599780 (20%)]\tLoss: 3.820141\n",
      "Train Epoch: 3 [1152000/5599780 (21%)]\tLoss: 3.857254\n",
      "Train Epoch: 3 [1157120/5599780 (21%)]\tLoss: 3.839880\n",
      "Train Epoch: 3 [1162240/5599780 (21%)]\tLoss: 3.851383\n",
      "Train Epoch: 3 [1167360/5599780 (21%)]\tLoss: 3.816238\n",
      "Train Epoch: 3 [1172480/5599780 (21%)]\tLoss: 3.841305\n",
      "Train Epoch: 3 [1177600/5599780 (21%)]\tLoss: 3.853734\n",
      "Train Epoch: 3 [1182720/5599780 (21%)]\tLoss: 3.806462\n",
      "Train Epoch: 3 [1187840/5599780 (21%)]\tLoss: 3.857241\n",
      "Train Epoch: 3 [1192960/5599780 (21%)]\tLoss: 3.827778\n",
      "Train Epoch: 3 [1198080/5599780 (21%)]\tLoss: 3.857181\n",
      "Train Epoch: 3 [1203200/5599780 (21%)]\tLoss: 3.835769\n",
      "Train Epoch: 3 [1208320/5599780 (22%)]\tLoss: 3.861160\n",
      "Train Epoch: 3 [1213440/5599780 (22%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [1218560/5599780 (22%)]\tLoss: 3.800601\n",
      "Train Epoch: 3 [1223680/5599780 (22%)]\tLoss: 3.827956\n",
      "Train Epoch: 3 [1228800/5599780 (22%)]\tLoss: 3.824077\n",
      "Train Epoch: 3 [1233920/5599780 (22%)]\tLoss: 3.843467\n",
      "Train Epoch: 3 [1239040/5599780 (22%)]\tLoss: 3.829835\n",
      "Train Epoch: 3 [1244160/5599780 (22%)]\tLoss: 3.826004\n",
      "Train Epoch: 3 [1249280/5599780 (22%)]\tLoss: 3.827767\n",
      "Train Epoch: 3 [1254400/5599780 (22%)]\tLoss: 3.874713\n",
      "Train Epoch: 3 [1259520/5599780 (22%)]\tLoss: 3.845535\n",
      "Train Epoch: 3 [1264640/5599780 (23%)]\tLoss: 3.827338\n",
      "Train Epoch: 3 [1269760/5599780 (23%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [1274880/5599780 (23%)]\tLoss: 3.806444\n",
      "Train Epoch: 3 [1280000/5599780 (23%)]\tLoss: 3.839665\n",
      "Train Epoch: 3 [1285120/5599780 (23%)]\tLoss: 3.835374\n",
      "Train Epoch: 3 [1290240/5599780 (23%)]\tLoss: 3.855301\n",
      "Train Epoch: 3 [1295360/5599780 (23%)]\tLoss: 3.812333\n",
      "Train Epoch: 3 [1300480/5599780 (23%)]\tLoss: 3.820300\n",
      "Train Epoch: 3 [1305600/5599780 (23%)]\tLoss: 3.831512\n",
      "Train Epoch: 3 [1310720/5599780 (23%)]\tLoss: 3.835760\n",
      "Train Epoch: 3 [1315840/5599780 (23%)]\tLoss: 3.849367\n",
      "Train Epoch: 3 [1320960/5599780 (24%)]\tLoss: 3.843158\n",
      "Train Epoch: 3 [1326080/5599780 (24%)]\tLoss: 3.797034\n",
      "Train Epoch: 3 [1331200/5599780 (24%)]\tLoss: 3.841630\n",
      "Train Epoch: 3 [1336320/5599780 (24%)]\tLoss: 3.810315\n",
      "Train Epoch: 3 [1341440/5599780 (24%)]\tLoss: 3.824051\n",
      "Train Epoch: 3 [1346560/5599780 (24%)]\tLoss: 3.819897\n",
      "Train Epoch: 3 [1351680/5599780 (24%)]\tLoss: 3.878738\n",
      "Train Epoch: 3 [1356800/5599780 (24%)]\tLoss: 3.831862\n",
      "Train Epoch: 3 [1361920/5599780 (24%)]\tLoss: 3.790785\n",
      "Train Epoch: 3 [1367040/5599780 (24%)]\tLoss: 3.826076\n",
      "Train Epoch: 3 [1372160/5599780 (25%)]\tLoss: 3.859207\n",
      "Train Epoch: 3 [1377280/5599780 (25%)]\tLoss: 3.873322\n",
      "Train Epoch: 3 [1382400/5599780 (25%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [1387520/5599780 (25%)]\tLoss: 3.845083\n",
      "Train Epoch: 3 [1392640/5599780 (25%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [1397760/5599780 (25%)]\tLoss: 3.837721\n",
      "Train Epoch: 3 [1402880/5599780 (25%)]\tLoss: 3.820144\n",
      "Train Epoch: 3 [1408000/5599780 (25%)]\tLoss: 3.822062\n",
      "Train Epoch: 3 [1413120/5599780 (25%)]\tLoss: 3.841632\n",
      "Train Epoch: 3 [1418240/5599780 (25%)]\tLoss: 3.855056\n",
      "Train Epoch: 3 [1423360/5599780 (25%)]\tLoss: 3.823640\n",
      "Train Epoch: 3 [1428480/5599780 (26%)]\tLoss: 3.829921\n",
      "Train Epoch: 3 [1433600/5599780 (26%)]\tLoss: 3.843545\n",
      "Train Epoch: 3 [1438720/5599780 (26%)]\tLoss: 3.824055\n",
      "Train Epoch: 3 [1443840/5599780 (26%)]\tLoss: 3.821692\n",
      "Train Epoch: 3 [1448960/5599780 (26%)]\tLoss: 3.841604\n",
      "Train Epoch: 3 [1454080/5599780 (26%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [1459200/5599780 (26%)]\tLoss: 3.829910\n",
      "Train Epoch: 3 [1464320/5599780 (26%)]\tLoss: 3.842271\n",
      "Train Epoch: 3 [1469440/5599780 (26%)]\tLoss: 3.836002\n",
      "Train Epoch: 3 [1474560/5599780 (26%)]\tLoss: 3.860726\n",
      "Train Epoch: 3 [1479680/5599780 (26%)]\tLoss: 3.823819\n",
      "Train Epoch: 3 [1484800/5599780 (27%)]\tLoss: 3.792802\n",
      "Train Epoch: 3 [1489920/5599780 (27%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [1495040/5599780 (27%)]\tLoss: 3.826007\n",
      "Train Epoch: 3 [1500160/5599780 (27%)]\tLoss: 3.878736\n",
      "Train Epoch: 3 [1505280/5599780 (27%)]\tLoss: 3.855306\n",
      "Train Epoch: 3 [1510400/5599780 (27%)]\tLoss: 3.827957\n",
      "Train Epoch: 3 [1515520/5599780 (27%)]\tLoss: 3.810568\n",
      "Train Epoch: 3 [1520640/5599780 (27%)]\tLoss: 3.856707\n",
      "Train Epoch: 3 [1525760/5599780 (27%)]\tLoss: 3.867019\n",
      "Train Epoch: 3 [1530880/5599780 (27%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [1536000/5599780 (27%)]\tLoss: 3.804519\n",
      "Train Epoch: 3 [1541120/5599780 (28%)]\tLoss: 3.806472\n",
      "Train Epoch: 3 [1546240/5599780 (28%)]\tLoss: 3.783035\n",
      "Train Epoch: 3 [1551360/5599780 (28%)]\tLoss: 3.845535\n",
      "Train Epoch: 3 [1556480/5599780 (28%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [1561600/5599780 (28%)]\tLoss: 3.800465\n",
      "Train Epoch: 3 [1566720/5599780 (28%)]\tLoss: 3.806616\n",
      "Train Epoch: 3 [1571840/5599780 (28%)]\tLoss: 3.861160\n",
      "Train Epoch: 3 [1576960/5599780 (28%)]\tLoss: 3.837396\n",
      "Train Epoch: 3 [1582080/5599780 (28%)]\tLoss: 3.851394\n",
      "Train Epoch: 3 [1587200/5599780 (28%)]\tLoss: 3.821907\n",
      "Train Epoch: 3 [1592320/5599780 (28%)]\tLoss: 3.874832\n",
      "Train Epoch: 3 [1597440/5599780 (29%)]\tLoss: 3.831190\n",
      "Train Epoch: 3 [1602560/5599780 (29%)]\tLoss: 3.850938\n",
      "Train Epoch: 3 [1607680/5599780 (29%)]\tLoss: 3.833827\n",
      "Train Epoch: 3 [1612800/5599780 (29%)]\tLoss: 3.847488\n",
      "Train Epoch: 3 [1617920/5599780 (29%)]\tLoss: 3.868972\n",
      "Train Epoch: 3 [1623040/5599780 (29%)]\tLoss: 3.845535\n",
      "Train Epoch: 3 [1628160/5599780 (29%)]\tLoss: 3.814285\n",
      "Train Epoch: 3 [1633280/5599780 (29%)]\tLoss: 3.831862\n",
      "Train Epoch: 3 [1638400/5599780 (29%)]\tLoss: 3.859190\n",
      "Train Epoch: 3 [1643520/5599780 (29%)]\tLoss: 3.806472\n",
      "Train Epoch: 3 [1648640/5599780 (29%)]\tLoss: 3.841629\n",
      "Train Epoch: 3 [1653760/5599780 (30%)]\tLoss: 3.847490\n",
      "Train Epoch: 3 [1658880/5599780 (30%)]\tLoss: 3.820088\n",
      "Train Epoch: 3 [1664000/5599780 (30%)]\tLoss: 3.832400\n",
      "Train Epoch: 3 [1669120/5599780 (30%)]\tLoss: 3.829913\n",
      "Train Epoch: 3 [1674240/5599780 (30%)]\tLoss: 3.827570\n",
      "Train Epoch: 3 [1679360/5599780 (30%)]\tLoss: 3.802671\n",
      "Train Epoch: 3 [1684480/5599780 (30%)]\tLoss: 3.859207\n",
      "Train Epoch: 3 [1689600/5599780 (30%)]\tLoss: 3.788894\n",
      "Train Epoch: 3 [1694720/5599780 (30%)]\tLoss: 3.805834\n",
      "Train Epoch: 3 [1699840/5599780 (30%)]\tLoss: 3.816231\n",
      "Train Epoch: 3 [1704960/5599780 (30%)]\tLoss: 3.841563\n",
      "Train Epoch: 3 [1710080/5599780 (31%)]\tLoss: 3.794727\n",
      "Train Epoch: 3 [1715200/5599780 (31%)]\tLoss: 3.833812\n",
      "Train Epoch: 3 [1720320/5599780 (31%)]\tLoss: 3.865066\n",
      "Train Epoch: 3 [1725440/5599780 (31%)]\tLoss: 3.826003\n",
      "Train Epoch: 3 [1730560/5599780 (31%)]\tLoss: 3.827957\n",
      "Train Epoch: 3 [1735680/5599780 (31%)]\tLoss: 3.849441\n",
      "Train Epoch: 3 [1740800/5599780 (31%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [1745920/5599780 (31%)]\tLoss: 3.838021\n",
      "Train Epoch: 3 [1751040/5599780 (31%)]\tLoss: 3.763509\n",
      "Train Epoch: 3 [1756160/5599780 (31%)]\tLoss: 3.824042\n",
      "Train Epoch: 3 [1761280/5599780 (31%)]\tLoss: 3.825830\n",
      "Train Epoch: 3 [1766400/5599780 (32%)]\tLoss: 3.835768\n",
      "Train Epoch: 3 [1771520/5599780 (32%)]\tLoss: 3.870928\n",
      "Train Epoch: 3 [1776640/5599780 (32%)]\tLoss: 3.843582\n",
      "Train Epoch: 3 [1781760/5599780 (32%)]\tLoss: 3.849556\n",
      "Train Epoch: 3 [1786880/5599780 (32%)]\tLoss: 3.820179\n",
      "Train Epoch: 3 [1792000/5599780 (32%)]\tLoss: 3.811946\n",
      "Train Epoch: 3 [1797120/5599780 (32%)]\tLoss: 3.852382\n",
      "Train Epoch: 3 [1802240/5599780 (32%)]\tLoss: 3.869078\n",
      "Train Epoch: 3 [1807360/5599780 (32%)]\tLoss: 3.868973\n",
      "Train Epoch: 3 [1812480/5599780 (32%)]\tLoss: 3.837281\n",
      "Train Epoch: 3 [1817600/5599780 (32%)]\tLoss: 3.838240\n",
      "Train Epoch: 3 [1822720/5599780 (33%)]\tLoss: 3.848019\n",
      "Train Epoch: 3 [1827840/5599780 (33%)]\tLoss: 3.835794\n",
      "Train Epoch: 3 [1832960/5599780 (33%)]\tLoss: 3.813848\n",
      "Train Epoch: 3 [1838080/5599780 (33%)]\tLoss: 3.833817\n",
      "Train Epoch: 3 [1843200/5599780 (33%)]\tLoss: 3.806424\n",
      "Train Epoch: 3 [1848320/5599780 (33%)]\tLoss: 3.872850\n",
      "Train Epoch: 3 [1853440/5599780 (33%)]\tLoss: 3.834120\n",
      "Train Epoch: 3 [1858560/5599780 (33%)]\tLoss: 3.847407\n",
      "Train Epoch: 3 [1863680/5599780 (33%)]\tLoss: 3.806473\n",
      "Train Epoch: 3 [1868800/5599780 (33%)]\tLoss: 3.825676\n",
      "Train Epoch: 3 [1873920/5599780 (33%)]\tLoss: 3.841629\n",
      "Train Epoch: 3 [1879040/5599780 (34%)]\tLoss: 3.857244\n",
      "Train Epoch: 3 [1884160/5599780 (34%)]\tLoss: 3.820121\n",
      "Train Epoch: 3 [1889280/5599780 (34%)]\tLoss: 3.845586\n",
      "Train Epoch: 3 [1894400/5599780 (34%)]\tLoss: 3.820144\n",
      "Train Epoch: 3 [1899520/5599780 (34%)]\tLoss: 3.827953\n",
      "Train Epoch: 3 [1904640/5599780 (34%)]\tLoss: 3.842996\n",
      "Train Epoch: 3 [1909760/5599780 (34%)]\tLoss: 3.814285\n",
      "Train Epoch: 3 [1914880/5599780 (34%)]\tLoss: 3.825786\n",
      "Train Epoch: 3 [1920000/5599780 (34%)]\tLoss: 3.826101\n",
      "Train Epoch: 3 [1925120/5599780 (34%)]\tLoss: 3.872879\n",
      "Train Epoch: 3 [1930240/5599780 (34%)]\tLoss: 3.826001\n",
      "Train Epoch: 3 [1935360/5599780 (35%)]\tLoss: 3.845529\n",
      "Train Epoch: 3 [1940480/5599780 (35%)]\tLoss: 3.816094\n",
      "Train Epoch: 3 [1945600/5599780 (35%)]\tLoss: 3.798707\n",
      "Train Epoch: 3 [1950720/5599780 (35%)]\tLoss: 3.820258\n",
      "Train Epoch: 3 [1955840/5599780 (35%)]\tLoss: 3.843585\n",
      "Train Epoch: 3 [1960960/5599780 (35%)]\tLoss: 3.821899\n",
      "Train Epoch: 3 [1966080/5599780 (35%)]\tLoss: 3.849442\n",
      "Train Epoch: 3 [1971200/5599780 (35%)]\tLoss: 3.865100\n",
      "Train Epoch: 3 [1976320/5599780 (35%)]\tLoss: 3.818762\n",
      "Train Epoch: 3 [1981440/5599780 (35%)]\tLoss: 3.856817\n",
      "Train Epoch: 3 [1986560/5599780 (35%)]\tLoss: 3.816239\n",
      "Train Epoch: 3 [1991680/5599780 (36%)]\tLoss: 3.816237\n",
      "Train Epoch: 3 [1996800/5599780 (36%)]\tLoss: 3.827560\n",
      "Train Epoch: 3 [2001920/5599780 (36%)]\tLoss: 3.824048\n",
      "Train Epoch: 3 [2007040/5599780 (36%)]\tLoss: 3.830549\n",
      "Train Epoch: 3 [2012160/5599780 (36%)]\tLoss: 3.847391\n",
      "Train Epoch: 3 [2017280/5599780 (36%)]\tLoss: 3.827957\n",
      "Train Epoch: 3 [2022400/5599780 (36%)]\tLoss: 3.817980\n",
      "Train Epoch: 3 [2027520/5599780 (36%)]\tLoss: 3.820936\n",
      "Train Epoch: 3 [2032640/5599780 (36%)]\tLoss: 3.822097\n",
      "Train Epoch: 3 [2037760/5599780 (36%)]\tLoss: 3.822096\n",
      "Train Epoch: 3 [2042880/5599780 (36%)]\tLoss: 3.851388\n",
      "Train Epoch: 3 [2048000/5599780 (37%)]\tLoss: 3.835770\n",
      "Train Epoch: 3 [2053120/5599780 (37%)]\tLoss: 3.841629\n",
      "Train Epoch: 3 [2058240/5599780 (37%)]\tLoss: 3.849395\n",
      "Train Epoch: 3 [2063360/5599780 (37%)]\tLoss: 3.822138\n",
      "Train Epoch: 3 [2068480/5599780 (37%)]\tLoss: 3.861160\n",
      "Train Epoch: 3 [2073600/5599780 (37%)]\tLoss: 3.833322\n",
      "Train Epoch: 3 [2078720/5599780 (37%)]\tLoss: 3.818446\n",
      "Train Epoch: 3 [2083840/5599780 (37%)]\tLoss: 3.837666\n",
      "Train Epoch: 3 [2088960/5599780 (37%)]\tLoss: 3.835634\n",
      "Train Epoch: 3 [2094080/5599780 (37%)]\tLoss: 3.831858\n",
      "Train Epoch: 3 [2099200/5599780 (37%)]\tLoss: 3.804521\n",
      "Train Epoch: 3 [2104320/5599780 (38%)]\tLoss: 3.837723\n",
      "Train Epoch: 3 [2109440/5599780 (38%)]\tLoss: 3.839674\n",
      "Train Epoch: 3 [2114560/5599780 (38%)]\tLoss: 3.862485\n",
      "Train Epoch: 3 [2119680/5599780 (38%)]\tLoss: 3.798656\n",
      "Train Epoch: 3 [2124800/5599780 (38%)]\tLoss: 3.819794\n",
      "Train Epoch: 3 [2129920/5599780 (38%)]\tLoss: 3.820972\n",
      "Train Epoch: 3 [2135040/5599780 (38%)]\tLoss: 3.804510\n",
      "Train Epoch: 3 [2140160/5599780 (38%)]\tLoss: 3.849441\n",
      "Train Epoch: 3 [2145280/5599780 (38%)]\tLoss: 3.841560\n",
      "Train Epoch: 3 [2150400/5599780 (38%)]\tLoss: 3.843305\n",
      "Train Epoch: 3 [2155520/5599780 (38%)]\tLoss: 3.849427\n",
      "Train Epoch: 3 [2160640/5599780 (39%)]\tLoss: 3.859154\n",
      "Train Epoch: 3 [2165760/5599780 (39%)]\tLoss: 3.816228\n",
      "Train Epoch: 3 [2170880/5599780 (39%)]\tLoss: 3.837722\n",
      "Train Epoch: 3 [2176000/5599780 (39%)]\tLoss: 3.812229\n",
      "Train Epoch: 3 [2181120/5599780 (39%)]\tLoss: 3.839549\n",
      "Train Epoch: 3 [2186240/5599780 (39%)]\tLoss: 3.804511\n",
      "Train Epoch: 3 [2191360/5599780 (39%)]\tLoss: 3.829912\n",
      "Train Epoch: 3 [2196480/5599780 (39%)]\tLoss: 3.790868\n",
      "Train Epoch: 3 [2201600/5599780 (39%)]\tLoss: 3.829901\n",
      "Train Epoch: 3 [2206720/5599780 (39%)]\tLoss: 3.824007\n",
      "Train Epoch: 3 [2211840/5599780 (39%)]\tLoss: 3.827956\n",
      "Train Epoch: 3 [2216960/5599780 (40%)]\tLoss: 3.827936\n",
      "Train Epoch: 3 [2222080/5599780 (40%)]\tLoss: 3.808644\n",
      "Train Epoch: 3 [2227200/5599780 (40%)]\tLoss: 3.874808\n",
      "Train Epoch: 3 [2232320/5599780 (40%)]\tLoss: 3.792266\n",
      "Train Epoch: 3 [2237440/5599780 (40%)]\tLoss: 3.849053\n",
      "Train Epoch: 3 [2242560/5599780 (40%)]\tLoss: 3.810491\n",
      "Train Epoch: 3 [2247680/5599780 (40%)]\tLoss: 3.824420\n",
      "Train Epoch: 3 [2252800/5599780 (40%)]\tLoss: 3.824063\n",
      "Train Epoch: 3 [2257920/5599780 (40%)]\tLoss: 3.822349\n",
      "Train Epoch: 3 [2263040/5599780 (40%)]\tLoss: 3.863114\n",
      "Train Epoch: 3 [2268160/5599780 (41%)]\tLoss: 3.812268\n",
      "Train Epoch: 3 [2273280/5599780 (41%)]\tLoss: 3.847599\n",
      "Train Epoch: 3 [2278400/5599780 (41%)]\tLoss: 3.811090\n",
      "Train Epoch: 3 [2283520/5599780 (41%)]\tLoss: 3.787187\n",
      "Train Epoch: 3 [2288640/5599780 (41%)]\tLoss: 3.826003\n",
      "Train Epoch: 3 [2293760/5599780 (41%)]\tLoss: 3.820095\n",
      "Train Epoch: 3 [2298880/5599780 (41%)]\tLoss: 3.824051\n",
      "Train Epoch: 3 [2304000/5599780 (41%)]\tLoss: 3.836490\n",
      "Train Epoch: 3 [2309120/5599780 (41%)]\tLoss: 3.820643\n",
      "Train Epoch: 3 [2314240/5599780 (41%)]\tLoss: 3.824008\n",
      "Train Epoch: 3 [2319360/5599780 (41%)]\tLoss: 3.819600\n",
      "Train Epoch: 3 [2324480/5599780 (42%)]\tLoss: 3.835751\n",
      "Train Epoch: 3 [2329600/5599780 (42%)]\tLoss: 3.818903\n",
      "Train Epoch: 3 [2334720/5599780 (42%)]\tLoss: 3.825546\n",
      "Train Epoch: 3 [2339840/5599780 (42%)]\tLoss: 3.831812\n",
      "Train Epoch: 3 [2344960/5599780 (42%)]\tLoss: 3.800974\n",
      "Train Epoch: 3 [2350080/5599780 (42%)]\tLoss: 3.847488\n",
      "Train Epoch: 3 [2355200/5599780 (42%)]\tLoss: 3.845477\n",
      "Train Epoch: 3 [2360320/5599780 (42%)]\tLoss: 3.855842\n",
      "Train Epoch: 3 [2365440/5599780 (42%)]\tLoss: 3.847265\n",
      "Train Epoch: 3 [2370560/5599780 (42%)]\tLoss: 3.851397\n",
      "Train Epoch: 3 [2375680/5599780 (42%)]\tLoss: 3.827774\n",
      "Train Epoch: 3 [2380800/5599780 (43%)]\tLoss: 3.786798\n",
      "Train Epoch: 3 [2385920/5599780 (43%)]\tLoss: 3.861160\n",
      "Train Epoch: 3 [2391040/5599780 (43%)]\tLoss: 3.819875\n",
      "Train Epoch: 3 [2396160/5599780 (43%)]\tLoss: 3.796388\n",
      "Train Epoch: 3 [2401280/5599780 (43%)]\tLoss: 3.892182\n",
      "Train Epoch: 3 [2406400/5599780 (43%)]\tLoss: 3.841697\n",
      "Train Epoch: 3 [2411520/5599780 (43%)]\tLoss: 3.833816\n",
      "Train Epoch: 3 [2416640/5599780 (43%)]\tLoss: 3.853347\n",
      "Train Epoch: 3 [2421760/5599780 (43%)]\tLoss: 3.843713\n",
      "Train Epoch: 3 [2426880/5599780 (43%)]\tLoss: 3.802411\n",
      "Train Epoch: 3 [2432000/5599780 (43%)]\tLoss: 3.839676\n",
      "Train Epoch: 3 [2437120/5599780 (44%)]\tLoss: 3.803643\n",
      "Train Epoch: 3 [2442240/5599780 (44%)]\tLoss: 3.829956\n",
      "Train Epoch: 3 [2447360/5599780 (44%)]\tLoss: 3.841628\n",
      "Train Epoch: 3 [2452480/5599780 (44%)]\tLoss: 3.831868\n",
      "Train Epoch: 3 [2457600/5599780 (44%)]\tLoss: 3.872870\n",
      "Train Epoch: 3 [2462720/5599780 (44%)]\tLoss: 3.822277\n",
      "Train Epoch: 3 [2467840/5599780 (44%)]\tLoss: 3.839585\n",
      "Train Epoch: 3 [2472960/5599780 (44%)]\tLoss: 3.806459\n",
      "Train Epoch: 3 [2478080/5599780 (44%)]\tLoss: 3.793849\n",
      "Train Epoch: 3 [2483200/5599780 (44%)]\tLoss: 3.804470\n",
      "Train Epoch: 3 [2488320/5599780 (44%)]\tLoss: 3.839622\n",
      "Train Epoch: 3 [2493440/5599780 (45%)]\tLoss: 3.805457\n",
      "Train Epoch: 3 [2498560/5599780 (45%)]\tLoss: 3.789610\n",
      "Train Epoch: 3 [2503680/5599780 (45%)]\tLoss: 3.847494\n",
      "Train Epoch: 3 [2508800/5599780 (45%)]\tLoss: 3.855279\n",
      "Train Epoch: 3 [2513920/5599780 (45%)]\tLoss: 3.855301\n",
      "Train Epoch: 3 [2519040/5599780 (45%)]\tLoss: 3.861338\n",
      "Train Epoch: 3 [2524160/5599780 (45%)]\tLoss: 3.839884\n",
      "Train Epoch: 3 [2529280/5599780 (45%)]\tLoss: 3.804343\n",
      "Train Epoch: 3 [2534400/5599780 (45%)]\tLoss: 3.840205\n",
      "Train Epoch: 3 [2539520/5599780 (45%)]\tLoss: 3.849422\n",
      "Train Epoch: 3 [2544640/5599780 (45%)]\tLoss: 3.822284\n",
      "Train Epoch: 3 [2549760/5599780 (46%)]\tLoss: 3.845541\n",
      "Train Epoch: 3 [2554880/5599780 (46%)]\tLoss: 3.831863\n",
      "Train Epoch: 3 [2560000/5599780 (46%)]\tLoss: 3.851426\n",
      "Train Epoch: 3 [2565120/5599780 (46%)]\tLoss: 3.823665\n",
      "Train Epoch: 3 [2570240/5599780 (46%)]\tLoss: 3.808475\n",
      "Train Epoch: 3 [2575360/5599780 (46%)]\tLoss: 3.865088\n",
      "Train Epoch: 3 [2580480/5599780 (46%)]\tLoss: 3.855393\n",
      "Train Epoch: 3 [2585600/5599780 (46%)]\tLoss: 3.861160\n",
      "Train Epoch: 3 [2590720/5599780 (46%)]\tLoss: 3.851394\n",
      "Train Epoch: 3 [2595840/5599780 (46%)]\tLoss: 3.818301\n",
      "Train Epoch: 3 [2600960/5599780 (46%)]\tLoss: 3.842396\n",
      "Train Epoch: 3 [2606080/5599780 (47%)]\tLoss: 3.843409\n",
      "Train Epoch: 3 [2611200/5599780 (47%)]\tLoss: 3.849411\n",
      "Train Epoch: 3 [2616320/5599780 (47%)]\tLoss: 3.826046\n",
      "Train Epoch: 3 [2621440/5599780 (47%)]\tLoss: 3.843570\n",
      "Train Epoch: 3 [2626560/5599780 (47%)]\tLoss: 3.786699\n",
      "Train Epoch: 3 [2631680/5599780 (47%)]\tLoss: 3.819677\n",
      "Train Epoch: 3 [2636800/5599780 (47%)]\tLoss: 3.810380\n",
      "Train Epoch: 3 [2641920/5599780 (47%)]\tLoss: 3.816627\n",
      "Train Epoch: 3 [2647040/5599780 (47%)]\tLoss: 3.829587\n",
      "Train Epoch: 3 [2652160/5599780 (47%)]\tLoss: 3.815998\n",
      "Train Epoch: 3 [2657280/5599780 (47%)]\tLoss: 3.816238\n",
      "Train Epoch: 3 [2662400/5599780 (48%)]\tLoss: 3.843604\n",
      "Train Epoch: 3 [2667520/5599780 (48%)]\tLoss: 3.837935\n",
      "Train Epoch: 3 [2672640/5599780 (48%)]\tLoss: 3.835147\n",
      "Train Epoch: 3 [2677760/5599780 (48%)]\tLoss: 3.849433\n",
      "Train Epoch: 3 [2682880/5599780 (48%)]\tLoss: 3.837678\n",
      "Train Epoch: 3 [2688000/5599780 (48%)]\tLoss: 3.834115\n",
      "Train Epoch: 3 [2693120/5599780 (48%)]\tLoss: 3.870942\n",
      "Train Epoch: 3 [2698240/5599780 (48%)]\tLoss: 3.823410\n",
      "Train Epoch: 3 [2703360/5599780 (48%)]\tLoss: 3.798413\n",
      "Train Epoch: 3 [2708480/5599780 (48%)]\tLoss: 3.794892\n",
      "Train Epoch: 3 [2713600/5599780 (48%)]\tLoss: 3.808867\n",
      "Train Epoch: 3 [2718720/5599780 (49%)]\tLoss: 3.861182\n",
      "Train Epoch: 3 [2723840/5599780 (49%)]\tLoss: 3.855418\n",
      "Train Epoch: 3 [2728960/5599780 (49%)]\tLoss: 3.833141\n",
      "Train Epoch: 3 [2734080/5599780 (49%)]\tLoss: 3.796648\n",
      "Train Epoch: 3 [2739200/5599780 (49%)]\tLoss: 3.851106\n",
      "Train Epoch: 3 [2744320/5599780 (49%)]\tLoss: 3.816165\n",
      "Train Epoch: 3 [2749440/5599780 (49%)]\tLoss: 3.832285\n",
      "Train Epoch: 3 [2754560/5599780 (49%)]\tLoss: 3.883076\n",
      "Train Epoch: 3 [2759680/5599780 (49%)]\tLoss: 3.861128\n",
      "Train Epoch: 3 [2764800/5599780 (49%)]\tLoss: 3.823699\n",
      "Train Epoch: 3 [2769920/5599780 (49%)]\tLoss: 3.847220\n",
      "Train Epoch: 3 [2775040/5599780 (50%)]\tLoss: 3.829548\n",
      "Train Epoch: 3 [2780160/5599780 (50%)]\tLoss: 3.860844\n",
      "Train Epoch: 3 [2785280/5599780 (50%)]\tLoss: 3.812920\n",
      "Train Epoch: 3 [2790400/5599780 (50%)]\tLoss: 3.853464\n",
      "Train Epoch: 3 [2795520/5599780 (50%)]\tLoss: 3.818341\n",
      "Train Epoch: 3 [2800640/5599780 (50%)]\tLoss: 3.883434\n",
      "Train Epoch: 3 [2805760/5599780 (50%)]\tLoss: 3.846408\n",
      "Train Epoch: 3 [2810880/5599780 (50%)]\tLoss: 3.813967\n",
      "Train Epoch: 3 [2816000/5599780 (50%)]\tLoss: 3.805124\n",
      "Train Epoch: 3 [2821120/5599780 (50%)]\tLoss: 3.825787\n",
      "Train Epoch: 3 [2826240/5599780 (50%)]\tLoss: 3.835646\n",
      "Train Epoch: 3 [2831360/5599780 (51%)]\tLoss: 3.851850\n",
      "Train Epoch: 3 [2836480/5599780 (51%)]\tLoss: 3.707777\n",
      "Train Epoch: 3 [2841600/5599780 (51%)]\tLoss: 3.712509\n",
      "Train Epoch: 3 [2846720/5599780 (51%)]\tLoss: 3.746157\n",
      "Train Epoch: 3 [2851840/5599780 (51%)]\tLoss: 3.687152\n",
      "Train Epoch: 3 [2856960/5599780 (51%)]\tLoss: 3.709745\n",
      "Train Epoch: 3 [2862080/5599780 (51%)]\tLoss: 3.710023\n",
      "Train Epoch: 3 [2867200/5599780 (51%)]\tLoss: 3.727471\n",
      "Train Epoch: 3 [2872320/5599780 (51%)]\tLoss: 3.665082\n",
      "Train Epoch: 3 [2877440/5599780 (51%)]\tLoss: 3.714149\n",
      "Train Epoch: 3 [2882560/5599780 (51%)]\tLoss: 3.714799\n",
      "Train Epoch: 3 [2887680/5599780 (52%)]\tLoss: 3.747568\n",
      "Train Epoch: 3 [2892800/5599780 (52%)]\tLoss: 3.723581\n",
      "Train Epoch: 3 [2897920/5599780 (52%)]\tLoss: 3.694887\n",
      "Train Epoch: 3 [2903040/5599780 (52%)]\tLoss: 3.687132\n",
      "Train Epoch: 3 [2908160/5599780 (52%)]\tLoss: 3.760607\n",
      "Train Epoch: 3 [2913280/5599780 (52%)]\tLoss: 3.730493\n",
      "Train Epoch: 3 [2918400/5599780 (52%)]\tLoss: 3.741863\n",
      "Train Epoch: 3 [2923520/5599780 (52%)]\tLoss: 3.677931\n",
      "Train Epoch: 3 [2928640/5599780 (52%)]\tLoss: 3.705310\n",
      "Train Epoch: 3 [2933760/5599780 (52%)]\tLoss: 3.689592\n",
      "Train Epoch: 3 [2938880/5599780 (52%)]\tLoss: 3.735618\n",
      "Train Epoch: 3 [2944000/5599780 (53%)]\tLoss: 3.710868\n",
      "Train Epoch: 3 [2949120/5599780 (53%)]\tLoss: 3.728261\n",
      "Train Epoch: 3 [2954240/5599780 (53%)]\tLoss: 3.719282\n",
      "Train Epoch: 3 [2959360/5599780 (53%)]\tLoss: 3.746217\n",
      "Train Epoch: 3 [2964480/5599780 (53%)]\tLoss: 3.697473\n",
      "Train Epoch: 3 [2969600/5599780 (53%)]\tLoss: 3.710502\n",
      "Train Epoch: 3 [2974720/5599780 (53%)]\tLoss: 3.726119\n",
      "Train Epoch: 3 [2979840/5599780 (53%)]\tLoss: 3.702481\n",
      "Train Epoch: 3 [2984960/5599780 (53%)]\tLoss: 3.746043\n",
      "Train Epoch: 3 [2990080/5599780 (53%)]\tLoss: 3.724128\n",
      "Train Epoch: 3 [2995200/5599780 (53%)]\tLoss: 3.708398\n",
      "Train Epoch: 3 [3000320/5599780 (54%)]\tLoss: 3.732234\n",
      "Train Epoch: 3 [3005440/5599780 (54%)]\tLoss: 3.704772\n",
      "Train Epoch: 3 [3010560/5599780 (54%)]\tLoss: 3.693532\n",
      "Train Epoch: 3 [3015680/5599780 (54%)]\tLoss: 3.703345\n",
      "Train Epoch: 3 [3020800/5599780 (54%)]\tLoss: 3.734624\n",
      "Train Epoch: 3 [3025920/5599780 (54%)]\tLoss: 3.696856\n",
      "Train Epoch: 3 [3031040/5599780 (54%)]\tLoss: 3.710267\n",
      "Train Epoch: 3 [3036160/5599780 (54%)]\tLoss: 3.712078\n",
      "Train Epoch: 3 [3041280/5599780 (54%)]\tLoss: 3.716573\n",
      "Train Epoch: 3 [3046400/5599780 (54%)]\tLoss: 3.691544\n",
      "Train Epoch: 3 [3051520/5599780 (54%)]\tLoss: 3.717793\n",
      "Train Epoch: 3 [3056640/5599780 (55%)]\tLoss: 3.685951\n",
      "Train Epoch: 3 [3061760/5599780 (55%)]\tLoss: 3.735927\n",
      "Train Epoch: 3 [3066880/5599780 (55%)]\tLoss: 3.702117\n",
      "Train Epoch: 3 [3072000/5599780 (55%)]\tLoss: 3.681522\n",
      "Train Epoch: 3 [3077120/5599780 (55%)]\tLoss: 3.693347\n",
      "Train Epoch: 3 [3082240/5599780 (55%)]\tLoss: 3.667768\n",
      "Train Epoch: 3 [3087360/5599780 (55%)]\tLoss: 3.700474\n",
      "Train Epoch: 3 [3092480/5599780 (55%)]\tLoss: 3.708134\n",
      "Train Epoch: 3 [3097600/5599780 (55%)]\tLoss: 3.714796\n",
      "Train Epoch: 3 [3102720/5599780 (55%)]\tLoss: 3.710725\n",
      "Train Epoch: 3 [3107840/5599780 (55%)]\tLoss: 3.705647\n",
      "Train Epoch: 3 [3112960/5599780 (56%)]\tLoss: 3.726537\n",
      "Train Epoch: 3 [3118080/5599780 (56%)]\tLoss: 3.757250\n",
      "Train Epoch: 3 [3123200/5599780 (56%)]\tLoss: 3.674289\n",
      "Train Epoch: 3 [3128320/5599780 (56%)]\tLoss: 3.720482\n",
      "Train Epoch: 3 [3133440/5599780 (56%)]\tLoss: 3.731100\n",
      "Train Epoch: 3 [3138560/5599780 (56%)]\tLoss: 3.709013\n",
      "Train Epoch: 3 [3143680/5599780 (56%)]\tLoss: 3.716828\n",
      "Train Epoch: 3 [3148800/5599780 (56%)]\tLoss: 3.723478\n",
      "Train Epoch: 3 [3153920/5599780 (56%)]\tLoss: 3.705196\n",
      "Train Epoch: 3 [3159040/5599780 (56%)]\tLoss: 3.685046\n",
      "Train Epoch: 3 [3164160/5599780 (57%)]\tLoss: 3.735286\n",
      "Train Epoch: 3 [3169280/5599780 (57%)]\tLoss: 3.737429\n",
      "Train Epoch: 3 [3174400/5599780 (57%)]\tLoss: 3.720184\n",
      "Train Epoch: 3 [3179520/5599780 (57%)]\tLoss: 3.711381\n",
      "Train Epoch: 3 [3184640/5599780 (57%)]\tLoss: 3.722799\n",
      "Train Epoch: 3 [3189760/5599780 (57%)]\tLoss: 3.703147\n",
      "Train Epoch: 3 [3194880/5599780 (57%)]\tLoss: 3.718441\n",
      "Train Epoch: 3 [3200000/5599780 (57%)]\tLoss: 3.673836\n",
      "Train Epoch: 3 [3205120/5599780 (57%)]\tLoss: 3.693464\n",
      "Train Epoch: 3 [3210240/5599780 (57%)]\tLoss: 3.735912\n",
      "Train Epoch: 3 [3215360/5599780 (57%)]\tLoss: 3.687585\n",
      "Train Epoch: 3 [3220480/5599780 (58%)]\tLoss: 3.726725\n",
      "Train Epoch: 3 [3225600/5599780 (58%)]\tLoss: 3.738451\n",
      "Train Epoch: 3 [3230720/5599780 (58%)]\tLoss: 3.662597\n",
      "Train Epoch: 3 [3235840/5599780 (58%)]\tLoss: 3.701223\n",
      "Train Epoch: 3 [3240960/5599780 (58%)]\tLoss: 3.727836\n",
      "Train Epoch: 3 [3246080/5599780 (58%)]\tLoss: 3.707053\n",
      "Train Epoch: 3 [3251200/5599780 (58%)]\tLoss: 3.699076\n",
      "Train Epoch: 3 [3256320/5599780 (58%)]\tLoss: 3.690303\n",
      "Train Epoch: 3 [3261440/5599780 (58%)]\tLoss: 3.688949\n",
      "Train Epoch: 3 [3266560/5599780 (58%)]\tLoss: 3.731509\n",
      "Train Epoch: 3 [3271680/5599780 (58%)]\tLoss: 3.707290\n",
      "Train Epoch: 3 [3276800/5599780 (59%)]\tLoss: 3.713533\n",
      "Train Epoch: 3 [3281920/5599780 (59%)]\tLoss: 3.721862\n",
      "Train Epoch: 3 [3287040/5599780 (59%)]\tLoss: 3.738801\n",
      "Train Epoch: 3 [3292160/5599780 (59%)]\tLoss: 3.750135\n",
      "Train Epoch: 3 [3297280/5599780 (59%)]\tLoss: 3.713675\n",
      "Train Epoch: 3 [3302400/5599780 (59%)]\tLoss: 3.721046\n",
      "Train Epoch: 3 [3307520/5599780 (59%)]\tLoss: 3.693416\n",
      "Train Epoch: 3 [3312640/5599780 (59%)]\tLoss: 3.711881\n",
      "Train Epoch: 3 [3317760/5599780 (59%)]\tLoss: 3.689643\n",
      "Train Epoch: 3 [3322880/5599780 (59%)]\tLoss: 3.734561\n",
      "Train Epoch: 3 [3328000/5599780 (59%)]\tLoss: 3.736207\n",
      "Train Epoch: 3 [3333120/5599780 (60%)]\tLoss: 3.714404\n",
      "Train Epoch: 3 [3338240/5599780 (60%)]\tLoss: 3.705096\n",
      "Train Epoch: 3 [3343360/5599780 (60%)]\tLoss: 3.692533\n",
      "Train Epoch: 3 [3348480/5599780 (60%)]\tLoss: 3.712739\n",
      "Train Epoch: 3 [3353600/5599780 (60%)]\tLoss: 3.670299\n",
      "Train Epoch: 3 [3358720/5599780 (60%)]\tLoss: 3.707665\n",
      "Train Epoch: 3 [3363840/5599780 (60%)]\tLoss: 3.721510\n",
      "Train Epoch: 3 [3368960/5599780 (60%)]\tLoss: 3.744286\n",
      "Train Epoch: 3 [3374080/5599780 (60%)]\tLoss: 3.718654\n",
      "Train Epoch: 3 [3379200/5599780 (60%)]\tLoss: 3.716601\n",
      "Train Epoch: 3 [3384320/5599780 (60%)]\tLoss: 3.675600\n",
      "Train Epoch: 3 [3389440/5599780 (61%)]\tLoss: 3.740954\n",
      "Train Epoch: 3 [3394560/5599780 (61%)]\tLoss: 3.746213\n",
      "Train Epoch: 3 [3399680/5599780 (61%)]\tLoss: 3.725475\n",
      "Train Epoch: 3 [3404800/5599780 (61%)]\tLoss: 3.674167\n",
      "Train Epoch: 3 [3409920/5599780 (61%)]\tLoss: 3.717399\n",
      "Train Epoch: 3 [3415040/5599780 (61%)]\tLoss: 3.745082\n",
      "Train Epoch: 3 [3420160/5599780 (61%)]\tLoss: 3.721095\n",
      "Train Epoch: 3 [3425280/5599780 (61%)]\tLoss: 3.698636\n",
      "Train Epoch: 3 [3430400/5599780 (61%)]\tLoss: 3.687726\n",
      "Train Epoch: 3 [3435520/5599780 (61%)]\tLoss: 3.706931\n",
      "Train Epoch: 3 [3440640/5599780 (61%)]\tLoss: 3.734053\n",
      "Train Epoch: 3 [3445760/5599780 (62%)]\tLoss: 3.711518\n",
      "Train Epoch: 3 [3450880/5599780 (62%)]\tLoss: 3.684716\n",
      "Train Epoch: 3 [3456000/5599780 (62%)]\tLoss: 3.747619\n",
      "Train Epoch: 3 [3461120/5599780 (62%)]\tLoss: 3.694771\n",
      "Train Epoch: 3 [3466240/5599780 (62%)]\tLoss: 3.734539\n",
      "Train Epoch: 3 [3471360/5599780 (62%)]\tLoss: 3.695241\n",
      "Train Epoch: 3 [3476480/5599780 (62%)]\tLoss: 3.709601\n",
      "Train Epoch: 3 [3481600/5599780 (62%)]\tLoss: 3.722417\n",
      "Train Epoch: 3 [3486720/5599780 (62%)]\tLoss: 3.727577\n",
      "Train Epoch: 3 [3491840/5599780 (62%)]\tLoss: 3.692698\n",
      "Train Epoch: 3 [3496960/5599780 (62%)]\tLoss: 3.695953\n",
      "Train Epoch: 3 [3502080/5599780 (63%)]\tLoss: 3.718242\n",
      "Train Epoch: 3 [3507200/5599780 (63%)]\tLoss: 3.706059\n",
      "Train Epoch: 3 [3512320/5599780 (63%)]\tLoss: 3.698535\n",
      "Train Epoch: 3 [3517440/5599780 (63%)]\tLoss: 3.714013\n",
      "Train Epoch: 3 [3522560/5599780 (63%)]\tLoss: 3.744096\n",
      "Train Epoch: 3 [3527680/5599780 (63%)]\tLoss: 3.736920\n",
      "Train Epoch: 3 [3532800/5599780 (63%)]\tLoss: 3.746304\n",
      "Train Epoch: 3 [3537920/5599780 (63%)]\tLoss: 3.767777\n",
      "Train Epoch: 3 [3543040/5599780 (63%)]\tLoss: 3.702505\n",
      "Train Epoch: 3 [3548160/5599780 (63%)]\tLoss: 3.710561\n",
      "Train Epoch: 3 [3553280/5599780 (63%)]\tLoss: 3.689746\n",
      "Train Epoch: 3 [3558400/5599780 (64%)]\tLoss: 3.712668\n",
      "Train Epoch: 3 [3563520/5599780 (64%)]\tLoss: 3.732545\n",
      "Train Epoch: 3 [3568640/5599780 (64%)]\tLoss: 3.729373\n",
      "Train Epoch: 3 [3573760/5599780 (64%)]\tLoss: 3.687862\n",
      "Train Epoch: 3 [3578880/5599780 (64%)]\tLoss: 3.723914\n",
      "Train Epoch: 3 [3584000/5599780 (64%)]\tLoss: 3.731003\n",
      "Train Epoch: 3 [3589120/5599780 (64%)]\tLoss: 3.762387\n",
      "Train Epoch: 3 [3594240/5599780 (64%)]\tLoss: 3.766718\n",
      "Train Epoch: 3 [3599360/5599780 (64%)]\tLoss: 3.724863\n",
      "Train Epoch: 3 [3604480/5599780 (64%)]\tLoss: 3.734637\n",
      "Train Epoch: 3 [3609600/5599780 (64%)]\tLoss: 3.719492\n",
      "Train Epoch: 3 [3614720/5599780 (65%)]\tLoss: 3.727226\n",
      "Train Epoch: 3 [3619840/5599780 (65%)]\tLoss: 3.712226\n",
      "Train Epoch: 3 [3624960/5599780 (65%)]\tLoss: 3.704210\n",
      "Train Epoch: 3 [3630080/5599780 (65%)]\tLoss: 3.745234\n",
      "Train Epoch: 3 [3635200/5599780 (65%)]\tLoss: 3.728131\n",
      "Train Epoch: 3 [3640320/5599780 (65%)]\tLoss: 3.715123\n",
      "Train Epoch: 3 [3645440/5599780 (65%)]\tLoss: 3.723429\n",
      "Train Epoch: 3 [3650560/5599780 (65%)]\tLoss: 3.693393\n",
      "Train Epoch: 3 [3655680/5599780 (65%)]\tLoss: 3.772530\n",
      "Train Epoch: 3 [3660800/5599780 (65%)]\tLoss: 3.742845\n",
      "Train Epoch: 3 [3665920/5599780 (65%)]\tLoss: 3.725532\n",
      "Train Epoch: 3 [3671040/5599780 (66%)]\tLoss: 3.683136\n",
      "Train Epoch: 3 [3676160/5599780 (66%)]\tLoss: 3.745699\n",
      "Train Epoch: 3 [3681280/5599780 (66%)]\tLoss: 3.731626\n",
      "Train Epoch: 3 [3686400/5599780 (66%)]\tLoss: 3.734051\n",
      "Train Epoch: 3 [3691520/5599780 (66%)]\tLoss: 3.724161\n",
      "Train Epoch: 3 [3696640/5599780 (66%)]\tLoss: 3.732949\n",
      "Train Epoch: 3 [3701760/5599780 (66%)]\tLoss: 3.675123\n",
      "Train Epoch: 3 [3706880/5599780 (66%)]\tLoss: 3.732320\n",
      "Train Epoch: 3 [3712000/5599780 (66%)]\tLoss: 3.733854\n",
      "Train Epoch: 3 [3717120/5599780 (66%)]\tLoss: 3.692889\n",
      "Train Epoch: 3 [3722240/5599780 (66%)]\tLoss: 3.736552\n",
      "Train Epoch: 3 [3727360/5599780 (67%)]\tLoss: 3.753788\n",
      "Train Epoch: 3 [3732480/5599780 (67%)]\tLoss: 3.711993\n",
      "Train Epoch: 3 [3737600/5599780 (67%)]\tLoss: 3.718987\n",
      "Train Epoch: 3 [3742720/5599780 (67%)]\tLoss: 3.744078\n",
      "Train Epoch: 3 [3747840/5599780 (67%)]\tLoss: 3.696715\n",
      "Train Epoch: 3 [3752960/5599780 (67%)]\tLoss: 3.759914\n",
      "Train Epoch: 3 [3758080/5599780 (67%)]\tLoss: 3.719214\n",
      "Train Epoch: 3 [3763200/5599780 (67%)]\tLoss: 3.704875\n",
      "Train Epoch: 3 [3768320/5599780 (67%)]\tLoss: 3.711106\n",
      "Train Epoch: 3 [3773440/5599780 (67%)]\tLoss: 3.710788\n",
      "Train Epoch: 3 [3778560/5599780 (67%)]\tLoss: 3.743582\n",
      "Train Epoch: 3 [3783680/5599780 (68%)]\tLoss: 3.708762\n",
      "Train Epoch: 3 [3788800/5599780 (68%)]\tLoss: 3.728729\n",
      "Train Epoch: 3 [3793920/5599780 (68%)]\tLoss: 3.711900\n",
      "Train Epoch: 3 [3799040/5599780 (68%)]\tLoss: 3.731925\n",
      "Train Epoch: 3 [3804160/5599780 (68%)]\tLoss: 3.691579\n",
      "Train Epoch: 3 [3809280/5599780 (68%)]\tLoss: 3.670365\n",
      "Train Epoch: 3 [3814400/5599780 (68%)]\tLoss: 3.683201\n",
      "Train Epoch: 3 [3819520/5599780 (68%)]\tLoss: 3.698236\n",
      "Train Epoch: 3 [3824640/5599780 (68%)]\tLoss: 3.711764\n",
      "Train Epoch: 3 [3829760/5599780 (68%)]\tLoss: 3.711440\n",
      "Train Epoch: 3 [3834880/5599780 (68%)]\tLoss: 3.705839\n",
      "Train Epoch: 3 [3840000/5599780 (69%)]\tLoss: 3.710380\n",
      "Train Epoch: 3 [3845120/5599780 (69%)]\tLoss: 3.743538\n",
      "Train Epoch: 3 [3850240/5599780 (69%)]\tLoss: 3.681532\n",
      "Train Epoch: 3 [3855360/5599780 (69%)]\tLoss: 3.696236\n",
      "Train Epoch: 3 [3860480/5599780 (69%)]\tLoss: 3.711085\n",
      "Train Epoch: 3 [3865600/5599780 (69%)]\tLoss: 3.706069\n",
      "Train Epoch: 3 [3870720/5599780 (69%)]\tLoss: 3.725270\n",
      "Train Epoch: 3 [3875840/5599780 (69%)]\tLoss: 3.700315\n",
      "Train Epoch: 3 [3880960/5599780 (69%)]\tLoss: 3.722356\n",
      "Train Epoch: 3 [3886080/5599780 (69%)]\tLoss: 3.712497\n",
      "Train Epoch: 3 [3891200/5599780 (69%)]\tLoss: 3.773052\n",
      "Train Epoch: 3 [3896320/5599780 (70%)]\tLoss: 3.761765\n",
      "Train Epoch: 3 [3901440/5599780 (70%)]\tLoss: 3.737164\n",
      "Train Epoch: 3 [3906560/5599780 (70%)]\tLoss: 3.756194\n",
      "Train Epoch: 3 [3911680/5599780 (70%)]\tLoss: 3.743275\n",
      "Train Epoch: 3 [3916800/5599780 (70%)]\tLoss: 3.705056\n",
      "Train Epoch: 3 [3921920/5599780 (70%)]\tLoss: 3.689728\n",
      "Train Epoch: 3 [3927040/5599780 (70%)]\tLoss: 3.730407\n",
      "Train Epoch: 3 [3932160/5599780 (70%)]\tLoss: 3.738776\n",
      "Train Epoch: 3 [3937280/5599780 (70%)]\tLoss: 3.682506\n",
      "Train Epoch: 3 [3942400/5599780 (70%)]\tLoss: 3.701245\n",
      "Train Epoch: 3 [3947520/5599780 (70%)]\tLoss: 3.706994\n",
      "Train Epoch: 3 [3952640/5599780 (71%)]\tLoss: 3.712435\n",
      "Train Epoch: 3 [3957760/5599780 (71%)]\tLoss: 3.728173\n",
      "Train Epoch: 3 [3962880/5599780 (71%)]\tLoss: 3.699370\n",
      "Train Epoch: 3 [3968000/5599780 (71%)]\tLoss: 3.726944\n",
      "Train Epoch: 3 [3973120/5599780 (71%)]\tLoss: 3.754194\n",
      "Train Epoch: 3 [3978240/5599780 (71%)]\tLoss: 3.717871\n",
      "Train Epoch: 3 [3983360/5599780 (71%)]\tLoss: 3.709618\n",
      "Train Epoch: 3 [3988480/5599780 (71%)]\tLoss: 3.708978\n",
      "Train Epoch: 3 [3993600/5599780 (71%)]\tLoss: 3.725611\n",
      "Train Epoch: 3 [3998720/5599780 (71%)]\tLoss: 3.715663\n",
      "Train Epoch: 3 [4003840/5599780 (71%)]\tLoss: 3.733891\n",
      "Train Epoch: 3 [4008960/5599780 (72%)]\tLoss: 3.734648\n",
      "Train Epoch: 3 [4014080/5599780 (72%)]\tLoss: 3.681695\n",
      "Train Epoch: 3 [4019200/5599780 (72%)]\tLoss: 3.695103\n",
      "Train Epoch: 3 [4024320/5599780 (72%)]\tLoss: 3.722507\n",
      "Train Epoch: 3 [4029440/5599780 (72%)]\tLoss: 3.741893\n",
      "Train Epoch: 3 [4034560/5599780 (72%)]\tLoss: 3.698730\n",
      "Train Epoch: 3 [4039680/5599780 (72%)]\tLoss: 3.722416\n",
      "Train Epoch: 3 [4044800/5599780 (72%)]\tLoss: 3.735936\n",
      "Train Epoch: 3 [4049920/5599780 (72%)]\tLoss: 3.706950\n",
      "Train Epoch: 3 [4055040/5599780 (72%)]\tLoss: 3.724610\n",
      "Train Epoch: 3 [4060160/5599780 (72%)]\tLoss: 3.754131\n",
      "Train Epoch: 3 [4065280/5599780 (73%)]\tLoss: 3.692232\n",
      "Train Epoch: 3 [4070400/5599780 (73%)]\tLoss: 3.704660\n",
      "Train Epoch: 3 [4075520/5599780 (73%)]\tLoss: 3.704656\n",
      "Train Epoch: 3 [4080640/5599780 (73%)]\tLoss: 3.715720\n",
      "Train Epoch: 3 [4085760/5599780 (73%)]\tLoss: 3.682314\n",
      "Train Epoch: 3 [4090880/5599780 (73%)]\tLoss: 3.701045\n",
      "Train Epoch: 3 [4096000/5599780 (73%)]\tLoss: 3.735896\n",
      "Train Epoch: 3 [4101120/5599780 (73%)]\tLoss: 3.708570\n",
      "Train Epoch: 3 [4106240/5599780 (73%)]\tLoss: 3.711277\n",
      "Train Epoch: 3 [4111360/5599780 (73%)]\tLoss: 3.699072\n",
      "Train Epoch: 3 [4116480/5599780 (74%)]\tLoss: 3.732893\n",
      "Train Epoch: 3 [4121600/5599780 (74%)]\tLoss: 3.721775\n",
      "Train Epoch: 3 [4126720/5599780 (74%)]\tLoss: 3.737712\n",
      "Train Epoch: 3 [4131840/5599780 (74%)]\tLoss: 3.696996\n",
      "Train Epoch: 3 [4136960/5599780 (74%)]\tLoss: 3.717577\n",
      "Train Epoch: 3 [4142080/5599780 (74%)]\tLoss: 3.722338\n",
      "Train Epoch: 3 [4147200/5599780 (74%)]\tLoss: 3.722687\n",
      "Train Epoch: 3 [4152320/5599780 (74%)]\tLoss: 3.697705\n",
      "Train Epoch: 3 [4157440/5599780 (74%)]\tLoss: 3.699330\n",
      "Train Epoch: 3 [4162560/5599780 (74%)]\tLoss: 3.688807\n",
      "Train Epoch: 3 [4167680/5599780 (74%)]\tLoss: 3.687019\n",
      "Train Epoch: 3 [4172800/5599780 (75%)]\tLoss: 3.721828\n",
      "Train Epoch: 3 [4177920/5599780 (75%)]\tLoss: 3.735831\n",
      "Train Epoch: 3 [4183040/5599780 (75%)]\tLoss: 3.682016\n",
      "Train Epoch: 3 [4188160/5599780 (75%)]\tLoss: 3.695815\n",
      "Train Epoch: 3 [4193280/5599780 (75%)]\tLoss: 3.700199\n",
      "Train Epoch: 3 [4198400/5599780 (75%)]\tLoss: 3.713448\n",
      "Train Epoch: 3 [4203520/5599780 (75%)]\tLoss: 3.723491\n",
      "Train Epoch: 3 [4208640/5599780 (75%)]\tLoss: 3.691455\n",
      "Train Epoch: 3 [4213760/5599780 (75%)]\tLoss: 3.744115\n",
      "Train Epoch: 3 [4218880/5599780 (75%)]\tLoss: 3.715017\n",
      "Train Epoch: 3 [4224000/5599780 (75%)]\tLoss: 3.728709\n",
      "Train Epoch: 3 [4229120/5599780 (76%)]\tLoss: 3.702681\n",
      "Train Epoch: 3 [4234240/5599780 (76%)]\tLoss: 3.725514\n",
      "Train Epoch: 3 [4239360/5599780 (76%)]\tLoss: 3.736286\n",
      "Train Epoch: 3 [4244480/5599780 (76%)]\tLoss: 3.689344\n",
      "Train Epoch: 3 [4249600/5599780 (76%)]\tLoss: 3.716362\n",
      "Train Epoch: 3 [4254720/5599780 (76%)]\tLoss: 3.718929\n",
      "Train Epoch: 3 [4259840/5599780 (76%)]\tLoss: 3.735442\n",
      "Train Epoch: 3 [4264960/5599780 (76%)]\tLoss: 3.741935\n",
      "Train Epoch: 3 [4270080/5599780 (76%)]\tLoss: 3.736723\n",
      "Train Epoch: 3 [4275200/5599780 (76%)]\tLoss: 3.688998\n",
      "Train Epoch: 3 [4280320/5599780 (76%)]\tLoss: 3.704922\n",
      "Train Epoch: 3 [4285440/5599780 (77%)]\tLoss: 3.728341\n",
      "Train Epoch: 3 [4290560/5599780 (77%)]\tLoss: 3.735781\n",
      "Train Epoch: 3 [4295680/5599780 (77%)]\tLoss: 3.773277\n",
      "Train Epoch: 3 [4300800/5599780 (77%)]\tLoss: 3.718676\n",
      "Train Epoch: 3 [4305920/5599780 (77%)]\tLoss: 3.683164\n",
      "Train Epoch: 3 [4311040/5599780 (77%)]\tLoss: 3.689626\n",
      "Train Epoch: 3 [4316160/5599780 (77%)]\tLoss: 3.721265\n",
      "Train Epoch: 3 [4321280/5599780 (77%)]\tLoss: 3.665560\n",
      "Train Epoch: 3 [4326400/5599780 (77%)]\tLoss: 3.719090\n",
      "Train Epoch: 3 [4331520/5599780 (77%)]\tLoss: 3.689289\n",
      "Train Epoch: 3 [4336640/5599780 (77%)]\tLoss: 3.718853\n",
      "Train Epoch: 3 [4341760/5599780 (78%)]\tLoss: 3.770025\n",
      "Train Epoch: 3 [4346880/5599780 (78%)]\tLoss: 3.748256\n",
      "Train Epoch: 3 [4352000/5599780 (78%)]\tLoss: 3.700761\n",
      "Train Epoch: 3 [4357120/5599780 (78%)]\tLoss: 3.699042\n",
      "Train Epoch: 3 [4362240/5599780 (78%)]\tLoss: 3.665631\n",
      "Train Epoch: 3 [4367360/5599780 (78%)]\tLoss: 3.748258\n",
      "Train Epoch: 3 [4372480/5599780 (78%)]\tLoss: 3.755847\n",
      "Train Epoch: 3 [4377600/5599780 (78%)]\tLoss: 3.736461\n",
      "Train Epoch: 3 [4382720/5599780 (78%)]\tLoss: 3.730111\n",
      "Train Epoch: 3 [4387840/5599780 (78%)]\tLoss: 3.724524\n",
      "Train Epoch: 3 [4392960/5599780 (78%)]\tLoss: 3.694362\n",
      "Train Epoch: 3 [4398080/5599780 (79%)]\tLoss: 3.724854\n",
      "Train Epoch: 3 [4403200/5599780 (79%)]\tLoss: 3.710687\n",
      "Train Epoch: 3 [4408320/5599780 (79%)]\tLoss: 3.689334\n",
      "Train Epoch: 3 [4413440/5599780 (79%)]\tLoss: 3.717018\n",
      "Train Epoch: 3 [4418560/5599780 (79%)]\tLoss: 3.709939\n",
      "Train Epoch: 3 [4423680/5599780 (79%)]\tLoss: 3.751979\n",
      "Train Epoch: 3 [4428800/5599780 (79%)]\tLoss: 3.729618\n",
      "Train Epoch: 3 [4433920/5599780 (79%)]\tLoss: 3.697707\n",
      "Train Epoch: 3 [4439040/5599780 (79%)]\tLoss: 3.746255\n",
      "Train Epoch: 3 [4444160/5599780 (79%)]\tLoss: 3.715707\n",
      "Train Epoch: 3 [4449280/5599780 (79%)]\tLoss: 3.733487\n",
      "Train Epoch: 3 [4454400/5599780 (80%)]\tLoss: 3.708425\n",
      "Train Epoch: 3 [4459520/5599780 (80%)]\tLoss: 3.728365\n",
      "Train Epoch: 3 [4464640/5599780 (80%)]\tLoss: 3.715976\n",
      "Train Epoch: 3 [4469760/5599780 (80%)]\tLoss: 3.713703\n",
      "Train Epoch: 3 [4474880/5599780 (80%)]\tLoss: 3.716687\n",
      "Train Epoch: 3 [4480000/5599780 (80%)]\tLoss: 3.727754\n",
      "Train Epoch: 3 [4485120/5599780 (80%)]\tLoss: 3.712665\n",
      "Train Epoch: 3 [4490240/5599780 (80%)]\tLoss: 3.750784\n",
      "Train Epoch: 3 [4495360/5599780 (80%)]\tLoss: 3.736315\n",
      "Train Epoch: 3 [4500480/5599780 (80%)]\tLoss: 3.729709\n",
      "Train Epoch: 3 [4505600/5599780 (80%)]\tLoss: 3.725319\n",
      "Train Epoch: 3 [4510720/5599780 (81%)]\tLoss: 3.741892\n",
      "Train Epoch: 3 [4515840/5599780 (81%)]\tLoss: 3.710928\n",
      "Train Epoch: 3 [4520960/5599780 (81%)]\tLoss: 3.677965\n",
      "Train Epoch: 3 [4526080/5599780 (81%)]\tLoss: 3.688647\n",
      "Train Epoch: 3 [4531200/5599780 (81%)]\tLoss: 3.747712\n",
      "Train Epoch: 3 [4536320/5599780 (81%)]\tLoss: 3.740047\n",
      "Train Epoch: 3 [4541440/5599780 (81%)]\tLoss: 3.723576\n",
      "Train Epoch: 3 [4546560/5599780 (81%)]\tLoss: 3.726382\n",
      "Train Epoch: 3 [4551680/5599780 (81%)]\tLoss: 3.701116\n",
      "Train Epoch: 3 [4556800/5599780 (81%)]\tLoss: 3.700906\n",
      "Train Epoch: 3 [4561920/5599780 (81%)]\tLoss: 3.728235\n",
      "Train Epoch: 3 [4567040/5599780 (82%)]\tLoss: 3.701832\n",
      "Train Epoch: 3 [4572160/5599780 (82%)]\tLoss: 3.696328\n",
      "Train Epoch: 3 [4577280/5599780 (82%)]\tLoss: 3.767416\n",
      "Train Epoch: 3 [4582400/5599780 (82%)]\tLoss: 3.726926\n",
      "Train Epoch: 3 [4587520/5599780 (82%)]\tLoss: 3.719342\n",
      "Train Epoch: 3 [4592640/5599780 (82%)]\tLoss: 3.738141\n",
      "Train Epoch: 3 [4597760/5599780 (82%)]\tLoss: 3.716706\n",
      "Train Epoch: 3 [4602880/5599780 (82%)]\tLoss: 3.710484\n",
      "Train Epoch: 3 [4608000/5599780 (82%)]\tLoss: 3.675822\n",
      "Train Epoch: 3 [4613120/5599780 (82%)]\tLoss: 3.722489\n",
      "Train Epoch: 3 [4618240/5599780 (82%)]\tLoss: 3.694115\n",
      "Train Epoch: 3 [4623360/5599780 (83%)]\tLoss: 3.716764\n",
      "Train Epoch: 3 [4628480/5599780 (83%)]\tLoss: 3.730308\n",
      "Train Epoch: 3 [4633600/5599780 (83%)]\tLoss: 3.730355\n",
      "Train Epoch: 3 [4638720/5599780 (83%)]\tLoss: 3.733487\n",
      "Train Epoch: 3 [4643840/5599780 (83%)]\tLoss: 3.710611\n",
      "Train Epoch: 3 [4648960/5599780 (83%)]\tLoss: 3.716086\n",
      "Train Epoch: 3 [4654080/5599780 (83%)]\tLoss: 3.749662\n",
      "Train Epoch: 3 [4659200/5599780 (83%)]\tLoss: 3.714427\n",
      "Train Epoch: 3 [4664320/5599780 (83%)]\tLoss: 3.674470\n",
      "Train Epoch: 3 [4669440/5599780 (83%)]\tLoss: 3.701073\n",
      "Train Epoch: 3 [4674560/5599780 (83%)]\tLoss: 3.749511\n",
      "Train Epoch: 3 [4679680/5599780 (84%)]\tLoss: 3.687307\n",
      "Train Epoch: 3 [4684800/5599780 (84%)]\tLoss: 3.743203\n",
      "Train Epoch: 3 [4689920/5599780 (84%)]\tLoss: 3.730843\n",
      "Train Epoch: 3 [4695040/5599780 (84%)]\tLoss: 3.728240\n",
      "Train Epoch: 3 [4700160/5599780 (84%)]\tLoss: 3.696175\n",
      "Train Epoch: 3 [4705280/5599780 (84%)]\tLoss: 3.729805\n",
      "Train Epoch: 3 [4710400/5599780 (84%)]\tLoss: 3.701542\n",
      "Train Epoch: 3 [4715520/5599780 (84%)]\tLoss: 3.737464\n",
      "Train Epoch: 3 [4720640/5599780 (84%)]\tLoss: 3.716620\n",
      "Train Epoch: 3 [4725760/5599780 (84%)]\tLoss: 3.704930\n",
      "Train Epoch: 3 [4730880/5599780 (84%)]\tLoss: 3.725296\n",
      "Train Epoch: 3 [4736000/5599780 (85%)]\tLoss: 3.711764\n",
      "Train Epoch: 3 [4741120/5599780 (85%)]\tLoss: 3.759301\n",
      "Train Epoch: 3 [4746240/5599780 (85%)]\tLoss: 3.704899\n",
      "Train Epoch: 3 [4751360/5599780 (85%)]\tLoss: 3.728768\n",
      "Train Epoch: 3 [4756480/5599780 (85%)]\tLoss: 3.686489\n",
      "Train Epoch: 3 [4761600/5599780 (85%)]\tLoss: 3.729059\n",
      "Train Epoch: 3 [4766720/5599780 (85%)]\tLoss: 3.731404\n",
      "Train Epoch: 3 [4771840/5599780 (85%)]\tLoss: 3.741346\n",
      "Train Epoch: 3 [4776960/5599780 (85%)]\tLoss: 3.698197\n",
      "Train Epoch: 3 [4782080/5599780 (85%)]\tLoss: 3.705672\n",
      "Train Epoch: 3 [4787200/5599780 (85%)]\tLoss: 3.720193\n",
      "Train Epoch: 3 [4792320/5599780 (86%)]\tLoss: 3.716497\n",
      "Train Epoch: 3 [4797440/5599780 (86%)]\tLoss: 3.732023\n",
      "Train Epoch: 3 [4802560/5599780 (86%)]\tLoss: 3.681527\n",
      "Train Epoch: 3 [4807680/5599780 (86%)]\tLoss: 3.708025\n",
      "Train Epoch: 3 [4812800/5599780 (86%)]\tLoss: 3.706104\n",
      "Train Epoch: 3 [4817920/5599780 (86%)]\tLoss: 3.766394\n",
      "Train Epoch: 3 [4823040/5599780 (86%)]\tLoss: 3.720068\n",
      "Train Epoch: 3 [4828160/5599780 (86%)]\tLoss: 3.705707\n",
      "Train Epoch: 3 [4833280/5599780 (86%)]\tLoss: 3.669340\n",
      "Train Epoch: 3 [4838400/5599780 (86%)]\tLoss: 3.701566\n",
      "Train Epoch: 3 [4843520/5599780 (86%)]\tLoss: 3.749681\n",
      "Train Epoch: 3 [4848640/5599780 (87%)]\tLoss: 3.703449\n",
      "Train Epoch: 3 [4853760/5599780 (87%)]\tLoss: 3.726526\n",
      "Train Epoch: 3 [4858880/5599780 (87%)]\tLoss: 3.713947\n",
      "Train Epoch: 3 [4864000/5599780 (87%)]\tLoss: 3.706850\n",
      "Train Epoch: 3 [4869120/5599780 (87%)]\tLoss: 3.681418\n",
      "Train Epoch: 3 [4874240/5599780 (87%)]\tLoss: 3.741770\n",
      "Train Epoch: 3 [4879360/5599780 (87%)]\tLoss: 3.708278\n",
      "Train Epoch: 3 [4884480/5599780 (87%)]\tLoss: 3.762938\n",
      "Train Epoch: 3 [4889600/5599780 (87%)]\tLoss: 3.694342\n",
      "Train Epoch: 3 [4894720/5599780 (87%)]\tLoss: 3.707007\n",
      "Train Epoch: 3 [4899840/5599780 (87%)]\tLoss: 3.685353\n",
      "Train Epoch: 3 [4904960/5599780 (88%)]\tLoss: 3.709474\n",
      "Train Epoch: 3 [4910080/5599780 (88%)]\tLoss: 3.730020\n",
      "Train Epoch: 3 [4915200/5599780 (88%)]\tLoss: 3.711057\n",
      "Train Epoch: 3 [4920320/5599780 (88%)]\tLoss: 3.716435\n",
      "Train Epoch: 3 [4925440/5599780 (88%)]\tLoss: 3.739090\n",
      "Train Epoch: 3 [4930560/5599780 (88%)]\tLoss: 3.711127\n",
      "Train Epoch: 3 [4935680/5599780 (88%)]\tLoss: 3.692746\n",
      "Train Epoch: 3 [4940800/5599780 (88%)]\tLoss: 3.716640\n",
      "Train Epoch: 3 [4945920/5599780 (88%)]\tLoss: 3.686857\n",
      "Train Epoch: 3 [4951040/5599780 (88%)]\tLoss: 3.695277\n",
      "Train Epoch: 3 [4956160/5599780 (88%)]\tLoss: 3.737923\n",
      "Train Epoch: 3 [4961280/5599780 (89%)]\tLoss: 3.708495\n",
      "Train Epoch: 3 [4966400/5599780 (89%)]\tLoss: 3.709759\n",
      "Train Epoch: 3 [4971520/5599780 (89%)]\tLoss: 3.706879\n",
      "Train Epoch: 3 [4976640/5599780 (89%)]\tLoss: 3.697087\n",
      "Train Epoch: 3 [4981760/5599780 (89%)]\tLoss: 3.704972\n",
      "Train Epoch: 3 [4986880/5599780 (89%)]\tLoss: 3.701511\n",
      "Train Epoch: 3 [4992000/5599780 (89%)]\tLoss: 3.665929\n",
      "Train Epoch: 3 [4997120/5599780 (89%)]\tLoss: 3.694777\n",
      "Train Epoch: 3 [5002240/5599780 (89%)]\tLoss: 3.698378\n",
      "Train Epoch: 3 [5007360/5599780 (89%)]\tLoss: 3.716758\n",
      "Train Epoch: 3 [5012480/5599780 (90%)]\tLoss: 3.720968\n",
      "Train Epoch: 3 [5017600/5599780 (90%)]\tLoss: 3.728690\n",
      "Train Epoch: 3 [5022720/5599780 (90%)]\tLoss: 3.698999\n",
      "Train Epoch: 3 [5027840/5599780 (90%)]\tLoss: 3.705038\n",
      "Train Epoch: 3 [5032960/5599780 (90%)]\tLoss: 3.710760\n",
      "Train Epoch: 3 [5038080/5599780 (90%)]\tLoss: 3.687226\n",
      "Train Epoch: 3 [5043200/5599780 (90%)]\tLoss: 3.730282\n",
      "Train Epoch: 3 [5048320/5599780 (90%)]\tLoss: 3.763324\n",
      "Train Epoch: 3 [5053440/5599780 (90%)]\tLoss: 3.726255\n",
      "Train Epoch: 3 [5058560/5599780 (90%)]\tLoss: 3.694599\n",
      "Train Epoch: 3 [5063680/5599780 (90%)]\tLoss: 3.697127\n",
      "Train Epoch: 3 [5068800/5599780 (91%)]\tLoss: 3.696342\n",
      "Train Epoch: 3 [5073920/5599780 (91%)]\tLoss: 3.687368\n",
      "Train Epoch: 3 [5079040/5599780 (91%)]\tLoss: 3.759814\n",
      "Train Epoch: 3 [5084160/5599780 (91%)]\tLoss: 3.742494\n",
      "Train Epoch: 3 [5089280/5599780 (91%)]\tLoss: 3.681074\n",
      "Train Epoch: 3 [5094400/5599780 (91%)]\tLoss: 3.724530\n",
      "Train Epoch: 3 [5099520/5599780 (91%)]\tLoss: 3.722642\n",
      "Train Epoch: 3 [5104640/5599780 (91%)]\tLoss: 3.743845\n",
      "Train Epoch: 3 [5109760/5599780 (91%)]\tLoss: 3.732488\n",
      "Train Epoch: 3 [5114880/5599780 (91%)]\tLoss: 3.723872\n",
      "Train Epoch: 3 [5120000/5599780 (91%)]\tLoss: 3.731726\n",
      "Train Epoch: 3 [5125120/5599780 (92%)]\tLoss: 3.697484\n",
      "Train Epoch: 3 [5130240/5599780 (92%)]\tLoss: 3.713040\n",
      "Train Epoch: 3 [5135360/5599780 (92%)]\tLoss: 3.744437\n",
      "Train Epoch: 3 [5140480/5599780 (92%)]\tLoss: 3.720042\n",
      "Train Epoch: 3 [5145600/5599780 (92%)]\tLoss: 3.722489\n",
      "Train Epoch: 3 [5150720/5599780 (92%)]\tLoss: 3.706618\n",
      "Train Epoch: 3 [5155840/5599780 (92%)]\tLoss: 3.715243\n",
      "Train Epoch: 3 [5160960/5599780 (92%)]\tLoss: 3.734197\n",
      "Train Epoch: 3 [5166080/5599780 (92%)]\tLoss: 3.749823\n",
      "Train Epoch: 3 [5171200/5599780 (92%)]\tLoss: 3.719280\n",
      "Train Epoch: 3 [5176320/5599780 (92%)]\tLoss: 3.714447\n",
      "Train Epoch: 3 [5181440/5599780 (93%)]\tLoss: 3.717773\n",
      "Train Epoch: 3 [5186560/5599780 (93%)]\tLoss: 3.696747\n",
      "Train Epoch: 3 [5191680/5599780 (93%)]\tLoss: 3.689122\n",
      "Train Epoch: 3 [5196800/5599780 (93%)]\tLoss: 3.708821\n",
      "Train Epoch: 3 [5201920/5599780 (93%)]\tLoss: 3.710850\n",
      "Train Epoch: 3 [5207040/5599780 (93%)]\tLoss: 3.687269\n",
      "Train Epoch: 3 [5212160/5599780 (93%)]\tLoss: 3.700450\n",
      "Train Epoch: 3 [5217280/5599780 (93%)]\tLoss: 3.718519\n",
      "Train Epoch: 3 [5222400/5599780 (93%)]\tLoss: 3.740231\n",
      "Train Epoch: 3 [5227520/5599780 (93%)]\tLoss: 3.706800\n",
      "Train Epoch: 3 [5232640/5599780 (93%)]\tLoss: 3.697123\n",
      "Train Epoch: 3 [5237760/5599780 (94%)]\tLoss: 3.730327\n",
      "Train Epoch: 3 [5242880/5599780 (94%)]\tLoss: 3.705001\n",
      "Train Epoch: 3 [5248000/5599780 (94%)]\tLoss: 3.681614\n",
      "Train Epoch: 3 [5253120/5599780 (94%)]\tLoss: 3.739951\n",
      "Train Epoch: 3 [5258240/5599780 (94%)]\tLoss: 3.780371\n",
      "Train Epoch: 3 [5263360/5599780 (94%)]\tLoss: 3.740084\n",
      "Train Epoch: 3 [5268480/5599780 (94%)]\tLoss: 3.720076\n",
      "Train Epoch: 3 [5273600/5599780 (94%)]\tLoss: 3.732075\n",
      "Train Epoch: 3 [5278720/5599780 (94%)]\tLoss: 3.724211\n",
      "Train Epoch: 3 [5283840/5599780 (94%)]\tLoss: 3.724506\n",
      "Train Epoch: 3 [5288960/5599780 (94%)]\tLoss: 3.714453\n",
      "Train Epoch: 3 [5294080/5599780 (95%)]\tLoss: 3.681247\n",
      "Train Epoch: 3 [5299200/5599780 (95%)]\tLoss: 3.675373\n",
      "Train Epoch: 3 [5304320/5599780 (95%)]\tLoss: 3.696602\n",
      "Train Epoch: 3 [5309440/5599780 (95%)]\tLoss: 3.675581\n",
      "Train Epoch: 3 [5314560/5599780 (95%)]\tLoss: 3.702948\n",
      "Train Epoch: 3 [5319680/5599780 (95%)]\tLoss: 3.707759\n",
      "Train Epoch: 3 [5324800/5599780 (95%)]\tLoss: 3.732888\n",
      "Train Epoch: 3 [5329920/5599780 (95%)]\tLoss: 3.721266\n",
      "Train Epoch: 3 [5335040/5599780 (95%)]\tLoss: 3.750479\n",
      "Train Epoch: 3 [5340160/5599780 (95%)]\tLoss: 3.689636\n",
      "Train Epoch: 3 [5345280/5599780 (95%)]\tLoss: 3.707246\n",
      "Train Epoch: 3 [5350400/5599780 (96%)]\tLoss: 3.685374\n",
      "Train Epoch: 3 [5355520/5599780 (96%)]\tLoss: 3.722701\n",
      "Train Epoch: 3 [5360640/5599780 (96%)]\tLoss: 3.690378\n",
      "Train Epoch: 3 [5365760/5599780 (96%)]\tLoss: 3.686195\n",
      "Train Epoch: 3 [5370880/5599780 (96%)]\tLoss: 3.748625\n",
      "Train Epoch: 3 [5376000/5599780 (96%)]\tLoss: 3.754723\n",
      "Train Epoch: 3 [5381120/5599780 (96%)]\tLoss: 3.699486\n",
      "Train Epoch: 3 [5386240/5599780 (96%)]\tLoss: 3.706863\n",
      "Train Epoch: 3 [5391360/5599780 (96%)]\tLoss: 3.739753\n",
      "Train Epoch: 3 [5396480/5599780 (96%)]\tLoss: 3.693160\n",
      "Train Epoch: 3 [5401600/5599780 (96%)]\tLoss: 3.682117\n",
      "Train Epoch: 3 [5406720/5599780 (97%)]\tLoss: 3.710747\n",
      "Train Epoch: 3 [5411840/5599780 (97%)]\tLoss: 3.753701\n",
      "Train Epoch: 3 [5416960/5599780 (97%)]\tLoss: 3.738116\n",
      "Train Epoch: 3 [5422080/5599780 (97%)]\tLoss: 3.702968\n",
      "Train Epoch: 3 [5427200/5599780 (97%)]\tLoss: 3.722779\n",
      "Train Epoch: 3 [5432320/5599780 (97%)]\tLoss: 3.665744\n",
      "Train Epoch: 3 [5437440/5599780 (97%)]\tLoss: 3.708681\n",
      "Train Epoch: 3 [5442560/5599780 (97%)]\tLoss: 3.749777\n",
      "Train Epoch: 3 [5447680/5599780 (97%)]\tLoss: 3.715861\n",
      "Train Epoch: 3 [5452800/5599780 (97%)]\tLoss: 3.702507\n",
      "Train Epoch: 3 [5457920/5599780 (97%)]\tLoss: 3.700290\n",
      "Train Epoch: 3 [5463040/5599780 (98%)]\tLoss: 3.721617\n",
      "Train Epoch: 3 [5468160/5599780 (98%)]\tLoss: 3.717795\n",
      "Train Epoch: 3 [5473280/5599780 (98%)]\tLoss: 3.709732\n",
      "Train Epoch: 3 [5478400/5599780 (98%)]\tLoss: 3.710821\n",
      "Train Epoch: 3 [5483520/5599780 (98%)]\tLoss: 3.716628\n",
      "Train Epoch: 3 [5488640/5599780 (98%)]\tLoss: 3.700999\n",
      "Train Epoch: 3 [5493760/5599780 (98%)]\tLoss: 3.722853\n",
      "Train Epoch: 3 [5498880/5599780 (98%)]\tLoss: 3.693187\n",
      "Train Epoch: 3 [5504000/5599780 (98%)]\tLoss: 3.715467\n",
      "Train Epoch: 3 [5509120/5599780 (98%)]\tLoss: 3.742853\n",
      "Train Epoch: 3 [5514240/5599780 (98%)]\tLoss: 3.756797\n",
      "Train Epoch: 3 [5519360/5599780 (99%)]\tLoss: 3.717042\n",
      "Train Epoch: 3 [5524480/5599780 (99%)]\tLoss: 3.691347\n",
      "Train Epoch: 3 [5529600/5599780 (99%)]\tLoss: 3.716046\n",
      "Train Epoch: 3 [5534720/5599780 (99%)]\tLoss: 3.703566\n",
      "Train Epoch: 3 [5539840/5599780 (99%)]\tLoss: 3.726460\n",
      "Train Epoch: 3 [5544960/5599780 (99%)]\tLoss: 3.679889\n",
      "Train Epoch: 3 [5550080/5599780 (99%)]\tLoss: 3.708299\n",
      "Train Epoch: 3 [5555200/5599780 (99%)]\tLoss: 3.751935\n",
      "Train Epoch: 3 [5560320/5599780 (99%)]\tLoss: 3.732236\n",
      "Train Epoch: 3 [5565440/5599780 (99%)]\tLoss: 3.750062\n",
      "Train Epoch: 3 [5570560/5599780 (99%)]\tLoss: 3.700209\n",
      "Train Epoch: 3 [5575680/5599780 (100%)]\tLoss: 3.693220\n",
      "Train Epoch: 3 [5580800/5599780 (100%)]\tLoss: 3.697150\n",
      "Train Epoch: 3 [5585920/5599780 (100%)]\tLoss: 3.730113\n",
      "Train Epoch: 3 [5591040/5599780 (100%)]\tLoss: 3.695746\n",
      "Train Epoch: 3 [5596160/5599780 (100%)]\tLoss: 3.714770\n",
      "\n",
      "Test set: Average loss: 3.7157, Accuracy: 399029/849497 (47%)\n",
      "\n",
      "Registering model baseline_model\n",
      "Train Epoch: 4 [0/5599780 (0%)]\tLoss: 3.708770\n",
      "Train Epoch: 4 [5120/5599780 (0%)]\tLoss: 3.689503\n",
      "Train Epoch: 4 [10240/5599780 (0%)]\tLoss: 3.733457\n",
      "Train Epoch: 4 [15360/5599780 (0%)]\tLoss: 3.739415\n",
      "Train Epoch: 4 [20480/5599780 (0%)]\tLoss: 3.705202\n",
      "Train Epoch: 4 [25600/5599780 (0%)]\tLoss: 3.716196\n",
      "Train Epoch: 4 [30720/5599780 (1%)]\tLoss: 3.731446\n",
      "Train Epoch: 4 [35840/5599780 (1%)]\tLoss: 3.692408\n",
      "Train Epoch: 4 [40960/5599780 (1%)]\tLoss: 3.749154\n",
      "Train Epoch: 4 [46080/5599780 (1%)]\tLoss: 3.738128\n",
      "Train Epoch: 4 [51200/5599780 (1%)]\tLoss: 3.714863\n",
      "Train Epoch: 4 [56320/5599780 (1%)]\tLoss: 3.730317\n",
      "Train Epoch: 4 [61440/5599780 (1%)]\tLoss: 3.714293\n",
      "Train Epoch: 4 [66560/5599780 (1%)]\tLoss: 3.744001\n",
      "Train Epoch: 4 [71680/5599780 (1%)]\tLoss: 3.733551\n",
      "Train Epoch: 4 [76800/5599780 (1%)]\tLoss: 3.664287\n",
      "Train Epoch: 4 [81920/5599780 (1%)]\tLoss: 3.703396\n",
      "Train Epoch: 4 [87040/5599780 (2%)]\tLoss: 3.707019\n",
      "Train Epoch: 4 [92160/5599780 (2%)]\tLoss: 3.754562\n",
      "Train Epoch: 4 [97280/5599780 (2%)]\tLoss: 3.683696\n",
      "Train Epoch: 4 [102400/5599780 (2%)]\tLoss: 3.734221\n",
      "Train Epoch: 4 [107520/5599780 (2%)]\tLoss: 3.740744\n",
      "Train Epoch: 4 [112640/5599780 (2%)]\tLoss: 3.671858\n",
      "Train Epoch: 4 [117760/5599780 (2%)]\tLoss: 3.721704\n",
      "Train Epoch: 4 [122880/5599780 (2%)]\tLoss: 3.745855\n",
      "Train Epoch: 4 [128000/5599780 (2%)]\tLoss: 3.680996\n",
      "Train Epoch: 4 [133120/5599780 (2%)]\tLoss: 3.697694\n",
      "Train Epoch: 4 [138240/5599780 (2%)]\tLoss: 3.677613\n",
      "Train Epoch: 4 [143360/5599780 (3%)]\tLoss: 3.684868\n",
      "Train Epoch: 4 [148480/5599780 (3%)]\tLoss: 3.716885\n",
      "Train Epoch: 4 [153600/5599780 (3%)]\tLoss: 3.697159\n",
      "Train Epoch: 4 [158720/5599780 (3%)]\tLoss: 3.710772\n",
      "Train Epoch: 4 [163840/5599780 (3%)]\tLoss: 3.696518\n",
      "Train Epoch: 4 [168960/5599780 (3%)]\tLoss: 3.671708\n",
      "Train Epoch: 4 [174080/5599780 (3%)]\tLoss: 3.745376\n",
      "Train Epoch: 4 [179200/5599780 (3%)]\tLoss: 3.669430\n",
      "Train Epoch: 4 [184320/5599780 (3%)]\tLoss: 3.731053\n",
      "Train Epoch: 4 [189440/5599780 (3%)]\tLoss: 3.746069\n",
      "Train Epoch: 4 [194560/5599780 (3%)]\tLoss: 3.706806\n",
      "Train Epoch: 4 [199680/5599780 (4%)]\tLoss: 3.695402\n",
      "Train Epoch: 4 [204800/5599780 (4%)]\tLoss: 3.733391\n",
      "Train Epoch: 4 [209920/5599780 (4%)]\tLoss: 3.744831\n",
      "Train Epoch: 4 [215040/5599780 (4%)]\tLoss: 3.755286\n",
      "Train Epoch: 4 [220160/5599780 (4%)]\tLoss: 3.719770\n",
      "Train Epoch: 4 [225280/5599780 (4%)]\tLoss: 3.749817\n",
      "Train Epoch: 4 [230400/5599780 (4%)]\tLoss: 3.705670\n",
      "Train Epoch: 4 [235520/5599780 (4%)]\tLoss: 3.736763\n",
      "Train Epoch: 4 [240640/5599780 (4%)]\tLoss: 3.698436\n",
      "Train Epoch: 4 [245760/5599780 (4%)]\tLoss: 3.718095\n",
      "Train Epoch: 4 [250880/5599780 (4%)]\tLoss: 3.746759\n",
      "Train Epoch: 4 [256000/5599780 (5%)]\tLoss: 3.740107\n",
      "Train Epoch: 4 [261120/5599780 (5%)]\tLoss: 3.709034\n",
      "Train Epoch: 4 [266240/5599780 (5%)]\tLoss: 3.718594\n",
      "Train Epoch: 4 [271360/5599780 (5%)]\tLoss: 3.704925\n",
      "Train Epoch: 4 [276480/5599780 (5%)]\tLoss: 3.740296\n",
      "Train Epoch: 4 [281600/5599780 (5%)]\tLoss: 3.708649\n",
      "Train Epoch: 4 [286720/5599780 (5%)]\tLoss: 3.714349\n",
      "Train Epoch: 4 [291840/5599780 (5%)]\tLoss: 3.663868\n",
      "Train Epoch: 4 [296960/5599780 (5%)]\tLoss: 3.704699\n",
      "Train Epoch: 4 [302080/5599780 (5%)]\tLoss: 3.742008\n",
      "Train Epoch: 4 [307200/5599780 (5%)]\tLoss: 3.693631\n",
      "Train Epoch: 4 [312320/5599780 (6%)]\tLoss: 3.753578\n",
      "Train Epoch: 4 [317440/5599780 (6%)]\tLoss: 3.743912\n",
      "Train Epoch: 4 [322560/5599780 (6%)]\tLoss: 3.701080\n",
      "Train Epoch: 4 [327680/5599780 (6%)]\tLoss: 3.704987\n",
      "Train Epoch: 4 [332800/5599780 (6%)]\tLoss: 3.695727\n",
      "Train Epoch: 4 [337920/5599780 (6%)]\tLoss: 3.744055\n",
      "Train Epoch: 4 [343040/5599780 (6%)]\tLoss: 3.716466\n",
      "Train Epoch: 4 [348160/5599780 (6%)]\tLoss: 3.717429\n",
      "Train Epoch: 4 [353280/5599780 (6%)]\tLoss: 3.709257\n",
      "Train Epoch: 4 [358400/5599780 (6%)]\tLoss: 3.696998\n",
      "Train Epoch: 4 [363520/5599780 (6%)]\tLoss: 3.683419\n",
      "Train Epoch: 4 [368640/5599780 (7%)]\tLoss: 3.708303\n",
      "Train Epoch: 4 [373760/5599780 (7%)]\tLoss: 3.741685\n",
      "Train Epoch: 4 [378880/5599780 (7%)]\tLoss: 3.704914\n",
      "Train Epoch: 4 [384000/5599780 (7%)]\tLoss: 3.721042\n",
      "Train Epoch: 4 [389120/5599780 (7%)]\tLoss: 3.679518\n",
      "Train Epoch: 4 [394240/5599780 (7%)]\tLoss: 3.703117\n",
      "Train Epoch: 4 [399360/5599780 (7%)]\tLoss: 3.721552\n",
      "Train Epoch: 4 [404480/5599780 (7%)]\tLoss: 3.700688\n",
      "Train Epoch: 4 [409600/5599780 (7%)]\tLoss: 3.749498\n",
      "Train Epoch: 4 [414720/5599780 (7%)]\tLoss: 3.713469\n",
      "Train Epoch: 4 [419840/5599780 (7%)]\tLoss: 3.707313\n",
      "Train Epoch: 4 [424960/5599780 (8%)]\tLoss: 3.718575\n",
      "Train Epoch: 4 [430080/5599780 (8%)]\tLoss: 3.644113\n",
      "Train Epoch: 4 [435200/5599780 (8%)]\tLoss: 3.763727\n",
      "Train Epoch: 4 [440320/5599780 (8%)]\tLoss: 3.707100\n",
      "Train Epoch: 4 [445440/5599780 (8%)]\tLoss: 3.753983\n",
      "Train Epoch: 4 [450560/5599780 (8%)]\tLoss: 3.736120\n",
      "Train Epoch: 4 [455680/5599780 (8%)]\tLoss: 3.702982\n",
      "Train Epoch: 4 [460800/5599780 (8%)]\tLoss: 3.677938\n",
      "Train Epoch: 4 [465920/5599780 (8%)]\tLoss: 3.740047\n",
      "Train Epoch: 4 [471040/5599780 (8%)]\tLoss: 3.751758\n",
      "Train Epoch: 4 [476160/5599780 (9%)]\tLoss: 3.673716\n",
      "Train Epoch: 4 [481280/5599780 (9%)]\tLoss: 3.728458\n",
      "Train Epoch: 4 [486400/5599780 (9%)]\tLoss: 3.710324\n",
      "Train Epoch: 4 [491520/5599780 (9%)]\tLoss: 3.718692\n",
      "Train Epoch: 4 [496640/5599780 (9%)]\tLoss: 3.740572\n",
      "Train Epoch: 4 [501760/5599780 (9%)]\tLoss: 3.729119\n",
      "Train Epoch: 4 [506880/5599780 (9%)]\tLoss: 3.732360\n",
      "Train Epoch: 4 [512000/5599780 (9%)]\tLoss: 3.685537\n",
      "Train Epoch: 4 [517120/5599780 (9%)]\tLoss: 3.694805\n",
      "Train Epoch: 4 [522240/5599780 (9%)]\tLoss: 3.726118\n",
      "Train Epoch: 4 [527360/5599780 (9%)]\tLoss: 3.708641\n",
      "Train Epoch: 4 [532480/5599780 (10%)]\tLoss: 3.738012\n",
      "Train Epoch: 4 [537600/5599780 (10%)]\tLoss: 3.702998\n",
      "Train Epoch: 4 [542720/5599780 (10%)]\tLoss: 3.711175\n",
      "Train Epoch: 4 [547840/5599780 (10%)]\tLoss: 3.716631\n",
      "Train Epoch: 4 [552960/5599780 (10%)]\tLoss: 3.729443\n",
      "Train Epoch: 4 [558080/5599780 (10%)]\tLoss: 3.707005\n",
      "Train Epoch: 4 [563200/5599780 (10%)]\tLoss: 3.740135\n",
      "Train Epoch: 4 [568320/5599780 (10%)]\tLoss: 3.736184\n",
      "Train Epoch: 4 [573440/5599780 (10%)]\tLoss: 3.724550\n",
      "Train Epoch: 4 [578560/5599780 (10%)]\tLoss: 3.713031\n",
      "Train Epoch: 4 [583680/5599780 (10%)]\tLoss: 3.720445\n",
      "Train Epoch: 4 [588800/5599780 (11%)]\tLoss: 3.743639\n",
      "Train Epoch: 4 [593920/5599780 (11%)]\tLoss: 3.693157\n",
      "Train Epoch: 4 [599040/5599780 (11%)]\tLoss: 3.742248\n",
      "Train Epoch: 4 [604160/5599780 (11%)]\tLoss: 3.709413\n",
      "Train Epoch: 4 [609280/5599780 (11%)]\tLoss: 3.736884\n",
      "Train Epoch: 4 [614400/5599780 (11%)]\tLoss: 3.680116\n",
      "Train Epoch: 4 [619520/5599780 (11%)]\tLoss: 3.733427\n",
      "Train Epoch: 4 [624640/5599780 (11%)]\tLoss: 3.738090\n",
      "Train Epoch: 4 [629760/5599780 (11%)]\tLoss: 3.704350\n",
      "Train Epoch: 4 [634880/5599780 (11%)]\tLoss: 3.727927\n",
      "Train Epoch: 4 [640000/5599780 (11%)]\tLoss: 3.752184\n",
      "Train Epoch: 4 [645120/5599780 (12%)]\tLoss: 3.728349\n",
      "Train Epoch: 4 [650240/5599780 (12%)]\tLoss: 3.671560\n",
      "Train Epoch: 4 [655360/5599780 (12%)]\tLoss: 3.701020\n",
      "Train Epoch: 4 [660480/5599780 (12%)]\tLoss: 3.722630\n",
      "Train Epoch: 4 [665600/5599780 (12%)]\tLoss: 3.727633\n",
      "Train Epoch: 4 [670720/5599780 (12%)]\tLoss: 3.662214\n",
      "Train Epoch: 4 [675840/5599780 (12%)]\tLoss: 3.726328\n",
      "Train Epoch: 4 [680960/5599780 (12%)]\tLoss: 3.745928\n",
      "Train Epoch: 4 [686080/5599780 (12%)]\tLoss: 3.709432\n",
      "Train Epoch: 4 [691200/5599780 (12%)]\tLoss: 3.718639\n",
      "Train Epoch: 4 [696320/5599780 (12%)]\tLoss: 3.693188\n",
      "Train Epoch: 4 [701440/5599780 (13%)]\tLoss: 3.677743\n",
      "Train Epoch: 4 [706560/5599780 (13%)]\tLoss: 3.679523\n",
      "Train Epoch: 4 [711680/5599780 (13%)]\tLoss: 3.727049\n",
      "Train Epoch: 4 [716800/5599780 (13%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [721920/5599780 (13%)]\tLoss: 3.737603\n",
      "Train Epoch: 4 [727040/5599780 (13%)]\tLoss: 3.683425\n",
      "Train Epoch: 4 [732160/5599780 (13%)]\tLoss: 3.724327\n",
      "Train Epoch: 4 [737280/5599780 (13%)]\tLoss: 3.695349\n",
      "Train Epoch: 4 [742400/5599780 (13%)]\tLoss: 3.726075\n",
      "Train Epoch: 4 [747520/5599780 (13%)]\tLoss: 3.725037\n",
      "Train Epoch: 4 [752640/5599780 (13%)]\tLoss: 3.717010\n",
      "Train Epoch: 4 [757760/5599780 (14%)]\tLoss: 3.689303\n",
      "Train Epoch: 4 [762880/5599780 (14%)]\tLoss: 3.689199\n",
      "Train Epoch: 4 [768000/5599780 (14%)]\tLoss: 3.693397\n",
      "Train Epoch: 4 [773120/5599780 (14%)]\tLoss: 3.726355\n",
      "Train Epoch: 4 [778240/5599780 (14%)]\tLoss: 3.683472\n",
      "Train Epoch: 4 [783360/5599780 (14%)]\tLoss: 3.758296\n",
      "Train Epoch: 4 [788480/5599780 (14%)]\tLoss: 3.725784\n",
      "Train Epoch: 4 [793600/5599780 (14%)]\tLoss: 3.685241\n",
      "Train Epoch: 4 [798720/5599780 (14%)]\tLoss: 3.683604\n",
      "Train Epoch: 4 [803840/5599780 (14%)]\tLoss: 3.721529\n",
      "Train Epoch: 4 [808960/5599780 (14%)]\tLoss: 3.685388\n",
      "Train Epoch: 4 [814080/5599780 (15%)]\tLoss: 3.712492\n",
      "Train Epoch: 4 [819200/5599780 (15%)]\tLoss: 3.714712\n",
      "Train Epoch: 4 [824320/5599780 (15%)]\tLoss: 3.697597\n",
      "Train Epoch: 4 [829440/5599780 (15%)]\tLoss: 3.698635\n",
      "Train Epoch: 4 [834560/5599780 (15%)]\tLoss: 3.710996\n",
      "Train Epoch: 4 [839680/5599780 (15%)]\tLoss: 3.707355\n",
      "Train Epoch: 4 [844800/5599780 (15%)]\tLoss: 3.702965\n",
      "Train Epoch: 4 [849920/5599780 (15%)]\tLoss: 3.691343\n",
      "Train Epoch: 4 [855040/5599780 (15%)]\tLoss: 3.721096\n",
      "Train Epoch: 4 [860160/5599780 (15%)]\tLoss: 3.741766\n",
      "Train Epoch: 4 [865280/5599780 (15%)]\tLoss: 3.673142\n",
      "Train Epoch: 4 [870400/5599780 (16%)]\tLoss: 3.718429\n",
      "Train Epoch: 4 [875520/5599780 (16%)]\tLoss: 3.736781\n",
      "Train Epoch: 4 [880640/5599780 (16%)]\tLoss: 3.714833\n",
      "Train Epoch: 4 [885760/5599780 (16%)]\tLoss: 3.721185\n",
      "Train Epoch: 4 [890880/5599780 (16%)]\tLoss: 3.660657\n",
      "Train Epoch: 4 [896000/5599780 (16%)]\tLoss: 3.698529\n",
      "Train Epoch: 4 [901120/5599780 (16%)]\tLoss: 3.704911\n",
      "Train Epoch: 4 [906240/5599780 (16%)]\tLoss: 3.699050\n",
      "Train Epoch: 4 [911360/5599780 (16%)]\tLoss: 3.736182\n",
      "Train Epoch: 4 [916480/5599780 (16%)]\tLoss: 3.704948\n",
      "Train Epoch: 4 [921600/5599780 (16%)]\tLoss: 3.719372\n",
      "Train Epoch: 4 [926720/5599780 (17%)]\tLoss: 3.743968\n",
      "Train Epoch: 4 [931840/5599780 (17%)]\tLoss: 3.747807\n",
      "Train Epoch: 4 [936960/5599780 (17%)]\tLoss: 3.712724\n",
      "Train Epoch: 4 [942080/5599780 (17%)]\tLoss: 3.753777\n",
      "Train Epoch: 4 [947200/5599780 (17%)]\tLoss: 3.737054\n",
      "Train Epoch: 4 [952320/5599780 (17%)]\tLoss: 3.710871\n",
      "Train Epoch: 4 [957440/5599780 (17%)]\tLoss: 3.732202\n",
      "Train Epoch: 4 [962560/5599780 (17%)]\tLoss: 3.716500\n",
      "Train Epoch: 4 [967680/5599780 (17%)]\tLoss: 3.724059\n",
      "Train Epoch: 4 [972800/5599780 (17%)]\tLoss: 3.720534\n",
      "Train Epoch: 4 [977920/5599780 (17%)]\tLoss: 3.702775\n",
      "Train Epoch: 4 [983040/5599780 (18%)]\tLoss: 3.747817\n",
      "Train Epoch: 4 [988160/5599780 (18%)]\tLoss: 3.726491\n",
      "Train Epoch: 4 [993280/5599780 (18%)]\tLoss: 3.730328\n",
      "Train Epoch: 4 [998400/5599780 (18%)]\tLoss: 3.744793\n",
      "Train Epoch: 4 [1003520/5599780 (18%)]\tLoss: 3.711036\n",
      "Train Epoch: 4 [1008640/5599780 (18%)]\tLoss: 3.684886\n",
      "Train Epoch: 4 [1013760/5599780 (18%)]\tLoss: 3.698639\n",
      "Train Epoch: 4 [1018880/5599780 (18%)]\tLoss: 3.747262\n",
      "Train Epoch: 4 [1024000/5599780 (18%)]\tLoss: 3.703111\n",
      "Train Epoch: 4 [1029120/5599780 (18%)]\tLoss: 3.706853\n",
      "Train Epoch: 4 [1034240/5599780 (18%)]\tLoss: 3.689514\n",
      "Train Epoch: 4 [1039360/5599780 (19%)]\tLoss: 3.707130\n",
      "Train Epoch: 4 [1044480/5599780 (19%)]\tLoss: 3.714643\n",
      "Train Epoch: 4 [1049600/5599780 (19%)]\tLoss: 3.724535\n",
      "Train Epoch: 4 [1054720/5599780 (19%)]\tLoss: 3.708697\n",
      "Train Epoch: 4 [1059840/5599780 (19%)]\tLoss: 3.740330\n",
      "Train Epoch: 4 [1064960/5599780 (19%)]\tLoss: 3.706463\n",
      "Train Epoch: 4 [1070080/5599780 (19%)]\tLoss: 3.681501\n",
      "Train Epoch: 4 [1075200/5599780 (19%)]\tLoss: 3.717995\n",
      "Train Epoch: 4 [1080320/5599780 (19%)]\tLoss: 3.737470\n",
      "Train Epoch: 4 [1085440/5599780 (19%)]\tLoss: 3.683364\n",
      "Train Epoch: 4 [1090560/5599780 (19%)]\tLoss: 3.722549\n",
      "Train Epoch: 4 [1095680/5599780 (20%)]\tLoss: 3.712735\n",
      "Train Epoch: 4 [1100800/5599780 (20%)]\tLoss: 3.724168\n",
      "Train Epoch: 4 [1105920/5599780 (20%)]\tLoss: 3.730162\n",
      "Train Epoch: 4 [1111040/5599780 (20%)]\tLoss: 3.722136\n",
      "Train Epoch: 4 [1116160/5599780 (20%)]\tLoss: 3.746001\n",
      "Train Epoch: 4 [1121280/5599780 (20%)]\tLoss: 3.707001\n",
      "Train Epoch: 4 [1126400/5599780 (20%)]\tLoss: 3.721207\n",
      "Train Epoch: 4 [1131520/5599780 (20%)]\tLoss: 3.740052\n",
      "Train Epoch: 4 [1136640/5599780 (20%)]\tLoss: 3.704918\n",
      "Train Epoch: 4 [1141760/5599780 (20%)]\tLoss: 3.696928\n",
      "Train Epoch: 4 [1146880/5599780 (20%)]\tLoss: 3.739182\n",
      "Train Epoch: 4 [1152000/5599780 (21%)]\tLoss: 3.681505\n",
      "Train Epoch: 4 [1157120/5599780 (21%)]\tLoss: 3.716628\n",
      "Train Epoch: 4 [1162240/5599780 (21%)]\tLoss: 3.725853\n",
      "Train Epoch: 4 [1167360/5599780 (21%)]\tLoss: 3.736028\n",
      "Train Epoch: 4 [1172480/5599780 (21%)]\tLoss: 3.735872\n",
      "Train Epoch: 4 [1177600/5599780 (21%)]\tLoss: 3.720541\n",
      "Train Epoch: 4 [1182720/5599780 (21%)]\tLoss: 3.722436\n",
      "Train Epoch: 4 [1187840/5599780 (21%)]\tLoss: 3.697175\n",
      "Train Epoch: 4 [1192960/5599780 (21%)]\tLoss: 3.687361\n",
      "Train Epoch: 4 [1198080/5599780 (21%)]\tLoss: 3.704946\n",
      "Train Epoch: 4 [1203200/5599780 (21%)]\tLoss: 3.727649\n",
      "Train Epoch: 4 [1208320/5599780 (22%)]\tLoss: 3.699013\n",
      "Train Epoch: 4 [1213440/5599780 (22%)]\tLoss: 3.740024\n",
      "Train Epoch: 4 [1218560/5599780 (22%)]\tLoss: 3.685649\n",
      "Train Epoch: 4 [1223680/5599780 (22%)]\tLoss: 3.702939\n",
      "Train Epoch: 4 [1228800/5599780 (22%)]\tLoss: 3.765325\n",
      "Train Epoch: 4 [1233920/5599780 (22%)]\tLoss: 3.712682\n",
      "Train Epoch: 4 [1239040/5599780 (22%)]\tLoss: 3.747344\n",
      "Train Epoch: 4 [1244160/5599780 (22%)]\tLoss: 3.707273\n",
      "Train Epoch: 4 [1249280/5599780 (22%)]\tLoss: 3.708815\n",
      "Train Epoch: 4 [1254400/5599780 (22%)]\tLoss: 3.725006\n",
      "Train Epoch: 4 [1259520/5599780 (22%)]\tLoss: 3.696777\n",
      "Train Epoch: 4 [1264640/5599780 (23%)]\tLoss: 3.750479\n",
      "Train Epoch: 4 [1269760/5599780 (23%)]\tLoss: 3.722522\n",
      "Train Epoch: 4 [1274880/5599780 (23%)]\tLoss: 3.709153\n",
      "Train Epoch: 4 [1280000/5599780 (23%)]\tLoss: 3.716676\n",
      "Train Epoch: 4 [1285120/5599780 (23%)]\tLoss: 3.685350\n",
      "Train Epoch: 4 [1290240/5599780 (23%)]\tLoss: 3.706886\n",
      "Train Epoch: 4 [1295360/5599780 (23%)]\tLoss: 3.722489\n",
      "Train Epoch: 4 [1300480/5599780 (23%)]\tLoss: 3.695683\n",
      "Train Epoch: 4 [1305600/5599780 (23%)]\tLoss: 3.702929\n",
      "Train Epoch: 4 [1310720/5599780 (23%)]\tLoss: 3.722477\n",
      "Train Epoch: 4 [1315840/5599780 (23%)]\tLoss: 3.710771\n",
      "Train Epoch: 4 [1320960/5599780 (24%)]\tLoss: 3.697210\n",
      "Train Epoch: 4 [1326080/5599780 (24%)]\tLoss: 3.687328\n",
      "Train Epoch: 4 [1331200/5599780 (24%)]\tLoss: 3.708804\n",
      "Train Epoch: 4 [1336320/5599780 (24%)]\tLoss: 3.718742\n",
      "Train Epoch: 4 [1341440/5599780 (24%)]\tLoss: 3.749733\n",
      "Train Epoch: 4 [1346560/5599780 (24%)]\tLoss: 3.668834\n",
      "Train Epoch: 4 [1351680/5599780 (24%)]\tLoss: 3.708817\n",
      "Train Epoch: 4 [1356800/5599780 (24%)]\tLoss: 3.681562\n",
      "Train Epoch: 4 [1361920/5599780 (24%)]\tLoss: 3.711470\n",
      "Train Epoch: 4 [1367040/5599780 (24%)]\tLoss: 3.763408\n",
      "Train Epoch: 4 [1372160/5599780 (25%)]\tLoss: 3.690851\n",
      "Train Epoch: 4 [1377280/5599780 (25%)]\tLoss: 3.740004\n",
      "Train Epoch: 4 [1382400/5599780 (25%)]\tLoss: 3.689284\n",
      "Train Epoch: 4 [1387520/5599780 (25%)]\tLoss: 3.726490\n",
      "Train Epoch: 4 [1392640/5599780 (25%)]\tLoss: 3.755847\n",
      "Train Epoch: 4 [1397760/5599780 (25%)]\tLoss: 3.738970\n",
      "Train Epoch: 4 [1402880/5599780 (25%)]\tLoss: 3.685400\n",
      "Train Epoch: 4 [1408000/5599780 (25%)]\tLoss: 3.707125\n",
      "Train Epoch: 4 [1413120/5599780 (25%)]\tLoss: 3.686954\n",
      "Train Epoch: 4 [1418240/5599780 (25%)]\tLoss: 3.708361\n",
      "Train Epoch: 4 [1423360/5599780 (25%)]\tLoss: 3.687244\n",
      "Train Epoch: 4 [1428480/5599780 (26%)]\tLoss: 3.736039\n",
      "Train Epoch: 4 [1433600/5599780 (26%)]\tLoss: 3.702547\n",
      "Train Epoch: 4 [1438720/5599780 (26%)]\tLoss: 3.712723\n",
      "Train Epoch: 4 [1443840/5599780 (26%)]\tLoss: 3.745881\n",
      "Train Epoch: 4 [1448960/5599780 (26%)]\tLoss: 3.742017\n",
      "Train Epoch: 4 [1454080/5599780 (26%)]\tLoss: 3.705787\n",
      "Train Epoch: 4 [1459200/5599780 (26%)]\tLoss: 3.757375\n",
      "Train Epoch: 4 [1464320/5599780 (26%)]\tLoss: 3.754881\n",
      "Train Epoch: 4 [1469440/5599780 (26%)]\tLoss: 3.741762\n",
      "Train Epoch: 4 [1474560/5599780 (26%)]\tLoss: 3.669761\n",
      "Train Epoch: 4 [1479680/5599780 (26%)]\tLoss: 3.683424\n",
      "Train Epoch: 4 [1484800/5599780 (27%)]\tLoss: 3.698513\n",
      "Train Epoch: 4 [1489920/5599780 (27%)]\tLoss: 3.685577\n",
      "Train Epoch: 4 [1495040/5599780 (27%)]\tLoss: 3.693807\n",
      "Train Epoch: 4 [1500160/5599780 (27%)]\tLoss: 3.703509\n",
      "Train Epoch: 4 [1505280/5599780 (27%)]\tLoss: 3.696873\n",
      "Train Epoch: 4 [1510400/5599780 (27%)]\tLoss: 3.726408\n",
      "Train Epoch: 4 [1515520/5599780 (27%)]\tLoss: 3.709047\n",
      "Train Epoch: 4 [1520640/5599780 (27%)]\tLoss: 3.689372\n",
      "Train Epoch: 4 [1525760/5599780 (27%)]\tLoss: 3.689281\n",
      "Train Epoch: 4 [1530880/5599780 (27%)]\tLoss: 3.704907\n",
      "Train Epoch: 4 [1536000/5599780 (27%)]\tLoss: 3.646711\n",
      "Train Epoch: 4 [1541120/5599780 (28%)]\tLoss: 3.716479\n",
      "Train Epoch: 4 [1546240/5599780 (28%)]\tLoss: 3.653301\n",
      "Train Epoch: 4 [1551360/5599780 (28%)]\tLoss: 3.767802\n",
      "Train Epoch: 4 [1556480/5599780 (28%)]\tLoss: 3.675094\n",
      "Train Epoch: 4 [1561600/5599780 (28%)]\tLoss: 3.729893\n",
      "Train Epoch: 4 [1566720/5599780 (28%)]\tLoss: 3.721644\n",
      "Train Epoch: 4 [1571840/5599780 (28%)]\tLoss: 3.726537\n",
      "Train Epoch: 4 [1576960/5599780 (28%)]\tLoss: 3.732013\n",
      "Train Epoch: 4 [1582080/5599780 (28%)]\tLoss: 3.724466\n",
      "Train Epoch: 4 [1587200/5599780 (28%)]\tLoss: 3.708758\n",
      "Train Epoch: 4 [1592320/5599780 (28%)]\tLoss: 3.701028\n",
      "Train Epoch: 4 [1597440/5599780 (29%)]\tLoss: 3.703151\n",
      "Train Epoch: 4 [1602560/5599780 (29%)]\tLoss: 3.683500\n",
      "Train Epoch: 4 [1607680/5599780 (29%)]\tLoss: 3.710784\n",
      "Train Epoch: 4 [1612800/5599780 (29%)]\tLoss: 3.693419\n",
      "Train Epoch: 4 [1617920/5599780 (29%)]\tLoss: 3.720610\n",
      "Train Epoch: 4 [1623040/5599780 (29%)]\tLoss: 3.712607\n",
      "Train Epoch: 4 [1628160/5599780 (29%)]\tLoss: 3.669755\n",
      "Train Epoch: 4 [1633280/5599780 (29%)]\tLoss: 3.702775\n",
      "Train Epoch: 4 [1638400/5599780 (29%)]\tLoss: 3.708068\n",
      "Train Epoch: 4 [1643520/5599780 (29%)]\tLoss: 3.719322\n",
      "Train Epoch: 4 [1648640/5599780 (29%)]\tLoss: 3.736598\n",
      "Train Epoch: 4 [1653760/5599780 (30%)]\tLoss: 3.681161\n",
      "Train Epoch: 4 [1658880/5599780 (30%)]\tLoss: 3.708817\n",
      "Train Epoch: 4 [1664000/5599780 (30%)]\tLoss: 3.732257\n",
      "Train Epoch: 4 [1669120/5599780 (30%)]\tLoss: 3.704929\n",
      "Train Epoch: 4 [1674240/5599780 (30%)]\tLoss: 3.689466\n",
      "Train Epoch: 4 [1679360/5599780 (30%)]\tLoss: 3.694271\n",
      "Train Epoch: 4 [1684480/5599780 (30%)]\tLoss: 3.712655\n",
      "Train Epoch: 4 [1689600/5599780 (30%)]\tLoss: 3.698502\n",
      "Train Epoch: 4 [1694720/5599780 (30%)]\tLoss: 3.744009\n",
      "Train Epoch: 4 [1699840/5599780 (30%)]\tLoss: 3.697097\n",
      "Train Epoch: 4 [1704960/5599780 (30%)]\tLoss: 3.734205\n",
      "Train Epoch: 4 [1710080/5599780 (31%)]\tLoss: 3.706724\n",
      "Train Epoch: 4 [1715200/5599780 (31%)]\tLoss: 3.738118\n",
      "Train Epoch: 4 [1720320/5599780 (31%)]\tLoss: 3.706864\n",
      "Train Epoch: 4 [1725440/5599780 (31%)]\tLoss: 3.733135\n",
      "Train Epoch: 4 [1730560/5599780 (31%)]\tLoss: 3.709025\n",
      "Train Epoch: 4 [1735680/5599780 (31%)]\tLoss: 3.712513\n",
      "Train Epoch: 4 [1740800/5599780 (31%)]\tLoss: 3.722247\n",
      "Train Epoch: 4 [1745920/5599780 (31%)]\tLoss: 3.714554\n",
      "Train Epoch: 4 [1751040/5599780 (31%)]\tLoss: 3.677566\n",
      "Train Epoch: 4 [1756160/5599780 (31%)]\tLoss: 3.716270\n",
      "Train Epoch: 4 [1761280/5599780 (31%)]\tLoss: 3.747848\n",
      "Train Epoch: 4 [1766400/5599780 (32%)]\tLoss: 3.667927\n",
      "Train Epoch: 4 [1771520/5599780 (32%)]\tLoss: 3.703360\n",
      "Train Epoch: 4 [1776640/5599780 (32%)]\tLoss: 3.732434\n",
      "Train Epoch: 4 [1781760/5599780 (32%)]\tLoss: 3.764163\n",
      "Train Epoch: 4 [1786880/5599780 (32%)]\tLoss: 3.684364\n",
      "Train Epoch: 4 [1792000/5599780 (32%)]\tLoss: 3.713043\n",
      "Train Epoch: 4 [1797120/5599780 (32%)]\tLoss: 3.720530\n",
      "Train Epoch: 4 [1802240/5599780 (32%)]\tLoss: 3.716630\n",
      "Train Epoch: 4 [1807360/5599780 (32%)]\tLoss: 3.716630\n",
      "Train Epoch: 4 [1812480/5599780 (32%)]\tLoss: 3.740247\n",
      "Train Epoch: 4 [1817600/5599780 (32%)]\tLoss: 3.674124\n",
      "Train Epoch: 4 [1822720/5599780 (33%)]\tLoss: 3.727974\n",
      "Train Epoch: 4 [1827840/5599780 (33%)]\tLoss: 3.687395\n",
      "Train Epoch: 4 [1832960/5599780 (33%)]\tLoss: 3.695239\n",
      "Train Epoch: 4 [1838080/5599780 (33%)]\tLoss: 3.714619\n",
      "Train Epoch: 4 [1843200/5599780 (33%)]\tLoss: 3.708645\n",
      "Train Epoch: 4 [1848320/5599780 (33%)]\tLoss: 3.691238\n",
      "Train Epoch: 4 [1853440/5599780 (33%)]\tLoss: 3.714524\n",
      "Train Epoch: 4 [1858560/5599780 (33%)]\tLoss: 3.704830\n",
      "Train Epoch: 4 [1863680/5599780 (33%)]\tLoss: 3.667801\n",
      "Train Epoch: 4 [1868800/5599780 (33%)]\tLoss: 3.687868\n",
      "Train Epoch: 4 [1873920/5599780 (33%)]\tLoss: 3.672060\n",
      "Train Epoch: 4 [1879040/5599780 (34%)]\tLoss: 3.700821\n",
      "Train Epoch: 4 [1884160/5599780 (34%)]\tLoss: 3.732437\n",
      "Train Epoch: 4 [1889280/5599780 (34%)]\tLoss: 3.697087\n",
      "Train Epoch: 4 [1894400/5599780 (34%)]\tLoss: 3.691236\n",
      "Train Epoch: 4 [1899520/5599780 (34%)]\tLoss: 3.714597\n",
      "Train Epoch: 4 [1904640/5599780 (34%)]\tLoss: 3.730383\n",
      "Train Epoch: 4 [1909760/5599780 (34%)]\tLoss: 3.732075\n",
      "Train Epoch: 4 [1914880/5599780 (34%)]\tLoss: 3.702442\n",
      "Train Epoch: 4 [1920000/5599780 (34%)]\tLoss: 3.718850\n",
      "Train Epoch: 4 [1925120/5599780 (34%)]\tLoss: 3.711065\n",
      "Train Epoch: 4 [1930240/5599780 (34%)]\tLoss: 3.708812\n",
      "Train Epoch: 4 [1935360/5599780 (35%)]\tLoss: 3.706567\n",
      "Train Epoch: 4 [1940480/5599780 (35%)]\tLoss: 3.726254\n",
      "Train Epoch: 4 [1945600/5599780 (35%)]\tLoss: 3.708180\n",
      "Train Epoch: 4 [1950720/5599780 (35%)]\tLoss: 3.707472\n",
      "Train Epoch: 4 [1955840/5599780 (35%)]\tLoss: 3.730380\n",
      "Train Epoch: 4 [1960960/5599780 (35%)]\tLoss: 3.712802\n",
      "Train Epoch: 4 [1966080/5599780 (35%)]\tLoss: 3.731775\n",
      "Train Epoch: 4 [1971200/5599780 (35%)]\tLoss: 3.706349\n",
      "Train Epoch: 4 [1976320/5599780 (35%)]\tLoss: 3.738058\n",
      "Train Epoch: 4 [1981440/5599780 (35%)]\tLoss: 3.730020\n",
      "Train Epoch: 4 [1986560/5599780 (35%)]\tLoss: 3.736780\n",
      "Train Epoch: 4 [1991680/5599780 (36%)]\tLoss: 3.718792\n",
      "Train Epoch: 4 [1996800/5599780 (36%)]\tLoss: 3.714701\n",
      "Train Epoch: 4 [2001920/5599780 (36%)]\tLoss: 3.747825\n",
      "Train Epoch: 4 [2007040/5599780 (36%)]\tLoss: 3.739061\n",
      "Train Epoch: 4 [2012160/5599780 (36%)]\tLoss: 3.722489\n",
      "Train Epoch: 4 [2017280/5599780 (36%)]\tLoss: 3.694932\n",
      "Train Epoch: 4 [2022400/5599780 (36%)]\tLoss: 3.718718\n",
      "Train Epoch: 4 [2027520/5599780 (36%)]\tLoss: 3.679512\n",
      "Train Epoch: 4 [2032640/5599780 (36%)]\tLoss: 3.732391\n",
      "Train Epoch: 4 [2037760/5599780 (36%)]\tLoss: 3.714685\n",
      "Train Epoch: 4 [2042880/5599780 (36%)]\tLoss: 3.712841\n",
      "Train Epoch: 4 [2048000/5599780 (37%)]\tLoss: 3.718229\n",
      "Train Epoch: 4 [2053120/5599780 (37%)]\tLoss: 3.707213\n",
      "Train Epoch: 4 [2058240/5599780 (37%)]\tLoss: 3.738003\n",
      "Train Epoch: 4 [2063360/5599780 (37%)]\tLoss: 3.683640\n",
      "Train Epoch: 4 [2068480/5599780 (37%)]\tLoss: 3.739898\n",
      "Train Epoch: 4 [2073600/5599780 (37%)]\tLoss: 3.726420\n",
      "Train Epoch: 4 [2078720/5599780 (37%)]\tLoss: 3.706945\n",
      "Train Epoch: 4 [2083840/5599780 (37%)]\tLoss: 3.746697\n",
      "Train Epoch: 4 [2088960/5599780 (37%)]\tLoss: 3.708990\n",
      "Train Epoch: 4 [2094080/5599780 (37%)]\tLoss: 3.703741\n",
      "Train Epoch: 4 [2099200/5599780 (37%)]\tLoss: 3.704910\n",
      "Train Epoch: 4 [2104320/5599780 (38%)]\tLoss: 3.700691\n",
      "Train Epoch: 4 [2109440/5599780 (38%)]\tLoss: 3.742202\n",
      "Train Epoch: 4 [2114560/5599780 (38%)]\tLoss: 3.699697\n",
      "Train Epoch: 4 [2119680/5599780 (38%)]\tLoss: 3.695627\n",
      "Train Epoch: 4 [2124800/5599780 (38%)]\tLoss: 3.689769\n",
      "Train Epoch: 4 [2129920/5599780 (38%)]\tLoss: 3.756257\n",
      "Train Epoch: 4 [2135040/5599780 (38%)]\tLoss: 3.730407\n",
      "Train Epoch: 4 [2140160/5599780 (38%)]\tLoss: 3.771766\n",
      "Train Epoch: 4 [2145280/5599780 (38%)]\tLoss: 3.716623\n",
      "Train Epoch: 4 [2150400/5599780 (38%)]\tLoss: 3.700847\n",
      "Train Epoch: 4 [2155520/5599780 (38%)]\tLoss: 3.747514\n",
      "Train Epoch: 4 [2160640/5599780 (39%)]\tLoss: 3.705112\n",
      "Train Epoch: 4 [2165760/5599780 (39%)]\tLoss: 3.734226\n",
      "Train Epoch: 4 [2170880/5599780 (39%)]\tLoss: 3.730371\n",
      "Train Epoch: 4 [2176000/5599780 (39%)]\tLoss: 3.716666\n",
      "Train Epoch: 4 [2181120/5599780 (39%)]\tLoss: 3.725585\n",
      "Train Epoch: 4 [2186240/5599780 (39%)]\tLoss: 3.716808\n",
      "Train Epoch: 4 [2191360/5599780 (39%)]\tLoss: 3.724441\n",
      "Train Epoch: 4 [2196480/5599780 (39%)]\tLoss: 3.680497\n",
      "Train Epoch: 4 [2201600/5599780 (39%)]\tLoss: 3.728350\n",
      "Train Epoch: 4 [2206720/5599780 (39%)]\tLoss: 3.754177\n",
      "Train Epoch: 4 [2211840/5599780 (39%)]\tLoss: 3.681631\n",
      "Train Epoch: 4 [2216960/5599780 (40%)]\tLoss: 3.715116\n",
      "Train Epoch: 4 [2222080/5599780 (40%)]\tLoss: 3.727929\n",
      "Train Epoch: 4 [2227200/5599780 (40%)]\tLoss: 3.736160\n",
      "Train Epoch: 4 [2232320/5599780 (40%)]\tLoss: 3.745761\n",
      "Train Epoch: 4 [2237440/5599780 (40%)]\tLoss: 3.734463\n",
      "Train Epoch: 4 [2242560/5599780 (40%)]\tLoss: 3.697089\n",
      "Train Epoch: 4 [2247680/5599780 (40%)]\tLoss: 3.718582\n",
      "Train Epoch: 4 [2252800/5599780 (40%)]\tLoss: 3.761562\n",
      "Train Epoch: 4 [2257920/5599780 (40%)]\tLoss: 3.681902\n",
      "Train Epoch: 4 [2263040/5599780 (40%)]\tLoss: 3.730607\n",
      "Train Epoch: 4 [2268160/5599780 (41%)]\tLoss: 3.745838\n",
      "Train Epoch: 4 [2273280/5599780 (41%)]\tLoss: 3.693987\n",
      "Train Epoch: 4 [2278400/5599780 (41%)]\tLoss: 3.689542\n",
      "Train Epoch: 4 [2283520/5599780 (41%)]\tLoss: 3.728539\n",
      "Train Epoch: 4 [2288640/5599780 (41%)]\tLoss: 3.721635\n",
      "Train Epoch: 4 [2293760/5599780 (41%)]\tLoss: 3.751743\n",
      "Train Epoch: 4 [2298880/5599780 (41%)]\tLoss: 3.681524\n",
      "Train Epoch: 4 [2304000/5599780 (41%)]\tLoss: 3.674049\n",
      "Train Epoch: 4 [2309120/5599780 (41%)]\tLoss: 3.759426\n",
      "Train Epoch: 4 [2314240/5599780 (41%)]\tLoss: 3.681657\n",
      "Train Epoch: 4 [2319360/5599780 (41%)]\tLoss: 3.736161\n",
      "Train Epoch: 4 [2324480/5599780 (42%)]\tLoss: 3.729474\n",
      "Train Epoch: 4 [2329600/5599780 (42%)]\tLoss: 3.724440\n",
      "Train Epoch: 4 [2334720/5599780 (42%)]\tLoss: 3.722659\n",
      "Train Epoch: 4 [2339840/5599780 (42%)]\tLoss: 3.729917\n",
      "Train Epoch: 4 [2344960/5599780 (42%)]\tLoss: 3.695159\n",
      "Train Epoch: 4 [2350080/5599780 (42%)]\tLoss: 3.700635\n",
      "Train Epoch: 4 [2355200/5599780 (42%)]\tLoss: 3.745811\n",
      "Train Epoch: 4 [2360320/5599780 (42%)]\tLoss: 3.738113\n",
      "Train Epoch: 4 [2365440/5599780 (42%)]\tLoss: 3.710766\n",
      "Train Epoch: 4 [2370560/5599780 (42%)]\tLoss: 3.750228\n",
      "Train Epoch: 4 [2375680/5599780 (42%)]\tLoss: 3.751832\n",
      "Train Epoch: 4 [2380800/5599780 (43%)]\tLoss: 3.712596\n",
      "Train Epoch: 4 [2385920/5599780 (43%)]\tLoss: 3.709353\n",
      "Train Epoch: 4 [2391040/5599780 (43%)]\tLoss: 3.704908\n",
      "Train Epoch: 4 [2396160/5599780 (43%)]\tLoss: 3.718602\n",
      "Train Epoch: 4 [2401280/5599780 (43%)]\tLoss: 3.716899\n",
      "Train Epoch: 4 [2406400/5599780 (43%)]\tLoss: 3.718612\n",
      "Train Epoch: 4 [2411520/5599780 (43%)]\tLoss: 3.683609\n",
      "Train Epoch: 4 [2416640/5599780 (43%)]\tLoss: 3.720479\n",
      "Train Epoch: 4 [2421760/5599780 (43%)]\tLoss: 3.735669\n",
      "Train Epoch: 4 [2426880/5599780 (43%)]\tLoss: 3.718738\n",
      "Train Epoch: 4 [2432000/5599780 (43%)]\tLoss: 3.683406\n",
      "Train Epoch: 4 [2437120/5599780 (44%)]\tLoss: 3.736055\n",
      "Train Epoch: 4 [2442240/5599780 (44%)]\tLoss: 3.699055\n",
      "Train Epoch: 4 [2447360/5599780 (44%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [2452480/5599780 (44%)]\tLoss: 3.716548\n",
      "Train Epoch: 4 [2457600/5599780 (44%)]\tLoss: 3.713194\n",
      "Train Epoch: 4 [2462720/5599780 (44%)]\tLoss: 3.693275\n",
      "Train Epoch: 4 [2467840/5599780 (44%)]\tLoss: 3.712834\n",
      "Train Epoch: 4 [2472960/5599780 (44%)]\tLoss: 3.718582\n",
      "Train Epoch: 4 [2478080/5599780 (44%)]\tLoss: 3.705213\n",
      "Train Epoch: 4 [2483200/5599780 (44%)]\tLoss: 3.714086\n",
      "Train Epoch: 4 [2488320/5599780 (44%)]\tLoss: 3.695345\n",
      "Train Epoch: 4 [2493440/5599780 (45%)]\tLoss: 3.687439\n",
      "Train Epoch: 4 [2498560/5599780 (45%)]\tLoss: 3.713372\n",
      "Train Epoch: 4 [2503680/5599780 (45%)]\tLoss: 3.700372\n",
      "Train Epoch: 4 [2508800/5599780 (45%)]\tLoss: 3.693389\n",
      "Train Epoch: 4 [2513920/5599780 (45%)]\tLoss: 3.722419\n",
      "Train Epoch: 4 [2519040/5599780 (45%)]\tLoss: 3.722554\n",
      "Train Epoch: 4 [2524160/5599780 (45%)]\tLoss: 3.716648\n",
      "Train Epoch: 4 [2529280/5599780 (45%)]\tLoss: 3.730669\n",
      "Train Epoch: 4 [2534400/5599780 (45%)]\tLoss: 3.720487\n",
      "Train Epoch: 4 [2539520/5599780 (45%)]\tLoss: 3.738113\n",
      "Train Epoch: 4 [2544640/5599780 (45%)]\tLoss: 3.698564\n",
      "Train Epoch: 4 [2549760/5599780 (46%)]\tLoss: 3.724614\n",
      "Train Epoch: 4 [2554880/5599780 (46%)]\tLoss: 3.720495\n",
      "Train Epoch: 4 [2560000/5599780 (46%)]\tLoss: 3.716628\n",
      "Train Epoch: 4 [2565120/5599780 (46%)]\tLoss: 3.716534\n",
      "Train Epoch: 4 [2570240/5599780 (46%)]\tLoss: 3.702489\n",
      "Train Epoch: 4 [2575360/5599780 (46%)]\tLoss: 3.771654\n",
      "Train Epoch: 4 [2580480/5599780 (46%)]\tLoss: 3.716275\n",
      "Train Epoch: 4 [2585600/5599780 (46%)]\tLoss: 3.749883\n",
      "Train Epoch: 4 [2590720/5599780 (46%)]\tLoss: 3.685186\n",
      "Train Epoch: 4 [2595840/5599780 (46%)]\tLoss: 3.744009\n",
      "Train Epoch: 4 [2600960/5599780 (46%)]\tLoss: 3.724923\n",
      "Train Epoch: 4 [2606080/5599780 (47%)]\tLoss: 3.759730\n",
      "Train Epoch: 4 [2611200/5599780 (47%)]\tLoss: 3.731017\n",
      "Train Epoch: 4 [2616320/5599780 (47%)]\tLoss: 3.694973\n",
      "Train Epoch: 4 [2621440/5599780 (47%)]\tLoss: 3.708700\n",
      "Train Epoch: 4 [2626560/5599780 (47%)]\tLoss: 3.728355\n",
      "Train Epoch: 4 [2631680/5599780 (47%)]\tLoss: 3.722335\n",
      "Train Epoch: 4 [2636800/5599780 (47%)]\tLoss: 3.734508\n",
      "Train Epoch: 4 [2641920/5599780 (47%)]\tLoss: 3.745886\n",
      "Train Epoch: 4 [2647040/5599780 (47%)]\tLoss: 3.735118\n",
      "Train Epoch: 4 [2652160/5599780 (47%)]\tLoss: 3.663952\n",
      "Train Epoch: 4 [2657280/5599780 (47%)]\tLoss: 3.708748\n",
      "Train Epoch: 4 [2662400/5599780 (48%)]\tLoss: 3.716581\n",
      "Train Epoch: 4 [2667520/5599780 (48%)]\tLoss: 3.699430\n",
      "Train Epoch: 4 [2672640/5599780 (48%)]\tLoss: 3.724159\n",
      "Train Epoch: 4 [2677760/5599780 (48%)]\tLoss: 3.706786\n",
      "Train Epoch: 4 [2682880/5599780 (48%)]\tLoss: 3.734578\n",
      "Train Epoch: 4 [2688000/5599780 (48%)]\tLoss: 3.703030\n",
      "Train Epoch: 4 [2693120/5599780 (48%)]\tLoss: 3.702957\n",
      "Train Epoch: 4 [2698240/5599780 (48%)]\tLoss: 3.702956\n",
      "Train Epoch: 4 [2703360/5599780 (48%)]\tLoss: 3.703012\n",
      "Train Epoch: 4 [2708480/5599780 (48%)]\tLoss: 3.687301\n",
      "Train Epoch: 4 [2713600/5599780 (48%)]\tLoss: 3.729368\n",
      "Train Epoch: 4 [2718720/5599780 (49%)]\tLoss: 3.719978\n",
      "Train Epoch: 4 [2723840/5599780 (49%)]\tLoss: 3.697096\n",
      "Train Epoch: 4 [2728960/5599780 (49%)]\tLoss: 3.708946\n",
      "Train Epoch: 4 [2734080/5599780 (49%)]\tLoss: 3.683426\n",
      "Train Epoch: 4 [2739200/5599780 (49%)]\tLoss: 3.712700\n",
      "Train Epoch: 4 [2744320/5599780 (49%)]\tLoss: 3.726896\n",
      "Train Epoch: 4 [2749440/5599780 (49%)]\tLoss: 3.712567\n",
      "Train Epoch: 4 [2754560/5599780 (49%)]\tLoss: 3.716629\n",
      "Train Epoch: 4 [2759680/5599780 (49%)]\tLoss: 3.720569\n",
      "Train Epoch: 4 [2764800/5599780 (49%)]\tLoss: 3.716570\n",
      "Train Epoch: 4 [2769920/5599780 (49%)]\tLoss: 3.743887\n",
      "Train Epoch: 4 [2775040/5599780 (50%)]\tLoss: 3.699028\n",
      "Train Epoch: 4 [2780160/5599780 (50%)]\tLoss: 3.715051\n",
      "Train Epoch: 4 [2785280/5599780 (50%)]\tLoss: 3.704910\n",
      "Train Epoch: 4 [2790400/5599780 (50%)]\tLoss: 3.707066\n",
      "Train Epoch: 4 [2795520/5599780 (50%)]\tLoss: 3.728494\n",
      "Train Epoch: 4 [2800640/5599780 (50%)]\tLoss: 3.712719\n",
      "Train Epoch: 4 [2805760/5599780 (50%)]\tLoss: 3.681945\n",
      "Train Epoch: 4 [2810880/5599780 (50%)]\tLoss: 3.685477\n",
      "Train Epoch: 4 [2816000/5599780 (50%)]\tLoss: 3.718556\n",
      "Train Epoch: 4 [2821120/5599780 (50%)]\tLoss: 3.704910\n",
      "Train Epoch: 4 [2826240/5599780 (50%)]\tLoss: 3.685358\n",
      "Train Epoch: 4 [2831360/5599780 (51%)]\tLoss: 3.708827\n",
      "Train Epoch: 4 [2836480/5599780 (51%)]\tLoss: 3.726368\n",
      "Train Epoch: 4 [2841600/5599780 (51%)]\tLoss: 3.728348\n",
      "Train Epoch: 4 [2846720/5599780 (51%)]\tLoss: 3.720343\n",
      "Train Epoch: 4 [2851840/5599780 (51%)]\tLoss: 3.725758\n",
      "Train Epoch: 4 [2856960/5599780 (51%)]\tLoss: 3.716625\n",
      "Train Epoch: 4 [2862080/5599780 (51%)]\tLoss: 3.691456\n",
      "Train Epoch: 4 [2867200/5599780 (51%)]\tLoss: 3.726166\n",
      "Train Epoch: 4 [2872320/5599780 (51%)]\tLoss: 3.746086\n",
      "Train Epoch: 4 [2877440/5599780 (51%)]\tLoss: 3.718584\n",
      "Train Epoch: 4 [2882560/5599780 (51%)]\tLoss: 3.712722\n",
      "Train Epoch: 4 [2887680/5599780 (52%)]\tLoss: 3.728565\n",
      "Train Epoch: 4 [2892800/5599780 (52%)]\tLoss: 3.679527\n",
      "Train Epoch: 4 [2897920/5599780 (52%)]\tLoss: 3.751721\n",
      "Train Epoch: 4 [2903040/5599780 (52%)]\tLoss: 3.699053\n",
      "Train Epoch: 4 [2908160/5599780 (52%)]\tLoss: 3.710769\n",
      "Train Epoch: 4 [2913280/5599780 (52%)]\tLoss: 3.704457\n",
      "Train Epoch: 4 [2918400/5599780 (52%)]\tLoss: 3.715095\n",
      "Train Epoch: 4 [2923520/5599780 (52%)]\tLoss: 3.679493\n",
      "Train Epoch: 4 [2928640/5599780 (52%)]\tLoss: 3.718562\n",
      "Train Epoch: 4 [2933760/5599780 (52%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [2938880/5599780 (52%)]\tLoss: 3.710773\n",
      "Train Epoch: 4 [2944000/5599780 (53%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [2949120/5599780 (53%)]\tLoss: 3.685865\n",
      "Train Epoch: 4 [2954240/5599780 (53%)]\tLoss: 3.699163\n",
      "Train Epoch: 4 [2959360/5599780 (53%)]\tLoss: 3.691395\n",
      "Train Epoch: 4 [2964480/5599780 (53%)]\tLoss: 3.722488\n",
      "Train Epoch: 4 [2969600/5599780 (53%)]\tLoss: 3.677567\n",
      "Train Epoch: 4 [2974720/5599780 (53%)]\tLoss: 3.706869\n",
      "Train Epoch: 4 [2979840/5599780 (53%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [2984960/5599780 (53%)]\tLoss: 3.761551\n",
      "Train Epoch: 4 [2990080/5599780 (53%)]\tLoss: 3.701044\n",
      "Train Epoch: 4 [2995200/5599780 (53%)]\tLoss: 3.716625\n",
      "Train Epoch: 4 [3000320/5599780 (54%)]\tLoss: 3.716639\n",
      "Train Epoch: 4 [3005440/5599780 (54%)]\tLoss: 3.728385\n",
      "Train Epoch: 4 [3010560/5599780 (54%)]\tLoss: 3.693058\n",
      "Train Epoch: 4 [3015680/5599780 (54%)]\tLoss: 3.712888\n",
      "Train Epoch: 4 [3020800/5599780 (54%)]\tLoss: 3.691234\n",
      "Train Epoch: 4 [3025920/5599780 (54%)]\tLoss: 3.689301\n",
      "Train Epoch: 4 [3031040/5599780 (54%)]\tLoss: 3.673648\n",
      "Train Epoch: 4 [3036160/5599780 (54%)]\tLoss: 3.761550\n",
      "Train Epoch: 4 [3041280/5599780 (54%)]\tLoss: 3.683426\n",
      "Train Epoch: 4 [3046400/5599780 (54%)]\tLoss: 3.726385\n",
      "Train Epoch: 4 [3051520/5599780 (54%)]\tLoss: 3.720534\n",
      "Train Epoch: 4 [3056640/5599780 (55%)]\tLoss: 3.706790\n",
      "Train Epoch: 4 [3061760/5599780 (55%)]\tLoss: 3.727052\n",
      "Train Epoch: 4 [3066880/5599780 (55%)]\tLoss: 3.730289\n",
      "Train Epoch: 4 [3072000/5599780 (55%)]\tLoss: 3.730654\n",
      "Train Epoch: 4 [3077120/5599780 (55%)]\tLoss: 3.739870\n",
      "Train Epoch: 4 [3082240/5599780 (55%)]\tLoss: 3.695610\n",
      "Train Epoch: 4 [3087360/5599780 (55%)]\tLoss: 3.718583\n",
      "Train Epoch: 4 [3092480/5599780 (55%)]\tLoss: 3.703136\n",
      "Train Epoch: 4 [3097600/5599780 (55%)]\tLoss: 3.712551\n",
      "Train Epoch: 4 [3102720/5599780 (55%)]\tLoss: 3.710881\n",
      "Train Epoch: 4 [3107840/5599780 (55%)]\tLoss: 3.753740\n",
      "Train Epoch: 4 [3112960/5599780 (56%)]\tLoss: 3.728348\n",
      "Train Epoch: 4 [3118080/5599780 (56%)]\tLoss: 3.734204\n",
      "Train Epoch: 4 [3123200/5599780 (56%)]\tLoss: 3.743904\n",
      "Train Epoch: 4 [3128320/5599780 (56%)]\tLoss: 3.701162\n",
      "Train Epoch: 4 [3133440/5599780 (56%)]\tLoss: 3.699052\n",
      "Train Epoch: 4 [3138560/5599780 (56%)]\tLoss: 3.726197\n",
      "Train Epoch: 4 [3143680/5599780 (56%)]\tLoss: 3.720535\n",
      "Train Epoch: 4 [3148800/5599780 (56%)]\tLoss: 3.687508\n",
      "Train Epoch: 4 [3153920/5599780 (56%)]\tLoss: 3.708420\n",
      "Train Epoch: 4 [3159040/5599780 (56%)]\tLoss: 3.722488\n",
      "Train Epoch: 4 [3164160/5599780 (57%)]\tLoss: 3.702750\n",
      "Train Epoch: 4 [3169280/5599780 (57%)]\tLoss: 3.740605\n",
      "Train Epoch: 4 [3174400/5599780 (57%)]\tLoss: 3.742017\n",
      "Train Epoch: 4 [3179520/5599780 (57%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [3184640/5599780 (57%)]\tLoss: 3.761504\n",
      "Train Epoch: 4 [3189760/5599780 (57%)]\tLoss: 3.704911\n",
      "Train Epoch: 4 [3194880/5599780 (57%)]\tLoss: 3.696932\n",
      "Train Epoch: 4 [3200000/5599780 (57%)]\tLoss: 3.726394\n",
      "Train Epoch: 4 [3205120/5599780 (57%)]\tLoss: 3.714672\n",
      "Train Epoch: 4 [3210240/5599780 (57%)]\tLoss: 3.706964\n",
      "Train Epoch: 4 [3215360/5599780 (57%)]\tLoss: 3.697089\n",
      "Train Epoch: 4 [3220480/5599780 (58%)]\tLoss: 3.720866\n",
      "Train Epoch: 4 [3225600/5599780 (58%)]\tLoss: 3.742019\n",
      "Train Epoch: 4 [3230720/5599780 (58%)]\tLoss: 3.743682\n",
      "Train Epoch: 4 [3235840/5599780 (58%)]\tLoss: 3.689282\n",
      "Train Epoch: 4 [3240960/5599780 (58%)]\tLoss: 3.699037\n",
      "Train Epoch: 4 [3246080/5599780 (58%)]\tLoss: 3.746012\n",
      "Train Epoch: 4 [3251200/5599780 (58%)]\tLoss: 3.708420\n",
      "Train Epoch: 4 [3256320/5599780 (58%)]\tLoss: 3.712747\n",
      "Train Epoch: 4 [3261440/5599780 (58%)]\tLoss: 3.716796\n",
      "Train Epoch: 4 [3266560/5599780 (58%)]\tLoss: 3.679928\n",
      "Train Epoch: 4 [3271680/5599780 (58%)]\tLoss: 3.742077\n",
      "Train Epoch: 4 [3276800/5599780 (59%)]\tLoss: 3.695134\n",
      "Train Epoch: 4 [3281920/5599780 (59%)]\tLoss: 3.732245\n",
      "Train Epoch: 4 [3287040/5599780 (59%)]\tLoss: 3.685778\n",
      "Train Epoch: 4 [3292160/5599780 (59%)]\tLoss: 3.711053\n",
      "Train Epoch: 4 [3297280/5599780 (59%)]\tLoss: 3.757794\n",
      "Train Epoch: 4 [3302400/5599780 (59%)]\tLoss: 3.689295\n",
      "Train Epoch: 4 [3307520/5599780 (59%)]\tLoss: 3.722486\n",
      "Train Epoch: 4 [3312640/5599780 (59%)]\tLoss: 3.718581\n",
      "Train Epoch: 4 [3317760/5599780 (59%)]\tLoss: 3.699051\n",
      "Train Epoch: 4 [3322880/5599780 (59%)]\tLoss: 3.726393\n",
      "Train Epoch: 4 [3328000/5599780 (59%)]\tLoss: 3.726689\n",
      "Train Epoch: 4 [3333120/5599780 (60%)]\tLoss: 3.747859\n",
      "Train Epoch: 4 [3338240/5599780 (60%)]\tLoss: 3.763504\n",
      "Train Epoch: 4 [3343360/5599780 (60%)]\tLoss: 3.738115\n",
      "Train Epoch: 4 [3348480/5599780 (60%)]\tLoss: 3.699054\n",
      "Train Epoch: 4 [3353600/5599780 (60%)]\tLoss: 3.726388\n",
      "Train Epoch: 4 [3358720/5599780 (60%)]\tLoss: 3.736160\n",
      "Train Epoch: 4 [3363840/5599780 (60%)]\tLoss: 3.705369\n",
      "Train Epoch: 4 [3368960/5599780 (60%)]\tLoss: 3.747877\n",
      "Train Epoch: 4 [3374080/5599780 (60%)]\tLoss: 3.703152\n",
      "Train Epoch: 4 [3379200/5599780 (60%)]\tLoss: 3.681502\n",
      "Train Epoch: 4 [3384320/5599780 (60%)]\tLoss: 3.689285\n",
      "Train Epoch: 4 [3389440/5599780 (61%)]\tLoss: 3.730051\n",
      "Train Epoch: 4 [3394560/5599780 (61%)]\tLoss: 3.707148\n",
      "Train Epoch: 4 [3399680/5599780 (61%)]\tLoss: 3.722470\n",
      "Train Epoch: 4 [3404800/5599780 (61%)]\tLoss: 3.700847\n",
      "Train Epoch: 4 [3409920/5599780 (61%)]\tLoss: 3.699051\n",
      "Train Epoch: 4 [3415040/5599780 (61%)]\tLoss: 3.697097\n",
      "Train Epoch: 4 [3420160/5599780 (61%)]\tLoss: 3.738105\n",
      "Train Epoch: 4 [3425280/5599780 (61%)]\tLoss: 3.707148\n",
      "Train Epoch: 4 [3430400/5599780 (61%)]\tLoss: 3.755802\n",
      "Train Epoch: 4 [3435520/5599780 (61%)]\tLoss: 3.714659\n",
      "Train Epoch: 4 [3440640/5599780 (61%)]\tLoss: 3.693733\n",
      "Train Epoch: 4 [3445760/5599780 (62%)]\tLoss: 3.727960\n",
      "Train Epoch: 4 [3450880/5599780 (62%)]\tLoss: 3.708828\n",
      "Train Epoch: 4 [3456000/5599780 (62%)]\tLoss: 3.708761\n",
      "Train Epoch: 4 [3461120/5599780 (62%)]\tLoss: 3.701011\n",
      "Train Epoch: 4 [3466240/5599780 (62%)]\tLoss: 3.728347\n",
      "Train Epoch: 4 [3471360/5599780 (62%)]\tLoss: 3.667796\n",
      "Train Epoch: 4 [3476480/5599780 (62%)]\tLoss: 3.763093\n",
      "Train Epoch: 4 [3481600/5599780 (62%)]\tLoss: 3.699036\n",
      "Train Epoch: 4 [3486720/5599780 (62%)]\tLoss: 3.705095\n",
      "Train Epoch: 4 [3491840/5599780 (62%)]\tLoss: 3.673657\n",
      "Train Epoch: 4 [3496960/5599780 (62%)]\tLoss: 3.700968\n",
      "Train Epoch: 4 [3502080/5599780 (63%)]\tLoss: 3.753960\n",
      "Train Epoch: 4 [3507200/5599780 (63%)]\tLoss: 3.720338\n",
      "Train Epoch: 4 [3512320/5599780 (63%)]\tLoss: 3.704903\n",
      "Train Epoch: 4 [3517440/5599780 (63%)]\tLoss: 3.742075\n",
      "Train Epoch: 4 [3522560/5599780 (63%)]\tLoss: 3.710901\n",
      "Train Epoch: 4 [3527680/5599780 (63%)]\tLoss: 3.753738\n",
      "Train Epoch: 4 [3532800/5599780 (63%)]\tLoss: 3.737788\n",
      "Train Epoch: 4 [3537920/5599780 (63%)]\tLoss: 3.706777\n",
      "Train Epoch: 4 [3543040/5599780 (63%)]\tLoss: 3.722535\n",
      "Train Epoch: 4 [3548160/5599780 (63%)]\tLoss: 3.695141\n",
      "Train Epoch: 4 [3553280/5599780 (63%)]\tLoss: 3.712765\n",
      "Train Epoch: 4 [3558400/5599780 (64%)]\tLoss: 3.700939\n",
      "Train Epoch: 4 [3563520/5599780 (64%)]\tLoss: 3.742023\n",
      "Train Epoch: 4 [3568640/5599780 (64%)]\tLoss: 3.720574\n",
      "Train Epoch: 4 [3573760/5599780 (64%)]\tLoss: 3.714728\n",
      "Train Epoch: 4 [3578880/5599780 (64%)]\tLoss: 3.691239\n",
      "Train Epoch: 4 [3584000/5599780 (64%)]\tLoss: 3.695144\n",
      "Train Epoch: 4 [3589120/5599780 (64%)]\tLoss: 3.734179\n",
      "Train Epoch: 4 [3594240/5599780 (64%)]\tLoss: 3.745322\n",
      "Train Epoch: 4 [3599360/5599780 (64%)]\tLoss: 3.685364\n",
      "Train Epoch: 4 [3604480/5599780 (64%)]\tLoss: 3.722951\n",
      "Train Epoch: 4 [3609600/5599780 (64%)]\tLoss: 3.732254\n",
      "Train Epoch: 4 [3614720/5599780 (65%)]\tLoss: 3.774716\n",
      "Train Epoch: 4 [3619840/5599780 (65%)]\tLoss: 3.736219\n",
      "Train Epoch: 4 [3624960/5599780 (65%)]\tLoss: 3.724431\n",
      "Train Epoch: 4 [3630080/5599780 (65%)]\tLoss: 3.726436\n",
      "Train Epoch: 4 [3635200/5599780 (65%)]\tLoss: 3.736156\n",
      "Train Epoch: 4 [3640320/5599780 (65%)]\tLoss: 3.712698\n",
      "Train Epoch: 4 [3645440/5599780 (65%)]\tLoss: 3.714669\n",
      "Train Epoch: 4 [3650560/5599780 (65%)]\tLoss: 3.734109\n",
      "Train Epoch: 4 [3655680/5599780 (65%)]\tLoss: 3.693403\n",
      "Train Epoch: 4 [3660800/5599780 (65%)]\tLoss: 3.748200\n",
      "Train Epoch: 4 [3665920/5599780 (65%)]\tLoss: 3.748165\n",
      "Train Epoch: 4 [3671040/5599780 (66%)]\tLoss: 3.720559\n",
      "Train Epoch: 4 [3676160/5599780 (66%)]\tLoss: 3.728856\n",
      "Train Epoch: 4 [3681280/5599780 (66%)]\tLoss: 3.703178\n",
      "Train Epoch: 4 [3686400/5599780 (66%)]\tLoss: 3.714613\n",
      "Train Epoch: 4 [3691520/5599780 (66%)]\tLoss: 3.673656\n",
      "Train Epoch: 4 [3696640/5599780 (66%)]\tLoss: 3.730346\n",
      "Train Epoch: 4 [3701760/5599780 (66%)]\tLoss: 3.746447\n",
      "Train Epoch: 4 [3706880/5599780 (66%)]\tLoss: 3.743947\n",
      "Train Epoch: 4 [3712000/5599780 (66%)]\tLoss: 3.716692\n",
      "Train Epoch: 4 [3717120/5599780 (66%)]\tLoss: 3.718346\n",
      "Train Epoch: 4 [3722240/5599780 (66%)]\tLoss: 3.737863\n",
      "Train Epoch: 4 [3727360/5599780 (67%)]\tLoss: 3.730713\n",
      "Train Epoch: 4 [3732480/5599780 (67%)]\tLoss: 3.731662\n",
      "Train Epoch: 4 [3737600/5599780 (67%)]\tLoss: 3.718582\n",
      "Train Epoch: 4 [3742720/5599780 (67%)]\tLoss: 3.740084\n",
      "Train Epoch: 4 [3747840/5599780 (67%)]\tLoss: 3.726309\n",
      "Train Epoch: 4 [3752960/5599780 (67%)]\tLoss: 3.734201\n",
      "Train Epoch: 4 [3758080/5599780 (67%)]\tLoss: 3.693180\n",
      "Train Epoch: 4 [3763200/5599780 (67%)]\tLoss: 3.683381\n",
      "Train Epoch: 4 [3768320/5599780 (67%)]\tLoss: 3.724774\n",
      "Train Epoch: 4 [3773440/5599780 (67%)]\tLoss: 3.728324\n",
      "Train Epoch: 4 [3778560/5599780 (67%)]\tLoss: 3.740211\n",
      "Train Epoch: 4 [3783680/5599780 (68%)]\tLoss: 3.696884\n",
      "Train Epoch: 4 [3788800/5599780 (68%)]\tLoss: 3.714261\n",
      "Train Epoch: 4 [3793920/5599780 (68%)]\tLoss: 3.683862\n",
      "Train Epoch: 4 [3799040/5599780 (68%)]\tLoss: 3.724379\n",
      "Train Epoch: 4 [3804160/5599780 (68%)]\tLoss: 3.753630\n",
      "Train Epoch: 4 [3809280/5599780 (68%)]\tLoss: 3.718327\n",
      "Train Epoch: 4 [3814400/5599780 (68%)]\tLoss: 3.716633\n",
      "Train Epoch: 4 [3819520/5599780 (68%)]\tLoss: 3.718833\n",
      "Train Epoch: 4 [3824640/5599780 (68%)]\tLoss: 3.708795\n",
      "Train Epoch: 4 [3829760/5599780 (68%)]\tLoss: 3.730358\n",
      "Train Epoch: 4 [3834880/5599780 (68%)]\tLoss: 3.744036\n",
      "Train Epoch: 4 [3840000/5599780 (69%)]\tLoss: 3.693199\n",
      "Train Epoch: 4 [3845120/5599780 (69%)]\tLoss: 3.699073\n",
      "Train Epoch: 4 [3850240/5599780 (69%)]\tLoss: 3.749843\n",
      "Train Epoch: 4 [3855360/5599780 (69%)]\tLoss: 3.716629\n",
      "Train Epoch: 4 [3860480/5599780 (69%)]\tLoss: 3.708816\n",
      "Train Epoch: 4 [3865600/5599780 (69%)]\tLoss: 3.697039\n",
      "Train Epoch: 4 [3870720/5599780 (69%)]\tLoss: 3.714675\n",
      "Train Epoch: 4 [3875840/5599780 (69%)]\tLoss: 3.682936\n",
      "Train Epoch: 4 [3880960/5599780 (69%)]\tLoss: 3.767156\n",
      "Train Epoch: 4 [3886080/5599780 (69%)]\tLoss: 3.695351\n",
      "Train Epoch: 4 [3891200/5599780 (69%)]\tLoss: 3.728480\n",
      "Train Epoch: 4 [3896320/5599780 (70%)]\tLoss: 3.738473\n",
      "Train Epoch: 4 [3901440/5599780 (70%)]\tLoss: 3.685381\n",
      "Train Epoch: 4 [3906560/5599780 (70%)]\tLoss: 3.708994\n",
      "Train Epoch: 4 [3911680/5599780 (70%)]\tLoss: 3.704910\n",
      "Train Epoch: 4 [3916800/5599780 (70%)]\tLoss: 3.738146\n",
      "Train Epoch: 4 [3921920/5599780 (70%)]\tLoss: 3.755667\n",
      "Train Epoch: 4 [3927040/5599780 (70%)]\tLoss: 3.679518\n",
      "Train Epoch: 4 [3932160/5599780 (70%)]\tLoss: 3.734207\n",
      "Train Epoch: 4 [3937280/5599780 (70%)]\tLoss: 3.730468\n",
      "Train Epoch: 4 [3942400/5599780 (70%)]\tLoss: 3.728447\n",
      "Train Epoch: 4 [3947520/5599780 (70%)]\tLoss: 3.749823\n",
      "Train Epoch: 4 [3952640/5599780 (71%)]\tLoss: 3.671703\n",
      "Train Epoch: 4 [3957760/5599780 (71%)]\tLoss: 3.722486\n",
      "Train Epoch: 4 [3962880/5599780 (71%)]\tLoss: 3.685486\n",
      "Train Epoch: 4 [3968000/5599780 (71%)]\tLoss: 3.650110\n",
      "Train Epoch: 4 [3973120/5599780 (71%)]\tLoss: 3.732255\n",
      "Train Epoch: 4 [3978240/5599780 (71%)]\tLoss: 3.699318\n",
      "Train Epoch: 4 [3983360/5599780 (71%)]\tLoss: 3.714446\n",
      "Train Epoch: 4 [3988480/5599780 (71%)]\tLoss: 3.702966\n",
      "Train Epoch: 4 [3993600/5599780 (71%)]\tLoss: 3.716282\n",
      "Train Epoch: 4 [3998720/5599780 (71%)]\tLoss: 3.732795\n",
      "Train Epoch: 4 [4003840/5599780 (71%)]\tLoss: 3.751785\n",
      "Train Epoch: 4 [4008960/5599780 (72%)]\tLoss: 3.724817\n",
      "Train Epoch: 4 [4014080/5599780 (72%)]\tLoss: 3.736480\n",
      "Train Epoch: 4 [4019200/5599780 (72%)]\tLoss: 3.685644\n",
      "Train Epoch: 4 [4024320/5599780 (72%)]\tLoss: 3.712723\n",
      "Train Epoch: 4 [4029440/5599780 (72%)]\tLoss: 3.679432\n",
      "Train Epoch: 4 [4034560/5599780 (72%)]\tLoss: 3.702957\n",
      "Train Epoch: 4 [4039680/5599780 (72%)]\tLoss: 3.724441\n",
      "Train Epoch: 4 [4044800/5599780 (72%)]\tLoss: 3.743970\n",
      "Train Epoch: 4 [4049920/5599780 (72%)]\tLoss: 3.728333\n",
      "Train Epoch: 4 [4055040/5599780 (72%)]\tLoss: 3.669642\n",
      "Train Epoch: 4 [4060160/5599780 (72%)]\tLoss: 3.744413\n",
      "Train Epoch: 4 [4065280/5599780 (73%)]\tLoss: 3.724439\n",
      "Train Epoch: 4 [4070400/5599780 (73%)]\tLoss: 3.728347\n",
      "Train Epoch: 4 [4075520/5599780 (73%)]\tLoss: 3.714676\n",
      "Train Epoch: 4 [4080640/5599780 (73%)]\tLoss: 3.763674\n",
      "Train Epoch: 4 [4085760/5599780 (73%)]\tLoss: 3.708726\n",
      "Train Epoch: 4 [4090880/5599780 (73%)]\tLoss: 3.699170\n",
      "Train Epoch: 4 [4096000/5599780 (73%)]\tLoss: 3.738113\n",
      "Train Epoch: 4 [4101120/5599780 (73%)]\tLoss: 3.734206\n",
      "Train Epoch: 4 [4106240/5599780 (73%)]\tLoss: 3.745965\n",
      "Train Epoch: 4 [4111360/5599780 (73%)]\tLoss: 3.714812\n",
      "Train Epoch: 4 [4116480/5599780 (74%)]\tLoss: 3.701004\n",
      "Train Epoch: 4 [4121600/5599780 (74%)]\tLoss: 3.704943\n",
      "Train Epoch: 4 [4126720/5599780 (74%)]\tLoss: 3.723093\n",
      "Train Epoch: 4 [4131840/5599780 (74%)]\tLoss: 3.710499\n",
      "Train Epoch: 4 [4136960/5599780 (74%)]\tLoss: 3.697510\n",
      "Train Epoch: 4 [4142080/5599780 (74%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [4147200/5599780 (74%)]\tLoss: 3.728460\n",
      "Train Epoch: 4 [4152320/5599780 (74%)]\tLoss: 3.726394\n",
      "Train Epoch: 4 [4157440/5599780 (74%)]\tLoss: 3.677684\n",
      "Train Epoch: 4 [4162560/5599780 (74%)]\tLoss: 3.704912\n",
      "Train Epoch: 4 [4167680/5599780 (74%)]\tLoss: 3.716629\n",
      "Train Epoch: 4 [4172800/5599780 (75%)]\tLoss: 3.742019\n",
      "Train Epoch: 4 [4177920/5599780 (75%)]\tLoss: 3.742017\n",
      "Train Epoch: 4 [4183040/5599780 (75%)]\tLoss: 3.735711\n",
      "Train Epoch: 4 [4188160/5599780 (75%)]\tLoss: 3.687330\n",
      "Train Epoch: 4 [4193280/5599780 (75%)]\tLoss: 3.738094\n",
      "Train Epoch: 4 [4198400/5599780 (75%)]\tLoss: 3.701022\n",
      "Train Epoch: 4 [4203520/5599780 (75%)]\tLoss: 3.753680\n",
      "Train Epoch: 4 [4208640/5599780 (75%)]\tLoss: 3.742023\n",
      "Train Epoch: 4 [4213760/5599780 (75%)]\tLoss: 3.695085\n",
      "Train Epoch: 4 [4218880/5599780 (75%)]\tLoss: 3.713033\n",
      "Train Epoch: 4 [4224000/5599780 (75%)]\tLoss: 3.697098\n",
      "Train Epoch: 4 [4229120/5599780 (76%)]\tLoss: 3.681792\n",
      "Train Epoch: 4 [4234240/5599780 (76%)]\tLoss: 3.689313\n",
      "Train Epoch: 4 [4239360/5599780 (76%)]\tLoss: 3.724299\n",
      "Train Epoch: 4 [4244480/5599780 (76%)]\tLoss: 3.702916\n",
      "Train Epoch: 4 [4249600/5599780 (76%)]\tLoss: 3.757644\n",
      "Train Epoch: 4 [4254720/5599780 (76%)]\tLoss: 3.714630\n",
      "Train Epoch: 4 [4259840/5599780 (76%)]\tLoss: 3.788894\n",
      "Train Epoch: 4 [4264960/5599780 (76%)]\tLoss: 3.706484\n",
      "Train Epoch: 4 [4270080/5599780 (76%)]\tLoss: 3.728303\n",
      "Train Epoch: 4 [4275200/5599780 (76%)]\tLoss: 3.724498\n",
      "Train Epoch: 4 [4280320/5599780 (76%)]\tLoss: 3.699048\n",
      "Train Epoch: 4 [4285440/5599780 (77%)]\tLoss: 3.734202\n",
      "Train Epoch: 4 [4290560/5599780 (77%)]\tLoss: 3.706885\n",
      "Train Epoch: 4 [4295680/5599780 (77%)]\tLoss: 3.749895\n",
      "Train Epoch: 4 [4300800/5599780 (77%)]\tLoss: 3.738123\n",
      "Train Epoch: 4 [4305920/5599780 (77%)]\tLoss: 3.689292\n",
      "Train Epoch: 4 [4311040/5599780 (77%)]\tLoss: 3.710696\n",
      "Train Epoch: 4 [4316160/5599780 (77%)]\tLoss: 3.697191\n",
      "Train Epoch: 4 [4321280/5599780 (77%)]\tLoss: 3.706888\n",
      "Train Epoch: 4 [4326400/5599780 (77%)]\tLoss: 3.749832\n",
      "Train Epoch: 4 [4331520/5599780 (77%)]\tLoss: 3.730759\n",
      "Train Epoch: 4 [4336640/5599780 (77%)]\tLoss: 3.730756\n",
      "Train Epoch: 4 [4341760/5599780 (78%)]\tLoss: 3.691238\n",
      "Train Epoch: 4 [4346880/5599780 (78%)]\tLoss: 3.701003\n",
      "Train Epoch: 4 [4352000/5599780 (78%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [4357120/5599780 (78%)]\tLoss: 3.720549\n",
      "Train Epoch: 4 [4362240/5599780 (78%)]\tLoss: 3.708878\n",
      "Train Epoch: 4 [4367360/5599780 (78%)]\tLoss: 3.728236\n",
      "Train Epoch: 4 [4372480/5599780 (78%)]\tLoss: 3.710870\n",
      "Train Epoch: 4 [4377600/5599780 (78%)]\tLoss: 3.706864\n",
      "Train Epoch: 4 [4382720/5599780 (78%)]\tLoss: 3.683830\n",
      "Train Epoch: 4 [4387840/5599780 (78%)]\tLoss: 3.749806\n",
      "Train Epoch: 4 [4392960/5599780 (78%)]\tLoss: 3.710767\n",
      "Train Epoch: 4 [4398080/5599780 (79%)]\tLoss: 3.747878\n",
      "Train Epoch: 4 [4403200/5599780 (79%)]\tLoss: 3.720313\n",
      "Train Epoch: 4 [4408320/5599780 (79%)]\tLoss: 3.705013\n",
      "Train Epoch: 4 [4413440/5599780 (79%)]\tLoss: 3.677566\n",
      "Train Epoch: 4 [4418560/5599780 (79%)]\tLoss: 3.703210\n",
      "Train Epoch: 4 [4423680/5599780 (79%)]\tLoss: 3.724158\n",
      "Train Epoch: 4 [4428800/5599780 (79%)]\tLoss: 3.751980\n",
      "Train Epoch: 4 [4433920/5599780 (79%)]\tLoss: 3.736161\n",
      "Train Epoch: 4 [4439040/5599780 (79%)]\tLoss: 3.718582\n",
      "Train Epoch: 4 [4444160/5599780 (79%)]\tLoss: 3.753457\n",
      "Train Epoch: 4 [4449280/5599780 (79%)]\tLoss: 3.738226\n",
      "Train Epoch: 4 [4454400/5599780 (80%)]\tLoss: 3.765463\n",
      "Train Epoch: 4 [4459520/5599780 (80%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [4464640/5599780 (80%)]\tLoss: 3.738125\n",
      "Train Epoch: 4 [4469760/5599780 (80%)]\tLoss: 3.679238\n",
      "Train Epoch: 4 [4474880/5599780 (80%)]\tLoss: 3.691257\n",
      "Train Epoch: 4 [4480000/5599780 (80%)]\tLoss: 3.741991\n",
      "Train Epoch: 4 [4485120/5599780 (80%)]\tLoss: 3.724140\n",
      "Train Epoch: 4 [4490240/5599780 (80%)]\tLoss: 3.702961\n",
      "Train Epoch: 4 [4495360/5599780 (80%)]\tLoss: 3.734424\n",
      "Train Epoch: 4 [4500480/5599780 (80%)]\tLoss: 3.726361\n",
      "Train Epoch: 4 [4505600/5599780 (80%)]\tLoss: 3.710731\n",
      "Train Epoch: 4 [4510720/5599780 (81%)]\tLoss: 3.714679\n",
      "Train Epoch: 4 [4515840/5599780 (81%)]\tLoss: 3.700955\n",
      "Train Epoch: 4 [4520960/5599780 (81%)]\tLoss: 3.720638\n",
      "Train Epoch: 4 [4526080/5599780 (81%)]\tLoss: 3.714676\n",
      "Train Epoch: 4 [4531200/5599780 (81%)]\tLoss: 3.726407\n",
      "Train Epoch: 4 [4536320/5599780 (81%)]\tLoss: 3.712629\n",
      "Train Epoch: 4 [4541440/5599780 (81%)]\tLoss: 3.695135\n",
      "Train Epoch: 4 [4546560/5599780 (81%)]\tLoss: 3.692920\n",
      "Train Epoch: 4 [4551680/5599780 (81%)]\tLoss: 3.685378\n",
      "Train Epoch: 4 [4556800/5599780 (81%)]\tLoss: 3.671694\n",
      "Train Epoch: 4 [4561920/5599780 (81%)]\tLoss: 3.726390\n",
      "Train Epoch: 4 [4567040/5599780 (82%)]\tLoss: 3.740066\n",
      "Train Epoch: 4 [4572160/5599780 (82%)]\tLoss: 3.740066\n",
      "Train Epoch: 4 [4577280/5599780 (82%)]\tLoss: 3.708836\n",
      "Train Epoch: 4 [4582400/5599780 (82%)]\tLoss: 3.728677\n",
      "Train Epoch: 4 [4587520/5599780 (82%)]\tLoss: 3.745930\n",
      "Train Epoch: 4 [4592640/5599780 (82%)]\tLoss: 3.693308\n",
      "Train Epoch: 4 [4597760/5599780 (82%)]\tLoss: 3.687332\n",
      "Train Epoch: 4 [4602880/5599780 (82%)]\tLoss: 3.749831\n",
      "Train Epoch: 4 [4608000/5599780 (82%)]\tLoss: 3.726379\n",
      "Train Epoch: 4 [4613120/5599780 (82%)]\tLoss: 3.726475\n",
      "Train Epoch: 4 [4618240/5599780 (82%)]\tLoss: 3.697090\n",
      "Train Epoch: 4 [4623360/5599780 (83%)]\tLoss: 3.706605\n",
      "Train Epoch: 4 [4628480/5599780 (83%)]\tLoss: 3.707181\n",
      "Train Epoch: 4 [4633600/5599780 (83%)]\tLoss: 3.734214\n",
      "Train Epoch: 4 [4638720/5599780 (83%)]\tLoss: 3.724694\n",
      "Train Epoch: 4 [4643840/5599780 (83%)]\tLoss: 3.736107\n",
      "Train Epoch: 4 [4648960/5599780 (83%)]\tLoss: 3.722487\n",
      "Train Epoch: 4 [4654080/5599780 (83%)]\tLoss: 3.747813\n",
      "Train Epoch: 4 [4659200/5599780 (83%)]\tLoss: 3.718562\n",
      "Train Epoch: 4 [4664320/5599780 (83%)]\tLoss: 3.673660\n",
      "Train Epoch: 4 [4669440/5599780 (83%)]\tLoss: 3.724451\n",
      "Train Epoch: 4 [4674560/5599780 (83%)]\tLoss: 3.704975\n",
      "Train Epoch: 4 [4679680/5599780 (84%)]\tLoss: 3.732324\n",
      "Train Epoch: 4 [4684800/5599780 (84%)]\tLoss: 3.710749\n",
      "Train Epoch: 4 [4689920/5599780 (84%)]\tLoss: 3.712721\n",
      "Train Epoch: 4 [4695040/5599780 (84%)]\tLoss: 3.722678\n",
      "Train Epoch: 4 [4700160/5599780 (84%)]\tLoss: 3.732254\n",
      "Train Epoch: 4 [4705280/5599780 (84%)]\tLoss: 3.722092\n",
      "Train Epoch: 4 [4710400/5599780 (84%)]\tLoss: 3.737957\n",
      "Train Epoch: 4 [4715520/5599780 (84%)]\tLoss: 3.712727\n",
      "Train Epoch: 4 [4720640/5599780 (84%)]\tLoss: 3.755647\n",
      "Train Epoch: 4 [4725760/5599780 (84%)]\tLoss: 3.699049\n",
      "Train Epoch: 4 [4730880/5599780 (84%)]\tLoss: 3.726391\n",
      "Train Epoch: 4 [4736000/5599780 (85%)]\tLoss: 3.736154\n",
      "Train Epoch: 4 [4741120/5599780 (85%)]\tLoss: 3.699050\n",
      "Train Epoch: 4 [4746240/5599780 (85%)]\tLoss: 3.726867\n",
      "Train Epoch: 4 [4751360/5599780 (85%)]\tLoss: 3.708816\n",
      "Train Epoch: 4 [4756480/5599780 (85%)]\tLoss: 3.700964\n",
      "Train Epoch: 4 [4761600/5599780 (85%)]\tLoss: 3.706945\n",
      "Train Epoch: 4 [4766720/5599780 (85%)]\tLoss: 3.718578\n",
      "Train Epoch: 4 [4771840/5599780 (85%)]\tLoss: 3.671703\n",
      "Train Epoch: 4 [4776960/5599780 (85%)]\tLoss: 3.740066\n",
      "Train Epoch: 4 [4782080/5599780 (85%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [4787200/5599780 (85%)]\tLoss: 3.749815\n",
      "Train Epoch: 4 [4792320/5599780 (86%)]\tLoss: 3.700994\n",
      "Train Epoch: 4 [4797440/5599780 (86%)]\tLoss: 3.712854\n",
      "Train Epoch: 4 [4802560/5599780 (86%)]\tLoss: 3.684319\n",
      "Train Epoch: 4 [4807680/5599780 (86%)]\tLoss: 3.710746\n",
      "Train Epoch: 4 [4812800/5599780 (86%)]\tLoss: 3.724263\n",
      "Train Epoch: 4 [4817920/5599780 (86%)]\tLoss: 3.712724\n",
      "Train Epoch: 4 [4823040/5599780 (86%)]\tLoss: 3.747879\n",
      "Train Epoch: 4 [4828160/5599780 (86%)]\tLoss: 3.720536\n",
      "Train Epoch: 4 [4833280/5599780 (86%)]\tLoss: 3.663894\n",
      "Train Epoch: 4 [4838400/5599780 (86%)]\tLoss: 3.732253\n",
      "Train Epoch: 4 [4843520/5599780 (86%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [4848640/5599780 (87%)]\tLoss: 3.706864\n",
      "Train Epoch: 4 [4853760/5599780 (87%)]\tLoss: 3.734207\n",
      "Train Epoch: 4 [4858880/5599780 (87%)]\tLoss: 3.747870\n",
      "Train Epoch: 4 [4864000/5599780 (87%)]\tLoss: 3.740094\n",
      "Train Epoch: 4 [4869120/5599780 (87%)]\tLoss: 3.740261\n",
      "Train Epoch: 4 [4874240/5599780 (87%)]\tLoss: 3.751785\n",
      "Train Epoch: 4 [4879360/5599780 (87%)]\tLoss: 3.724108\n",
      "Train Epoch: 4 [4884480/5599780 (87%)]\tLoss: 3.746370\n",
      "Train Epoch: 4 [4889600/5599780 (87%)]\tLoss: 3.714691\n",
      "Train Epoch: 4 [4894720/5599780 (87%)]\tLoss: 3.683426\n",
      "Train Epoch: 4 [4899840/5599780 (87%)]\tLoss: 3.699051\n",
      "Train Epoch: 4 [4904960/5599780 (88%)]\tLoss: 3.720542\n",
      "Train Epoch: 4 [4910080/5599780 (88%)]\tLoss: 3.724442\n",
      "Train Epoch: 4 [4915200/5599780 (88%)]\tLoss: 3.708988\n",
      "Train Epoch: 4 [4920320/5599780 (88%)]\tLoss: 3.738076\n",
      "Train Epoch: 4 [4925440/5599780 (88%)]\tLoss: 3.703012\n",
      "Train Epoch: 4 [4930560/5599780 (88%)]\tLoss: 3.710752\n",
      "Train Epoch: 4 [4935680/5599780 (88%)]\tLoss: 3.734225\n",
      "Train Epoch: 4 [4940800/5599780 (88%)]\tLoss: 3.741699\n",
      "Train Epoch: 4 [4945920/5599780 (88%)]\tLoss: 3.720752\n",
      "Train Epoch: 4 [4951040/5599780 (88%)]\tLoss: 3.714933\n",
      "Train Epoch: 4 [4956160/5599780 (88%)]\tLoss: 3.703556\n",
      "Train Epoch: 4 [4961280/5599780 (89%)]\tLoss: 3.701623\n",
      "Train Epoch: 4 [4966400/5599780 (89%)]\tLoss: 3.759597\n",
      "Train Epoch: 4 [4971520/5599780 (89%)]\tLoss: 3.738122\n",
      "Train Epoch: 4 [4976640/5599780 (89%)]\tLoss: 3.734207\n",
      "Train Epoch: 4 [4981760/5599780 (89%)]\tLoss: 3.709169\n",
      "Train Epoch: 4 [4986880/5599780 (89%)]\tLoss: 3.743973\n",
      "Train Epoch: 4 [4992000/5599780 (89%)]\tLoss: 3.714613\n",
      "Train Epoch: 4 [4997120/5599780 (89%)]\tLoss: 3.703035\n",
      "Train Epoch: 4 [5002240/5599780 (89%)]\tLoss: 3.745919\n",
      "Train Epoch: 4 [5007360/5599780 (89%)]\tLoss: 3.699065\n",
      "Train Epoch: 4 [5012480/5599780 (90%)]\tLoss: 3.667820\n",
      "Train Epoch: 4 [5017600/5599780 (90%)]\tLoss: 3.704781\n",
      "Train Epoch: 4 [5022720/5599780 (90%)]\tLoss: 3.698927\n",
      "Train Epoch: 4 [5027840/5599780 (90%)]\tLoss: 3.718580\n",
      "Train Epoch: 4 [5032960/5599780 (90%)]\tLoss: 3.736160\n",
      "Train Epoch: 4 [5038080/5599780 (90%)]\tLoss: 3.710769\n",
      "Train Epoch: 4 [5043200/5599780 (90%)]\tLoss: 3.656088\n",
      "Train Epoch: 4 [5048320/5599780 (90%)]\tLoss: 3.751785\n",
      "Train Epoch: 4 [5053440/5599780 (90%)]\tLoss: 3.738221\n",
      "Train Epoch: 4 [5058560/5599780 (90%)]\tLoss: 3.706864\n",
      "Train Epoch: 4 [5063680/5599780 (90%)]\tLoss: 3.701003\n",
      "Train Epoch: 4 [5068800/5599780 (91%)]\tLoss: 3.735982\n",
      "Train Epoch: 4 [5073920/5599780 (91%)]\tLoss: 3.687333\n",
      "Train Epoch: 4 [5079040/5599780 (91%)]\tLoss: 3.718582\n",
      "Train Epoch: 4 [5084160/5599780 (91%)]\tLoss: 3.710774\n",
      "Train Epoch: 4 [5089280/5599780 (91%)]\tLoss: 3.699037\n",
      "Train Epoch: 4 [5094400/5599780 (91%)]\tLoss: 3.734259\n",
      "Train Epoch: 4 [5099520/5599780 (91%)]\tLoss: 3.673868\n",
      "Train Epoch: 4 [5104640/5599780 (91%)]\tLoss: 3.753740\n",
      "Train Epoch: 4 [5109760/5599780 (91%)]\tLoss: 3.718396\n",
      "Train Epoch: 4 [5114880/5599780 (91%)]\tLoss: 3.724317\n",
      "Train Epoch: 4 [5120000/5599780 (91%)]\tLoss: 3.749723\n",
      "Train Epoch: 4 [5125120/5599780 (92%)]\tLoss: 3.706864\n",
      "Train Epoch: 4 [5130240/5599780 (92%)]\tLoss: 3.705477\n",
      "Train Epoch: 4 [5135360/5599780 (92%)]\tLoss: 3.675383\n",
      "Train Epoch: 4 [5140480/5599780 (92%)]\tLoss: 3.742019\n",
      "Train Epoch: 4 [5145600/5599780 (92%)]\tLoss: 3.708410\n",
      "Train Epoch: 4 [5150720/5599780 (92%)]\tLoss: 3.738106\n",
      "Train Epoch: 4 [5155840/5599780 (92%)]\tLoss: 3.704912\n",
      "Train Epoch: 4 [5160960/5599780 (92%)]\tLoss: 3.693191\n",
      "Train Epoch: 4 [5166080/5599780 (92%)]\tLoss: 3.693232\n",
      "Train Epoch: 4 [5171200/5599780 (92%)]\tLoss: 3.706889\n",
      "Train Epoch: 4 [5176320/5599780 (92%)]\tLoss: 3.722674\n",
      "Train Epoch: 4 [5181440/5599780 (93%)]\tLoss: 3.753821\n",
      "Train Epoch: 4 [5186560/5599780 (93%)]\tLoss: 3.769363\n",
      "Train Epoch: 4 [5191680/5599780 (93%)]\tLoss: 3.747879\n",
      "Train Epoch: 4 [5196800/5599780 (93%)]\tLoss: 3.702955\n",
      "Train Epoch: 4 [5201920/5599780 (93%)]\tLoss: 3.682998\n",
      "Train Epoch: 4 [5207040/5599780 (93%)]\tLoss: 3.722488\n",
      "Train Epoch: 4 [5212160/5599780 (93%)]\tLoss: 3.710769\n",
      "Train Epoch: 4 [5217280/5599780 (93%)]\tLoss: 3.734208\n",
      "Train Epoch: 4 [5222400/5599780 (93%)]\tLoss: 3.718443\n",
      "Train Epoch: 4 [5227520/5599780 (93%)]\tLoss: 3.724441\n",
      "Train Epoch: 4 [5232640/5599780 (93%)]\tLoss: 3.687181\n",
      "Train Epoch: 4 [5237760/5599780 (94%)]\tLoss: 3.726394\n",
      "Train Epoch: 4 [5242880/5599780 (94%)]\tLoss: 3.652176\n",
      "Train Epoch: 4 [5248000/5599780 (94%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [5253120/5599780 (94%)]\tLoss: 3.716645\n",
      "Train Epoch: 4 [5258240/5599780 (94%)]\tLoss: 3.689285\n",
      "Train Epoch: 4 [5263360/5599780 (94%)]\tLoss: 3.712535\n",
      "Train Epoch: 4 [5268480/5599780 (94%)]\tLoss: 3.712752\n",
      "Train Epoch: 4 [5273600/5599780 (94%)]\tLoss: 3.710769\n",
      "Train Epoch: 4 [5278720/5599780 (94%)]\tLoss: 3.745924\n",
      "Train Epoch: 4 [5283840/5599780 (94%)]\tLoss: 3.712724\n",
      "Train Epoch: 4 [5288960/5599780 (94%)]\tLoss: 3.718572\n",
      "Train Epoch: 4 [5294080/5599780 (95%)]\tLoss: 3.759598\n",
      "Train Epoch: 4 [5299200/5599780 (95%)]\tLoss: 3.701248\n",
      "Train Epoch: 4 [5304320/5599780 (95%)]\tLoss: 3.724441\n",
      "Train Epoch: 4 [5309440/5599780 (95%)]\tLoss: 3.701348\n",
      "Train Epoch: 4 [5314560/5599780 (95%)]\tLoss: 3.699147\n",
      "Train Epoch: 4 [5319680/5599780 (95%)]\tLoss: 3.720259\n",
      "Train Epoch: 4 [5324800/5599780 (95%)]\tLoss: 3.679519\n",
      "Train Epoch: 4 [5329920/5599780 (95%)]\tLoss: 3.667797\n",
      "Train Epoch: 4 [5335040/5599780 (95%)]\tLoss: 3.738117\n",
      "Train Epoch: 4 [5340160/5599780 (95%)]\tLoss: 3.740068\n",
      "Train Epoch: 4 [5345280/5599780 (95%)]\tLoss: 3.751780\n",
      "Train Epoch: 4 [5350400/5599780 (96%)]\tLoss: 3.740066\n",
      "Train Epoch: 4 [5355520/5599780 (96%)]\tLoss: 3.702986\n",
      "Train Epoch: 4 [5360640/5599780 (96%)]\tLoss: 3.722303\n",
      "Train Epoch: 4 [5365760/5599780 (96%)]\tLoss: 3.732254\n",
      "Train Epoch: 4 [5370880/5599780 (96%)]\tLoss: 3.740099\n",
      "Train Epoch: 4 [5376000/5599780 (96%)]\tLoss: 3.724441\n",
      "Train Epoch: 4 [5381120/5599780 (96%)]\tLoss: 3.737727\n",
      "Train Epoch: 4 [5386240/5599780 (96%)]\tLoss: 3.747885\n",
      "Train Epoch: 4 [5391360/5599780 (96%)]\tLoss: 3.714882\n",
      "Train Epoch: 4 [5396480/5599780 (96%)]\tLoss: 3.699051\n",
      "Train Epoch: 4 [5401600/5599780 (96%)]\tLoss: 3.714796\n",
      "Train Epoch: 4 [5406720/5599780 (97%)]\tLoss: 3.697099\n",
      "Train Epoch: 4 [5411840/5599780 (97%)]\tLoss: 3.741953\n",
      "Train Epoch: 4 [5416960/5599780 (97%)]\tLoss: 3.780918\n",
      "Train Epoch: 4 [5422080/5599780 (97%)]\tLoss: 3.679519\n",
      "Train Epoch: 4 [5427200/5599780 (97%)]\tLoss: 3.702957\n",
      "Train Epoch: 4 [5432320/5599780 (97%)]\tLoss: 3.714673\n",
      "Train Epoch: 4 [5437440/5599780 (97%)]\tLoss: 3.722490\n",
      "Train Epoch: 4 [5442560/5599780 (97%)]\tLoss: 3.708816\n",
      "Train Epoch: 4 [5447680/5599780 (97%)]\tLoss: 3.712976\n",
      "Train Epoch: 4 [5452800/5599780 (97%)]\tLoss: 3.702957\n",
      "Train Epoch: 4 [5457920/5599780 (97%)]\tLoss: 3.749889\n",
      "Train Epoch: 4 [5463040/5599780 (98%)]\tLoss: 3.740065\n",
      "Train Epoch: 4 [5468160/5599780 (98%)]\tLoss: 3.740065\n",
      "Train Epoch: 4 [5473280/5599780 (98%)]\tLoss: 3.718580\n",
      "Train Epoch: 4 [5478400/5599780 (98%)]\tLoss: 3.701004\n",
      "Train Epoch: 4 [5483520/5599780 (98%)]\tLoss: 3.720535\n",
      "Train Epoch: 4 [5488640/5599780 (98%)]\tLoss: 3.736160\n",
      "Train Epoch: 4 [5493760/5599780 (98%)]\tLoss: 3.734526\n",
      "Train Epoch: 4 [5498880/5599780 (98%)]\tLoss: 3.730309\n",
      "Train Epoch: 4 [5504000/5599780 (98%)]\tLoss: 3.749831\n",
      "Train Epoch: 4 [5509120/5599780 (98%)]\tLoss: 3.716522\n",
      "Train Epoch: 4 [5514240/5599780 (98%)]\tLoss: 3.702907\n",
      "Train Epoch: 4 [5519360/5599780 (99%)]\tLoss: 3.736621\n",
      "Train Epoch: 4 [5524480/5599780 (99%)]\tLoss: 3.763485\n",
      "Train Epoch: 4 [5529600/5599780 (99%)]\tLoss: 3.724388\n",
      "Train Epoch: 4 [5534720/5599780 (99%)]\tLoss: 3.706863\n",
      "Train Epoch: 4 [5539840/5599780 (99%)]\tLoss: 3.722483\n",
      "Train Epoch: 4 [5544960/5599780 (99%)]\tLoss: 3.693192\n",
      "Train Epoch: 4 [5550080/5599780 (99%)]\tLoss: 3.677566\n",
      "Train Epoch: 4 [5555200/5599780 (99%)]\tLoss: 3.714675\n",
      "Train Epoch: 4 [5560320/5599780 (99%)]\tLoss: 3.703442\n",
      "Train Epoch: 4 [5565440/5599780 (99%)]\tLoss: 3.708817\n",
      "Train Epoch: 4 [5570560/5599780 (99%)]\tLoss: 3.706852\n",
      "Train Epoch: 4 [5575680/5599780 (100%)]\tLoss: 3.693288\n",
      "Train Epoch: 4 [5580800/5599780 (100%)]\tLoss: 3.716629\n",
      "Train Epoch: 4 [5585920/5599780 (100%)]\tLoss: 3.691245\n",
      "Train Epoch: 4 [5591040/5599780 (100%)]\tLoss: 3.689285\n",
      "Train Epoch: 4 [5596160/5599780 (100%)]\tLoss: 3.720535\n",
      "\n",
      "Test set: Average loss: 3.7157, Accuracy: 399029/849497 (47%)\n",
      "\n",
      "Registering model baseline_model\n",
      "Train Epoch: 5 [0/5599780 (0%)]\tLoss: 3.708789\n",
      "Train Epoch: 5 [5120/5599780 (0%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [10240/5599780 (0%)]\tLoss: 3.704935\n",
      "Train Epoch: 5 [15360/5599780 (0%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [20480/5599780 (0%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [25600/5599780 (0%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [30720/5599780 (1%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [35840/5599780 (1%)]\tLoss: 3.708812\n",
      "Train Epoch: 5 [40960/5599780 (1%)]\tLoss: 3.702828\n",
      "Train Epoch: 5 [46080/5599780 (1%)]\tLoss: 3.677491\n",
      "Train Epoch: 5 [51200/5599780 (1%)]\tLoss: 3.710767\n",
      "Train Epoch: 5 [56320/5599780 (1%)]\tLoss: 3.696788\n",
      "Train Epoch: 5 [61440/5599780 (1%)]\tLoss: 3.710993\n",
      "Train Epoch: 5 [66560/5599780 (1%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [71680/5599780 (1%)]\tLoss: 3.718472\n",
      "Train Epoch: 5 [76800/5599780 (1%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [81920/5599780 (1%)]\tLoss: 3.722570\n",
      "Train Epoch: 5 [87040/5599780 (2%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [92160/5599780 (2%)]\tLoss: 3.751805\n",
      "Train Epoch: 5 [97280/5599780 (2%)]\tLoss: 3.759596\n",
      "Train Epoch: 5 [102400/5599780 (2%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [107520/5599780 (2%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [112640/5599780 (2%)]\tLoss: 3.759598\n",
      "Train Epoch: 5 [117760/5599780 (2%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [122880/5599780 (2%)]\tLoss: 3.722491\n",
      "Train Epoch: 5 [128000/5599780 (2%)]\tLoss: 3.730640\n",
      "Train Epoch: 5 [133120/5599780 (2%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [138240/5599780 (2%)]\tLoss: 3.726399\n",
      "Train Epoch: 5 [143360/5599780 (3%)]\tLoss: 3.720480\n",
      "Train Epoch: 5 [148480/5599780 (3%)]\tLoss: 3.697100\n",
      "Train Epoch: 5 [153600/5599780 (3%)]\tLoss: 3.689286\n",
      "Train Epoch: 5 [158720/5599780 (3%)]\tLoss: 3.730090\n",
      "Train Epoch: 5 [163840/5599780 (3%)]\tLoss: 3.716661\n",
      "Train Epoch: 5 [168960/5599780 (3%)]\tLoss: 3.757644\n",
      "Train Epoch: 5 [174080/5599780 (3%)]\tLoss: 3.669777\n",
      "Train Epoch: 5 [179200/5599780 (3%)]\tLoss: 3.753593\n",
      "Train Epoch: 5 [184320/5599780 (3%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [189440/5599780 (3%)]\tLoss: 3.755691\n",
      "Train Epoch: 5 [194560/5599780 (3%)]\tLoss: 3.720534\n",
      "Train Epoch: 5 [199680/5599780 (4%)]\tLoss: 3.732404\n",
      "Train Epoch: 5 [204800/5599780 (4%)]\tLoss: 3.722489\n",
      "Train Epoch: 5 [209920/5599780 (4%)]\tLoss: 3.704880\n",
      "Train Epoch: 5 [215040/5599780 (4%)]\tLoss: 3.669803\n",
      "Train Epoch: 5 [220160/5599780 (4%)]\tLoss: 3.724561\n",
      "Train Epoch: 5 [225280/5599780 (4%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [230400/5599780 (4%)]\tLoss: 3.675632\n",
      "Train Epoch: 5 [235520/5599780 (4%)]\tLoss: 3.675613\n",
      "Train Epoch: 5 [240640/5599780 (4%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [245760/5599780 (4%)]\tLoss: 3.718202\n",
      "Train Epoch: 5 [250880/5599780 (4%)]\tLoss: 3.687168\n",
      "Train Epoch: 5 [256000/5599780 (5%)]\tLoss: 3.720534\n",
      "Train Epoch: 5 [261120/5599780 (5%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [266240/5599780 (5%)]\tLoss: 3.712719\n",
      "Train Epoch: 5 [271360/5599780 (5%)]\tLoss: 3.714707\n",
      "Train Epoch: 5 [276480/5599780 (5%)]\tLoss: 3.683599\n",
      "Train Epoch: 5 [281600/5599780 (5%)]\tLoss: 3.734256\n",
      "Train Epoch: 5 [286720/5599780 (5%)]\tLoss: 3.751785\n",
      "Train Epoch: 5 [291840/5599780 (5%)]\tLoss: 3.751785\n",
      "Train Epoch: 5 [296960/5599780 (5%)]\tLoss: 3.704919\n",
      "Train Epoch: 5 [302080/5599780 (5%)]\tLoss: 3.732486\n",
      "Train Epoch: 5 [307200/5599780 (5%)]\tLoss: 3.702588\n",
      "Train Epoch: 5 [312320/5599780 (6%)]\tLoss: 3.777180\n",
      "Train Epoch: 5 [317440/5599780 (6%)]\tLoss: 3.712704\n",
      "Train Epoch: 5 [322560/5599780 (6%)]\tLoss: 3.702943\n",
      "Train Epoch: 5 [327680/5599780 (6%)]\tLoss: 3.702917\n",
      "Train Epoch: 5 [332800/5599780 (6%)]\tLoss: 3.732489\n",
      "Train Epoch: 5 [337920/5599780 (6%)]\tLoss: 3.722484\n",
      "Train Epoch: 5 [343040/5599780 (6%)]\tLoss: 3.730295\n",
      "Train Epoch: 5 [348160/5599780 (6%)]\tLoss: 3.765455\n",
      "Train Epoch: 5 [353280/5599780 (6%)]\tLoss: 3.691660\n",
      "Train Epoch: 5 [358400/5599780 (6%)]\tLoss: 3.700979\n",
      "Train Epoch: 5 [363520/5599780 (6%)]\tLoss: 3.732255\n",
      "Train Epoch: 5 [368640/5599780 (7%)]\tLoss: 3.712733\n",
      "Train Epoch: 5 [373760/5599780 (7%)]\tLoss: 3.713105\n",
      "Train Epoch: 5 [378880/5599780 (7%)]\tLoss: 3.704913\n",
      "Train Epoch: 5 [384000/5599780 (7%)]\tLoss: 3.720540\n",
      "Train Epoch: 5 [389120/5599780 (7%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [394240/5599780 (7%)]\tLoss: 3.703358\n",
      "Train Epoch: 5 [399360/5599780 (7%)]\tLoss: 3.736203\n",
      "Train Epoch: 5 [404480/5599780 (7%)]\tLoss: 3.714675\n",
      "Train Epoch: 5 [409600/5599780 (7%)]\tLoss: 3.746065\n",
      "Train Epoch: 5 [414720/5599780 (7%)]\tLoss: 3.773276\n",
      "Train Epoch: 5 [419840/5599780 (7%)]\tLoss: 3.730160\n",
      "Train Epoch: 5 [424960/5599780 (8%)]\tLoss: 3.678037\n",
      "Train Epoch: 5 [430080/5599780 (8%)]\tLoss: 3.698820\n",
      "Train Epoch: 5 [435200/5599780 (8%)]\tLoss: 3.738099\n",
      "Train Epoch: 5 [440320/5599780 (8%)]\tLoss: 3.677485\n",
      "Train Epoch: 5 [445440/5599780 (8%)]\tLoss: 3.732253\n",
      "Train Epoch: 5 [450560/5599780 (8%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [455680/5599780 (8%)]\tLoss: 3.708680\n",
      "Train Epoch: 5 [460800/5599780 (8%)]\tLoss: 3.700902\n",
      "Train Epoch: 5 [465920/5599780 (8%)]\tLoss: 3.747847\n",
      "Train Epoch: 5 [471040/5599780 (8%)]\tLoss: 3.728347\n",
      "Train Epoch: 5 [476160/5599780 (9%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [481280/5599780 (9%)]\tLoss: 3.687184\n",
      "Train Epoch: 5 [486400/5599780 (9%)]\tLoss: 3.722869\n",
      "Train Epoch: 5 [491520/5599780 (9%)]\tLoss: 3.720252\n",
      "Train Epoch: 5 [496640/5599780 (9%)]\tLoss: 3.667800\n",
      "Train Epoch: 5 [501760/5599780 (9%)]\tLoss: 3.724418\n",
      "Train Epoch: 5 [506880/5599780 (9%)]\tLoss: 3.714538\n",
      "Train Epoch: 5 [512000/5599780 (9%)]\tLoss: 3.742020\n",
      "Train Epoch: 5 [517120/5599780 (9%)]\tLoss: 3.710791\n",
      "Train Epoch: 5 [522240/5599780 (9%)]\tLoss: 3.743964\n",
      "Train Epoch: 5 [527360/5599780 (9%)]\tLoss: 3.728347\n",
      "Train Epoch: 5 [532480/5599780 (10%)]\tLoss: 3.718841\n",
      "Train Epoch: 5 [537600/5599780 (10%)]\tLoss: 3.720567\n",
      "Train Epoch: 5 [542720/5599780 (10%)]\tLoss: 3.687355\n",
      "Train Epoch: 5 [547840/5599780 (10%)]\tLoss: 3.720438\n",
      "Train Epoch: 5 [552960/5599780 (10%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [558080/5599780 (10%)]\tLoss: 3.697130\n",
      "Train Epoch: 5 [563200/5599780 (10%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [568320/5599780 (10%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [573440/5599780 (10%)]\tLoss: 3.691238\n",
      "Train Epoch: 5 [578560/5599780 (10%)]\tLoss: 3.699050\n",
      "Train Epoch: 5 [583680/5599780 (10%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [588800/5599780 (11%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [593920/5599780 (11%)]\tLoss: 3.693208\n",
      "Train Epoch: 5 [599040/5599780 (11%)]\tLoss: 3.759597\n",
      "Train Epoch: 5 [604160/5599780 (11%)]\tLoss: 3.730500\n",
      "Train Epoch: 5 [609280/5599780 (11%)]\tLoss: 3.724408\n",
      "Train Epoch: 5 [614400/5599780 (11%)]\tLoss: 3.757644\n",
      "Train Epoch: 5 [619520/5599780 (11%)]\tLoss: 3.722416\n",
      "Train Epoch: 5 [624640/5599780 (11%)]\tLoss: 3.722655\n",
      "Train Epoch: 5 [629760/5599780 (11%)]\tLoss: 3.718363\n",
      "Train Epoch: 5 [634880/5599780 (11%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [640000/5599780 (11%)]\tLoss: 3.747913\n",
      "Train Epoch: 5 [645120/5599780 (12%)]\tLoss: 3.724428\n",
      "Train Epoch: 5 [650240/5599780 (12%)]\tLoss: 3.716611\n",
      "Train Epoch: 5 [655360/5599780 (12%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [660480/5599780 (12%)]\tLoss: 3.738112\n",
      "Train Epoch: 5 [665600/5599780 (12%)]\tLoss: 3.671523\n",
      "Train Epoch: 5 [670720/5599780 (12%)]\tLoss: 3.722566\n",
      "Train Epoch: 5 [675840/5599780 (12%)]\tLoss: 3.681474\n",
      "Train Epoch: 5 [680960/5599780 (12%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [686080/5599780 (12%)]\tLoss: 3.695160\n",
      "Train Epoch: 5 [691200/5599780 (12%)]\tLoss: 3.689284\n",
      "Train Epoch: 5 [696320/5599780 (12%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [701440/5599780 (13%)]\tLoss: 3.702967\n",
      "Train Epoch: 5 [706560/5599780 (13%)]\tLoss: 3.716632\n",
      "Train Epoch: 5 [711680/5599780 (13%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [716800/5599780 (13%)]\tLoss: 3.730220\n",
      "Train Epoch: 5 [721920/5599780 (13%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [727040/5599780 (13%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [732160/5599780 (13%)]\tLoss: 3.724436\n",
      "Train Epoch: 5 [737280/5599780 (13%)]\tLoss: 3.707001\n",
      "Train Epoch: 5 [742400/5599780 (13%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [747520/5599780 (13%)]\tLoss: 3.724325\n",
      "Train Epoch: 5 [752640/5599780 (13%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [757760/5599780 (14%)]\tLoss: 3.695306\n",
      "Train Epoch: 5 [762880/5599780 (14%)]\tLoss: 3.693177\n",
      "Train Epoch: 5 [768000/5599780 (14%)]\tLoss: 3.702781\n",
      "Train Epoch: 5 [773120/5599780 (14%)]\tLoss: 3.741969\n",
      "Train Epoch: 5 [778240/5599780 (14%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [783360/5599780 (14%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [788480/5599780 (14%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [793600/5599780 (14%)]\tLoss: 3.704921\n",
      "Train Epoch: 5 [798720/5599780 (14%)]\tLoss: 3.718549\n",
      "Train Epoch: 5 [803840/5599780 (14%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [808960/5599780 (14%)]\tLoss: 3.724683\n",
      "Train Epoch: 5 [814080/5599780 (15%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [819200/5599780 (15%)]\tLoss: 3.728313\n",
      "Train Epoch: 5 [824320/5599780 (15%)]\tLoss: 3.726490\n",
      "Train Epoch: 5 [829440/5599780 (15%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [834560/5599780 (15%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [839680/5599780 (15%)]\tLoss: 3.708887\n",
      "Train Epoch: 5 [844800/5599780 (15%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [849920/5599780 (15%)]\tLoss: 3.708772\n",
      "Train Epoch: 5 [855040/5599780 (15%)]\tLoss: 3.722483\n",
      "Train Epoch: 5 [860160/5599780 (15%)]\tLoss: 3.681472\n",
      "Train Epoch: 5 [865280/5599780 (15%)]\tLoss: 3.736159\n",
      "Train Epoch: 5 [870400/5599780 (16%)]\tLoss: 3.747758\n",
      "Train Epoch: 5 [875520/5599780 (16%)]\tLoss: 3.708805\n",
      "Train Epoch: 5 [880640/5599780 (16%)]\tLoss: 3.712721\n",
      "Train Epoch: 5 [885760/5599780 (16%)]\tLoss: 3.753738\n",
      "Train Epoch: 5 [890880/5599780 (16%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [896000/5599780 (16%)]\tLoss: 3.697243\n",
      "Train Epoch: 5 [901120/5599780 (16%)]\tLoss: 3.675613\n",
      "Train Epoch: 5 [906240/5599780 (16%)]\tLoss: 3.738403\n",
      "Train Epoch: 5 [911360/5599780 (16%)]\tLoss: 3.718718\n",
      "Train Epoch: 5 [916480/5599780 (16%)]\tLoss: 3.671710\n",
      "Train Epoch: 5 [921600/5599780 (16%)]\tLoss: 3.732451\n",
      "Train Epoch: 5 [926720/5599780 (17%)]\tLoss: 3.712721\n",
      "Train Epoch: 5 [931840/5599780 (17%)]\tLoss: 3.736161\n",
      "Train Epoch: 5 [936960/5599780 (17%)]\tLoss: 3.738095\n",
      "Train Epoch: 5 [942080/5599780 (17%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [947200/5599780 (17%)]\tLoss: 3.706864\n",
      "Train Epoch: 5 [952320/5599780 (17%)]\tLoss: 3.722697\n",
      "Train Epoch: 5 [957440/5599780 (17%)]\tLoss: 3.726284\n",
      "Train Epoch: 5 [962560/5599780 (17%)]\tLoss: 3.736106\n",
      "Train Epoch: 5 [967680/5599780 (17%)]\tLoss: 3.737798\n",
      "Train Epoch: 5 [972800/5599780 (17%)]\tLoss: 3.720472\n",
      "Train Epoch: 5 [977920/5599780 (17%)]\tLoss: 3.706966\n",
      "Train Epoch: 5 [983040/5599780 (18%)]\tLoss: 3.703045\n",
      "Train Epoch: 5 [988160/5599780 (18%)]\tLoss: 3.714739\n",
      "Train Epoch: 5 [993280/5599780 (18%)]\tLoss: 3.712697\n",
      "Train Epoch: 5 [998400/5599780 (18%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1003520/5599780 (18%)]\tLoss: 3.730254\n",
      "Train Epoch: 5 [1008640/5599780 (18%)]\tLoss: 3.734206\n",
      "Train Epoch: 5 [1013760/5599780 (18%)]\tLoss: 3.697098\n",
      "Train Epoch: 5 [1018880/5599780 (18%)]\tLoss: 3.722484\n",
      "Train Epoch: 5 [1024000/5599780 (18%)]\tLoss: 3.706898\n",
      "Train Epoch: 5 [1029120/5599780 (18%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [1034240/5599780 (18%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [1039360/5599780 (19%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1044480/5599780 (19%)]\tLoss: 3.697098\n",
      "Train Epoch: 5 [1049600/5599780 (19%)]\tLoss: 3.697098\n",
      "Train Epoch: 5 [1054720/5599780 (19%)]\tLoss: 3.692902\n",
      "Train Epoch: 5 [1059840/5599780 (19%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [1064960/5599780 (19%)]\tLoss: 3.695158\n",
      "Train Epoch: 5 [1070080/5599780 (19%)]\tLoss: 3.724406\n",
      "Train Epoch: 5 [1075200/5599780 (19%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [1080320/5599780 (19%)]\tLoss: 3.726406\n",
      "Train Epoch: 5 [1085440/5599780 (19%)]\tLoss: 3.697670\n",
      "Train Epoch: 5 [1090560/5599780 (19%)]\tLoss: 3.705417\n",
      "Train Epoch: 5 [1095680/5599780 (20%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [1100800/5599780 (20%)]\tLoss: 3.712724\n",
      "Train Epoch: 5 [1105920/5599780 (20%)]\tLoss: 3.699126\n",
      "Train Epoch: 5 [1111040/5599780 (20%)]\tLoss: 3.702815\n",
      "Train Epoch: 5 [1116160/5599780 (20%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [1121280/5599780 (20%)]\tLoss: 3.761551\n",
      "Train Epoch: 5 [1126400/5599780 (20%)]\tLoss: 3.697208\n",
      "Train Epoch: 5 [1131520/5599780 (20%)]\tLoss: 3.714758\n",
      "Train Epoch: 5 [1136640/5599780 (20%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1141760/5599780 (20%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [1146880/5599780 (20%)]\tLoss: 3.724432\n",
      "Train Epoch: 5 [1152000/5599780 (21%)]\tLoss: 3.701013\n",
      "Train Epoch: 5 [1157120/5599780 (21%)]\tLoss: 3.751785\n",
      "Train Epoch: 5 [1162240/5599780 (21%)]\tLoss: 3.669754\n",
      "Train Epoch: 5 [1167360/5599780 (21%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1172480/5599780 (21%)]\tLoss: 3.714342\n",
      "Train Epoch: 5 [1177600/5599780 (21%)]\tLoss: 3.681488\n",
      "Train Epoch: 5 [1182720/5599780 (21%)]\tLoss: 3.697337\n",
      "Train Epoch: 5 [1187840/5599780 (21%)]\tLoss: 3.699122\n",
      "Train Epoch: 5 [1192960/5599780 (21%)]\tLoss: 3.742462\n",
      "Train Epoch: 5 [1198080/5599780 (21%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [1203200/5599780 (21%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [1208320/5599780 (22%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [1213440/5599780 (22%)]\tLoss: 3.740059\n",
      "Train Epoch: 5 [1218560/5599780 (22%)]\tLoss: 3.683436\n",
      "Train Epoch: 5 [1223680/5599780 (22%)]\tLoss: 3.708831\n",
      "Train Epoch: 5 [1228800/5599780 (22%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [1233920/5599780 (22%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [1239040/5599780 (22%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [1244160/5599780 (22%)]\tLoss: 3.675613\n",
      "Train Epoch: 5 [1249280/5599780 (22%)]\tLoss: 3.720794\n",
      "Train Epoch: 5 [1254400/5599780 (22%)]\tLoss: 3.722493\n",
      "Train Epoch: 5 [1259520/5599780 (22%)]\tLoss: 3.771312\n",
      "Train Epoch: 5 [1264640/5599780 (23%)]\tLoss: 3.710771\n",
      "Train Epoch: 5 [1269760/5599780 (23%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [1274880/5599780 (23%)]\tLoss: 3.724437\n",
      "Train Epoch: 5 [1280000/5599780 (23%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [1285120/5599780 (23%)]\tLoss: 3.708815\n",
      "Train Epoch: 5 [1290240/5599780 (23%)]\tLoss: 3.683425\n",
      "Train Epoch: 5 [1295360/5599780 (23%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [1300480/5599780 (23%)]\tLoss: 3.716622\n",
      "Train Epoch: 5 [1305600/5599780 (23%)]\tLoss: 3.730286\n",
      "Train Epoch: 5 [1310720/5599780 (23%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [1315840/5599780 (23%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [1320960/5599780 (24%)]\tLoss: 3.695116\n",
      "Train Epoch: 5 [1326080/5599780 (24%)]\tLoss: 3.745697\n",
      "Train Epoch: 5 [1331200/5599780 (24%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [1336320/5599780 (24%)]\tLoss: 3.720510\n",
      "Train Epoch: 5 [1341440/5599780 (24%)]\tLoss: 3.695314\n",
      "Train Epoch: 5 [1346560/5599780 (24%)]\tLoss: 3.728383\n",
      "Train Epoch: 5 [1351680/5599780 (24%)]\tLoss: 3.691278\n",
      "Train Epoch: 5 [1356800/5599780 (24%)]\tLoss: 3.697261\n",
      "Train Epoch: 5 [1361920/5599780 (24%)]\tLoss: 3.722493\n",
      "Train Epoch: 5 [1367040/5599780 (24%)]\tLoss: 3.708817\n",
      "Train Epoch: 5 [1372160/5599780 (25%)]\tLoss: 3.677624\n",
      "Train Epoch: 5 [1377280/5599780 (25%)]\tLoss: 3.689363\n",
      "Train Epoch: 5 [1382400/5599780 (25%)]\tLoss: 3.724133\n",
      "Train Epoch: 5 [1387520/5599780 (25%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [1392640/5599780 (25%)]\tLoss: 3.675601\n",
      "Train Epoch: 5 [1397760/5599780 (25%)]\tLoss: 3.732177\n",
      "Train Epoch: 5 [1402880/5599780 (25%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [1408000/5599780 (25%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1413120/5599780 (25%)]\tLoss: 3.704909\n",
      "Train Epoch: 5 [1418240/5599780 (25%)]\tLoss: 3.687338\n",
      "Train Epoch: 5 [1423360/5599780 (25%)]\tLoss: 3.720529\n",
      "Train Epoch: 5 [1428480/5599780 (26%)]\tLoss: 3.722493\n",
      "Train Epoch: 5 [1433600/5599780 (26%)]\tLoss: 3.720514\n",
      "Train Epoch: 5 [1438720/5599780 (26%)]\tLoss: 3.728337\n",
      "Train Epoch: 5 [1443840/5599780 (26%)]\tLoss: 3.695207\n",
      "Train Epoch: 5 [1448960/5599780 (26%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [1454080/5599780 (26%)]\tLoss: 3.708804\n",
      "Train Epoch: 5 [1459200/5599780 (26%)]\tLoss: 3.722575\n",
      "Train Epoch: 5 [1464320/5599780 (26%)]\tLoss: 3.753740\n",
      "Train Epoch: 5 [1469440/5599780 (26%)]\tLoss: 3.745906\n",
      "Train Epoch: 5 [1474560/5599780 (26%)]\tLoss: 3.697098\n",
      "Train Epoch: 5 [1479680/5599780 (26%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [1484800/5599780 (27%)]\tLoss: 3.652177\n",
      "Train Epoch: 5 [1489920/5599780 (27%)]\tLoss: 3.726577\n",
      "Train Epoch: 5 [1495040/5599780 (27%)]\tLoss: 3.724443\n",
      "Train Epoch: 5 [1500160/5599780 (27%)]\tLoss: 3.710621\n",
      "Train Epoch: 5 [1505280/5599780 (27%)]\tLoss: 3.702953\n",
      "Train Epoch: 5 [1510400/5599780 (27%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [1515520/5599780 (27%)]\tLoss: 3.724421\n",
      "Train Epoch: 5 [1520640/5599780 (27%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [1525760/5599780 (27%)]\tLoss: 3.689308\n",
      "Train Epoch: 5 [1530880/5599780 (27%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [1536000/5599780 (27%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [1541120/5599780 (28%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [1546240/5599780 (28%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [1551360/5599780 (28%)]\tLoss: 3.693194\n",
      "Train Epoch: 5 [1556480/5599780 (28%)]\tLoss: 3.698944\n",
      "Train Epoch: 5 [1561600/5599780 (28%)]\tLoss: 3.726476\n",
      "Train Epoch: 5 [1566720/5599780 (28%)]\tLoss: 3.702915\n",
      "Train Epoch: 5 [1571840/5599780 (28%)]\tLoss: 3.743882\n",
      "Train Epoch: 5 [1576960/5599780 (28%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [1582080/5599780 (28%)]\tLoss: 3.689322\n",
      "Train Epoch: 5 [1587200/5599780 (28%)]\tLoss: 3.730534\n",
      "Train Epoch: 5 [1592320/5599780 (28%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [1597440/5599780 (29%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [1602560/5599780 (29%)]\tLoss: 3.720742\n",
      "Train Epoch: 5 [1607680/5599780 (29%)]\tLoss: 3.704740\n",
      "Train Epoch: 5 [1612800/5599780 (29%)]\tLoss: 3.704607\n",
      "Train Epoch: 5 [1617920/5599780 (29%)]\tLoss: 3.742150\n",
      "Train Epoch: 5 [1623040/5599780 (29%)]\tLoss: 3.695025\n",
      "Train Epoch: 5 [1628160/5599780 (29%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [1633280/5599780 (29%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [1638400/5599780 (29%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [1643520/5599780 (29%)]\tLoss: 3.718621\n",
      "Train Epoch: 5 [1648640/5599780 (29%)]\tLoss: 3.687333\n",
      "Train Epoch: 5 [1653760/5599780 (30%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [1658880/5599780 (30%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [1664000/5599780 (30%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [1669120/5599780 (30%)]\tLoss: 3.728292\n",
      "Train Epoch: 5 [1674240/5599780 (30%)]\tLoss: 3.673660\n",
      "Train Epoch: 5 [1679360/5599780 (30%)]\tLoss: 3.724448\n",
      "Train Epoch: 5 [1684480/5599780 (30%)]\tLoss: 3.710766\n",
      "Train Epoch: 5 [1689600/5599780 (30%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [1694720/5599780 (30%)]\tLoss: 3.681472\n",
      "Train Epoch: 5 [1699840/5599780 (30%)]\tLoss: 3.693191\n",
      "Train Epoch: 5 [1704960/5599780 (30%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [1710080/5599780 (31%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [1715200/5599780 (31%)]\tLoss: 3.749556\n",
      "Train Epoch: 5 [1720320/5599780 (31%)]\tLoss: 3.757357\n",
      "Train Epoch: 5 [1725440/5599780 (31%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [1730560/5599780 (31%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [1735680/5599780 (31%)]\tLoss: 3.706873\n",
      "Train Epoch: 5 [1740800/5599780 (31%)]\tLoss: 3.718554\n",
      "Train Epoch: 5 [1745920/5599780 (31%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [1751040/5599780 (31%)]\tLoss: 3.724391\n",
      "Train Epoch: 5 [1756160/5599780 (31%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [1761280/5599780 (31%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [1766400/5599780 (32%)]\tLoss: 3.726398\n",
      "Train Epoch: 5 [1771520/5599780 (32%)]\tLoss: 3.722478\n",
      "Train Epoch: 5 [1776640/5599780 (32%)]\tLoss: 3.724454\n",
      "Train Epoch: 5 [1781760/5599780 (32%)]\tLoss: 3.767409\n",
      "Train Epoch: 5 [1786880/5599780 (32%)]\tLoss: 3.718629\n",
      "Train Epoch: 5 [1792000/5599780 (32%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [1797120/5599780 (32%)]\tLoss: 3.712733\n",
      "Train Epoch: 5 [1802240/5599780 (32%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [1807360/5599780 (32%)]\tLoss: 3.714696\n",
      "Train Epoch: 5 [1812480/5599780 (32%)]\tLoss: 3.671707\n",
      "Train Epoch: 5 [1817600/5599780 (32%)]\tLoss: 3.739981\n",
      "Train Epoch: 5 [1822720/5599780 (33%)]\tLoss: 3.732255\n",
      "Train Epoch: 5 [1827840/5599780 (33%)]\tLoss: 3.695150\n",
      "Train Epoch: 5 [1832960/5599780 (33%)]\tLoss: 3.718581\n",
      "Train Epoch: 5 [1838080/5599780 (33%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [1843200/5599780 (33%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [1848320/5599780 (33%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [1853440/5599780 (33%)]\tLoss: 3.743984\n",
      "Train Epoch: 5 [1858560/5599780 (33%)]\tLoss: 3.726395\n",
      "Train Epoch: 5 [1863680/5599780 (33%)]\tLoss: 3.794755\n",
      "Train Epoch: 5 [1868800/5599780 (33%)]\tLoss: 3.734233\n",
      "Train Epoch: 5 [1873920/5599780 (33%)]\tLoss: 3.710910\n",
      "Train Epoch: 5 [1879040/5599780 (34%)]\tLoss: 3.759778\n",
      "Train Epoch: 5 [1884160/5599780 (34%)]\tLoss: 3.695142\n",
      "Train Epoch: 5 [1889280/5599780 (34%)]\tLoss: 3.699051\n",
      "Train Epoch: 5 [1894400/5599780 (34%)]\tLoss: 3.702956\n",
      "Train Epoch: 5 [1899520/5599780 (34%)]\tLoss: 3.730312\n",
      "Train Epoch: 5 [1904640/5599780 (34%)]\tLoss: 3.734322\n",
      "Train Epoch: 5 [1909760/5599780 (34%)]\tLoss: 3.724298\n",
      "Train Epoch: 5 [1914880/5599780 (34%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [1920000/5599780 (34%)]\tLoss: 3.706761\n",
      "Train Epoch: 5 [1925120/5599780 (34%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [1930240/5599780 (34%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [1935360/5599780 (35%)]\tLoss: 3.724472\n",
      "Train Epoch: 5 [1940480/5599780 (35%)]\tLoss: 3.761549\n",
      "Train Epoch: 5 [1945600/5599780 (35%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [1950720/5599780 (35%)]\tLoss: 3.708813\n",
      "Train Epoch: 5 [1955840/5599780 (35%)]\tLoss: 3.689349\n",
      "Train Epoch: 5 [1960960/5599780 (35%)]\tLoss: 3.728522\n",
      "Train Epoch: 5 [1966080/5599780 (35%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [1971200/5599780 (35%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [1976320/5599780 (35%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [1981440/5599780 (35%)]\tLoss: 3.747879\n",
      "Train Epoch: 5 [1986560/5599780 (35%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [1991680/5599780 (36%)]\tLoss: 3.753752\n",
      "Train Epoch: 5 [1996800/5599780 (36%)]\tLoss: 3.728369\n",
      "Train Epoch: 5 [2001920/5599780 (36%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [2007040/5599780 (36%)]\tLoss: 3.712720\n",
      "Train Epoch: 5 [2012160/5599780 (36%)]\tLoss: 3.722487\n",
      "Train Epoch: 5 [2017280/5599780 (36%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2022400/5599780 (36%)]\tLoss: 3.757751\n",
      "Train Epoch: 5 [2027520/5599780 (36%)]\tLoss: 3.732254\n",
      "Train Epoch: 5 [2032640/5599780 (36%)]\tLoss: 3.751785\n",
      "Train Epoch: 5 [2037760/5599780 (36%)]\tLoss: 3.763515\n",
      "Train Epoch: 5 [2042880/5599780 (36%)]\tLoss: 3.698910\n",
      "Train Epoch: 5 [2048000/5599780 (37%)]\tLoss: 3.730258\n",
      "Train Epoch: 5 [2053120/5599780 (37%)]\tLoss: 3.732256\n",
      "Train Epoch: 5 [2058240/5599780 (37%)]\tLoss: 3.752058\n",
      "Train Epoch: 5 [2063360/5599780 (37%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [2068480/5599780 (37%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [2073600/5599780 (37%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [2078720/5599780 (37%)]\tLoss: 3.667810\n",
      "Train Epoch: 5 [2083840/5599780 (37%)]\tLoss: 3.718642\n",
      "Train Epoch: 5 [2088960/5599780 (37%)]\tLoss: 3.695145\n",
      "Train Epoch: 5 [2094080/5599780 (37%)]\tLoss: 3.728349\n",
      "Train Epoch: 5 [2099200/5599780 (37%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [2104320/5599780 (38%)]\tLoss: 3.718754\n",
      "Train Epoch: 5 [2109440/5599780 (38%)]\tLoss: 3.693196\n",
      "Train Epoch: 5 [2114560/5599780 (38%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [2119680/5599780 (38%)]\tLoss: 3.710768\n",
      "Train Epoch: 5 [2124800/5599780 (38%)]\tLoss: 3.732203\n",
      "Train Epoch: 5 [2129920/5599780 (38%)]\tLoss: 3.714494\n",
      "Train Epoch: 5 [2135040/5599780 (38%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2140160/5599780 (38%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [2145280/5599780 (38%)]\tLoss: 3.763505\n",
      "Train Epoch: 5 [2150400/5599780 (38%)]\tLoss: 3.734369\n",
      "Train Epoch: 5 [2155520/5599780 (38%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [2160640/5599780 (39%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [2165760/5599780 (39%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [2170880/5599780 (39%)]\tLoss: 3.689285\n",
      "Train Epoch: 5 [2176000/5599780 (39%)]\tLoss: 3.712721\n",
      "Train Epoch: 5 [2181120/5599780 (39%)]\tLoss: 3.757644\n",
      "Train Epoch: 5 [2186240/5599780 (39%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2191360/5599780 (39%)]\tLoss: 3.771316\n",
      "Train Epoch: 5 [2196480/5599780 (39%)]\tLoss: 3.714760\n",
      "Train Epoch: 5 [2201600/5599780 (39%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [2206720/5599780 (39%)]\tLoss: 3.679520\n",
      "Train Epoch: 5 [2211840/5599780 (39%)]\tLoss: 3.693757\n",
      "Train Epoch: 5 [2216960/5599780 (40%)]\tLoss: 3.749878\n",
      "Train Epoch: 5 [2222080/5599780 (40%)]\tLoss: 3.720818\n",
      "Train Epoch: 5 [2227200/5599780 (40%)]\tLoss: 3.706955\n",
      "Train Epoch: 5 [2232320/5599780 (40%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [2237440/5599780 (40%)]\tLoss: 3.683425\n",
      "Train Epoch: 5 [2242560/5599780 (40%)]\tLoss: 3.736161\n",
      "Train Epoch: 5 [2247680/5599780 (40%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [2252800/5599780 (40%)]\tLoss: 3.708748\n",
      "Train Epoch: 5 [2257920/5599780 (40%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [2263040/5599780 (40%)]\tLoss: 3.722486\n",
      "Train Epoch: 5 [2268160/5599780 (41%)]\tLoss: 3.737755\n",
      "Train Epoch: 5 [2273280/5599780 (41%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [2278400/5599780 (41%)]\tLoss: 3.671596\n",
      "Train Epoch: 5 [2283520/5599780 (41%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [2288640/5599780 (41%)]\tLoss: 3.708643\n",
      "Train Epoch: 5 [2293760/5599780 (41%)]\tLoss: 3.699050\n",
      "Train Epoch: 5 [2298880/5599780 (41%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [2304000/5599780 (41%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [2309120/5599780 (41%)]\tLoss: 3.701080\n",
      "Train Epoch: 5 [2314240/5599780 (41%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [2319360/5599780 (41%)]\tLoss: 3.695142\n",
      "Train Epoch: 5 [2324480/5599780 (42%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [2329600/5599780 (42%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [2334720/5599780 (42%)]\tLoss: 3.714679\n",
      "Train Epoch: 5 [2339840/5599780 (42%)]\tLoss: 3.726402\n",
      "Train Epoch: 5 [2344960/5599780 (42%)]\tLoss: 3.691238\n",
      "Train Epoch: 5 [2350080/5599780 (42%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [2355200/5599780 (42%)]\tLoss: 3.718698\n",
      "Train Epoch: 5 [2360320/5599780 (42%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [2365440/5599780 (42%)]\tLoss: 3.718607\n",
      "Train Epoch: 5 [2370560/5599780 (42%)]\tLoss: 3.773355\n",
      "Train Epoch: 5 [2375680/5599780 (42%)]\tLoss: 3.703152\n",
      "Train Epoch: 5 [2380800/5599780 (43%)]\tLoss: 3.761551\n",
      "Train Epoch: 5 [2385920/5599780 (43%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [2391040/5599780 (43%)]\tLoss: 3.687332\n",
      "Train Epoch: 5 [2396160/5599780 (43%)]\tLoss: 3.726395\n",
      "Train Epoch: 5 [2401280/5599780 (43%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2406400/5599780 (43%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [2411520/5599780 (43%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [2416640/5599780 (43%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [2421760/5599780 (43%)]\tLoss: 3.726333\n",
      "Train Epoch: 5 [2426880/5599780 (43%)]\tLoss: 3.753922\n",
      "Train Epoch: 5 [2432000/5599780 (43%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [2437120/5599780 (44%)]\tLoss: 3.716630\n",
      "Train Epoch: 5 [2442240/5599780 (44%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [2447360/5599780 (44%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [2452480/5599780 (44%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [2457600/5599780 (44%)]\tLoss: 3.730300\n",
      "Train Epoch: 5 [2462720/5599780 (44%)]\tLoss: 3.722617\n",
      "Train Epoch: 5 [2467840/5599780 (44%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [2472960/5599780 (44%)]\tLoss: 3.702950\n",
      "Train Epoch: 5 [2478080/5599780 (44%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [2483200/5599780 (44%)]\tLoss: 3.693225\n",
      "Train Epoch: 5 [2488320/5599780 (44%)]\tLoss: 3.712722\n",
      "Train Epoch: 5 [2493440/5599780 (45%)]\tLoss: 3.726408\n",
      "Train Epoch: 5 [2498560/5599780 (45%)]\tLoss: 3.706881\n",
      "Train Epoch: 5 [2503680/5599780 (45%)]\tLoss: 3.728362\n",
      "Train Epoch: 5 [2508800/5599780 (45%)]\tLoss: 3.677566\n",
      "Train Epoch: 5 [2513920/5599780 (45%)]\tLoss: 3.728278\n",
      "Train Epoch: 5 [2519040/5599780 (45%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [2524160/5599780 (45%)]\tLoss: 3.679519\n",
      "Train Epoch: 5 [2529280/5599780 (45%)]\tLoss: 3.677396\n",
      "Train Epoch: 5 [2534400/5599780 (45%)]\tLoss: 3.679772\n",
      "Train Epoch: 5 [2539520/5599780 (45%)]\tLoss: 3.687326\n",
      "Train Epoch: 5 [2544640/5599780 (45%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [2549760/5599780 (46%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [2554880/5599780 (46%)]\tLoss: 3.679522\n",
      "Train Epoch: 5 [2560000/5599780 (46%)]\tLoss: 3.749849\n",
      "Train Epoch: 5 [2565120/5599780 (46%)]\tLoss: 3.700989\n",
      "Train Epoch: 5 [2570240/5599780 (46%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [2575360/5599780 (46%)]\tLoss: 3.718576\n",
      "Train Epoch: 5 [2580480/5599780 (46%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [2585600/5599780 (46%)]\tLoss: 3.751768\n",
      "Train Epoch: 5 [2590720/5599780 (46%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [2595840/5599780 (46%)]\tLoss: 3.695109\n",
      "Train Epoch: 5 [2600960/5599780 (46%)]\tLoss: 3.722839\n",
      "Train Epoch: 5 [2606080/5599780 (47%)]\tLoss: 3.743971\n",
      "Train Epoch: 5 [2611200/5599780 (47%)]\tLoss: 3.716675\n",
      "Train Epoch: 5 [2616320/5599780 (47%)]\tLoss: 3.728390\n",
      "Train Epoch: 5 [2621440/5599780 (47%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [2626560/5599780 (47%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [2631680/5599780 (47%)]\tLoss: 3.730272\n",
      "Train Epoch: 5 [2636800/5599780 (47%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2641920/5599780 (47%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2647040/5599780 (47%)]\tLoss: 3.701481\n",
      "Train Epoch: 5 [2652160/5599780 (47%)]\tLoss: 3.716627\n",
      "Train Epoch: 5 [2657280/5599780 (47%)]\tLoss: 3.702935\n",
      "Train Epoch: 5 [2662400/5599780 (48%)]\tLoss: 3.667804\n",
      "Train Epoch: 5 [2667520/5599780 (48%)]\tLoss: 3.718576\n",
      "Train Epoch: 5 [2672640/5599780 (48%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [2677760/5599780 (48%)]\tLoss: 3.753641\n",
      "Train Epoch: 5 [2682880/5599780 (48%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [2688000/5599780 (48%)]\tLoss: 3.710749\n",
      "Train Epoch: 5 [2693120/5599780 (48%)]\tLoss: 3.732285\n",
      "Train Epoch: 5 [2698240/5599780 (48%)]\tLoss: 3.750494\n",
      "Train Epoch: 5 [2703360/5599780 (48%)]\tLoss: 3.699052\n",
      "Train Epoch: 5 [2708480/5599780 (48%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [2713600/5599780 (48%)]\tLoss: 3.720533\n",
      "Train Epoch: 5 [2718720/5599780 (49%)]\tLoss: 3.699045\n",
      "Train Epoch: 5 [2723840/5599780 (49%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [2728960/5599780 (49%)]\tLoss: 3.669753\n",
      "Train Epoch: 5 [2734080/5599780 (49%)]\tLoss: 3.700994\n",
      "Train Epoch: 5 [2739200/5599780 (49%)]\tLoss: 3.732296\n",
      "Train Epoch: 5 [2744320/5599780 (49%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [2749440/5599780 (49%)]\tLoss: 3.679519\n",
      "Train Epoch: 5 [2754560/5599780 (49%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [2759680/5599780 (49%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [2764800/5599780 (49%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [2769920/5599780 (49%)]\tLoss: 3.710772\n",
      "Train Epoch: 5 [2775040/5599780 (50%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [2780160/5599780 (50%)]\tLoss: 3.679523\n",
      "Train Epoch: 5 [2785280/5599780 (50%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [2790400/5599780 (50%)]\tLoss: 3.718585\n",
      "Train Epoch: 5 [2795520/5599780 (50%)]\tLoss: 3.710755\n",
      "Train Epoch: 5 [2800640/5599780 (50%)]\tLoss: 3.671707\n",
      "Train Epoch: 5 [2805760/5599780 (50%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [2810880/5599780 (50%)]\tLoss: 3.693178\n",
      "Train Epoch: 5 [2816000/5599780 (50%)]\tLoss: 3.710768\n",
      "Train Epoch: 5 [2821120/5599780 (50%)]\tLoss: 3.736176\n",
      "Train Epoch: 5 [2826240/5599780 (50%)]\tLoss: 3.757644\n",
      "Train Epoch: 5 [2831360/5599780 (51%)]\tLoss: 3.732160\n",
      "Train Epoch: 5 [2836480/5599780 (51%)]\tLoss: 3.728190\n",
      "Train Epoch: 5 [2841600/5599780 (51%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [2846720/5599780 (51%)]\tLoss: 3.691238\n",
      "Train Epoch: 5 [2851840/5599780 (51%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [2856960/5599780 (51%)]\tLoss: 3.724444\n",
      "Train Epoch: 5 [2862080/5599780 (51%)]\tLoss: 3.743882\n",
      "Train Epoch: 5 [2867200/5599780 (51%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [2872320/5599780 (51%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [2877440/5599780 (51%)]\tLoss: 3.707064\n",
      "Train Epoch: 5 [2882560/5599780 (51%)]\tLoss: 3.691153\n",
      "Train Epoch: 5 [2887680/5599780 (52%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [2892800/5599780 (52%)]\tLoss: 3.687253\n",
      "Train Epoch: 5 [2897920/5599780 (52%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [2903040/5599780 (52%)]\tLoss: 3.685379\n",
      "Train Epoch: 5 [2908160/5599780 (52%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [2913280/5599780 (52%)]\tLoss: 3.716646\n",
      "Train Epoch: 5 [2918400/5599780 (52%)]\tLoss: 3.661941\n",
      "Train Epoch: 5 [2923520/5599780 (52%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [2928640/5599780 (52%)]\tLoss: 3.697212\n",
      "Train Epoch: 5 [2933760/5599780 (52%)]\tLoss: 3.699051\n",
      "Train Epoch: 5 [2938880/5599780 (52%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [2944000/5599780 (53%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [2949120/5599780 (53%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [2954240/5599780 (53%)]\tLoss: 3.679622\n",
      "Train Epoch: 5 [2959360/5599780 (53%)]\tLoss: 3.724451\n",
      "Train Epoch: 5 [2964480/5599780 (53%)]\tLoss: 3.712722\n",
      "Train Epoch: 5 [2969600/5599780 (53%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [2974720/5599780 (53%)]\tLoss: 3.773488\n",
      "Train Epoch: 5 [2979840/5599780 (53%)]\tLoss: 3.722796\n",
      "Train Epoch: 5 [2984960/5599780 (53%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [2990080/5599780 (53%)]\tLoss: 3.732250\n",
      "Train Epoch: 5 [2995200/5599780 (53%)]\tLoss: 3.728368\n",
      "Train Epoch: 5 [3000320/5599780 (54%)]\tLoss: 3.747961\n",
      "Train Epoch: 5 [3005440/5599780 (54%)]\tLoss: 3.716554\n",
      "Train Epoch: 5 [3010560/5599780 (54%)]\tLoss: 3.722219\n",
      "Train Epoch: 5 [3015680/5599780 (54%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [3020800/5599780 (54%)]\tLoss: 3.747879\n",
      "Train Epoch: 5 [3025920/5599780 (54%)]\tLoss: 3.704848\n",
      "Train Epoch: 5 [3031040/5599780 (54%)]\tLoss: 3.693191\n",
      "Train Epoch: 5 [3036160/5599780 (54%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [3041280/5599780 (54%)]\tLoss: 3.743818\n",
      "Train Epoch: 5 [3046400/5599780 (54%)]\tLoss: 3.673659\n",
      "Train Epoch: 5 [3051520/5599780 (54%)]\tLoss: 3.765487\n",
      "Train Epoch: 5 [3056640/5599780 (55%)]\tLoss: 3.730122\n",
      "Train Epoch: 5 [3061760/5599780 (55%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [3066880/5599780 (55%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [3072000/5599780 (55%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [3077120/5599780 (55%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [3082240/5599780 (55%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [3087360/5599780 (55%)]\tLoss: 3.675613\n",
      "Train Epoch: 5 [3092480/5599780 (55%)]\tLoss: 3.706877\n",
      "Train Epoch: 5 [3097600/5599780 (55%)]\tLoss: 3.699050\n",
      "Train Epoch: 5 [3102720/5599780 (55%)]\tLoss: 3.730299\n",
      "Train Epoch: 5 [3107840/5599780 (55%)]\tLoss: 3.716632\n",
      "Train Epoch: 5 [3112960/5599780 (56%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [3118080/5599780 (56%)]\tLoss: 3.734206\n",
      "Train Epoch: 5 [3123200/5599780 (56%)]\tLoss: 3.695085\n",
      "Train Epoch: 5 [3128320/5599780 (56%)]\tLoss: 3.699051\n",
      "Train Epoch: 5 [3133440/5599780 (56%)]\tLoss: 3.695145\n",
      "Train Epoch: 5 [3138560/5599780 (56%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [3143680/5599780 (56%)]\tLoss: 3.745926\n",
      "Train Epoch: 5 [3148800/5599780 (56%)]\tLoss: 3.716635\n",
      "Train Epoch: 5 [3153920/5599780 (56%)]\tLoss: 3.718647\n",
      "Train Epoch: 5 [3159040/5599780 (56%)]\tLoss: 3.732254\n",
      "Train Epoch: 5 [3164160/5599780 (57%)]\tLoss: 3.693191\n",
      "Train Epoch: 5 [3169280/5599780 (57%)]\tLoss: 3.742018\n",
      "Train Epoch: 5 [3174400/5599780 (57%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [3179520/5599780 (57%)]\tLoss: 3.780888\n",
      "Train Epoch: 5 [3184640/5599780 (57%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [3189760/5599780 (57%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [3194880/5599780 (57%)]\tLoss: 3.712742\n",
      "Train Epoch: 5 [3200000/5599780 (57%)]\tLoss: 3.702956\n",
      "Train Epoch: 5 [3205120/5599780 (57%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [3210240/5599780 (57%)]\tLoss: 3.740067\n",
      "Train Epoch: 5 [3215360/5599780 (57%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [3220480/5599780 (58%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [3225600/5599780 (58%)]\tLoss: 3.751785\n",
      "Train Epoch: 5 [3230720/5599780 (58%)]\tLoss: 3.689254\n",
      "Train Epoch: 5 [3235840/5599780 (58%)]\tLoss: 3.679519\n",
      "Train Epoch: 5 [3240960/5599780 (58%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [3246080/5599780 (58%)]\tLoss: 3.726394\n",
      "Train Epoch: 5 [3251200/5599780 (58%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [3256320/5599780 (58%)]\tLoss: 3.710805\n",
      "Train Epoch: 5 [3261440/5599780 (58%)]\tLoss: 3.708811\n",
      "Train Epoch: 5 [3266560/5599780 (58%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [3271680/5599780 (58%)]\tLoss: 3.673666\n",
      "Train Epoch: 5 [3276800/5599780 (59%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [3281920/5599780 (59%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [3287040/5599780 (59%)]\tLoss: 3.761551\n",
      "Train Epoch: 5 [3292160/5599780 (59%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [3297280/5599780 (59%)]\tLoss: 3.710928\n",
      "Train Epoch: 5 [3302400/5599780 (59%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [3307520/5599780 (59%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [3312640/5599780 (59%)]\tLoss: 3.701003\n",
      "Train Epoch: 5 [3317760/5599780 (59%)]\tLoss: 3.701111\n",
      "Train Epoch: 5 [3322880/5599780 (59%)]\tLoss: 3.693193\n",
      "Train Epoch: 5 [3328000/5599780 (59%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [3333120/5599780 (60%)]\tLoss: 3.706676\n",
      "Train Epoch: 5 [3338240/5599780 (60%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [3343360/5599780 (60%)]\tLoss: 3.697390\n",
      "Train Epoch: 5 [3348480/5599780 (60%)]\tLoss: 3.734209\n",
      "Train Epoch: 5 [3353600/5599780 (60%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [3358720/5599780 (60%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [3363840/5599780 (60%)]\tLoss: 3.745962\n",
      "Train Epoch: 5 [3368960/5599780 (60%)]\tLoss: 3.699051\n",
      "Train Epoch: 5 [3374080/5599780 (60%)]\tLoss: 3.716682\n",
      "Train Epoch: 5 [3379200/5599780 (60%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [3384320/5599780 (60%)]\tLoss: 3.765457\n",
      "Train Epoch: 5 [3389440/5599780 (61%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [3394560/5599780 (61%)]\tLoss: 3.718529\n",
      "Train Epoch: 5 [3399680/5599780 (61%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [3404800/5599780 (61%)]\tLoss: 3.747879\n",
      "Train Epoch: 5 [3409920/5599780 (61%)]\tLoss: 3.687332\n",
      "Train Epoch: 5 [3415040/5599780 (61%)]\tLoss: 3.722447\n",
      "Train Epoch: 5 [3420160/5599780 (61%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [3425280/5599780 (61%)]\tLoss: 3.730300\n",
      "Train Epoch: 5 [3430400/5599780 (61%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [3435520/5599780 (61%)]\tLoss: 3.697097\n",
      "Train Epoch: 5 [3440640/5599780 (61%)]\tLoss: 3.710768\n",
      "Train Epoch: 5 [3445760/5599780 (62%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [3450880/5599780 (62%)]\tLoss: 3.726406\n",
      "Train Epoch: 5 [3456000/5599780 (62%)]\tLoss: 3.658037\n",
      "Train Epoch: 5 [3461120/5599780 (62%)]\tLoss: 3.712721\n",
      "Train Epoch: 5 [3466240/5599780 (62%)]\tLoss: 3.728351\n",
      "Train Epoch: 5 [3471360/5599780 (62%)]\tLoss: 3.712730\n",
      "Train Epoch: 5 [3476480/5599780 (62%)]\tLoss: 3.714675\n",
      "Train Epoch: 5 [3481600/5599780 (62%)]\tLoss: 3.712722\n",
      "Train Epoch: 5 [3486720/5599780 (62%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [3491840/5599780 (62%)]\tLoss: 3.716470\n",
      "Train Epoch: 5 [3496960/5599780 (62%)]\tLoss: 3.710975\n",
      "Train Epoch: 5 [3502080/5599780 (63%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [3507200/5599780 (63%)]\tLoss: 3.695156\n",
      "Train Epoch: 5 [3512320/5599780 (63%)]\tLoss: 3.744043\n",
      "Train Epoch: 5 [3517440/5599780 (63%)]\tLoss: 3.747879\n",
      "Train Epoch: 5 [3522560/5599780 (63%)]\tLoss: 3.665847\n",
      "Train Epoch: 5 [3527680/5599780 (63%)]\tLoss: 3.730345\n",
      "Train Epoch: 5 [3532800/5599780 (63%)]\tLoss: 3.648269\n",
      "Train Epoch: 5 [3537920/5599780 (63%)]\tLoss: 3.681363\n",
      "Train Epoch: 5 [3543040/5599780 (63%)]\tLoss: 3.681472\n",
      "Train Epoch: 5 [3548160/5599780 (63%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [3553280/5599780 (63%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [3558400/5599780 (64%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [3563520/5599780 (64%)]\tLoss: 3.738385\n",
      "Train Epoch: 5 [3568640/5599780 (64%)]\tLoss: 3.699051\n",
      "Train Epoch: 5 [3573760/5599780 (64%)]\tLoss: 3.714699\n",
      "Train Epoch: 5 [3578880/5599780 (64%)]\tLoss: 3.718569\n",
      "Train Epoch: 5 [3584000/5599780 (64%)]\tLoss: 3.720579\n",
      "Train Epoch: 5 [3589120/5599780 (64%)]\tLoss: 3.765457\n",
      "Train Epoch: 5 [3594240/5599780 (64%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [3599360/5599780 (64%)]\tLoss: 3.732464\n",
      "Train Epoch: 5 [3604480/5599780 (64%)]\tLoss: 3.732254\n",
      "Train Epoch: 5 [3609600/5599780 (64%)]\tLoss: 3.691125\n",
      "Train Epoch: 5 [3614720/5599780 (65%)]\tLoss: 3.695157\n",
      "Train Epoch: 5 [3619840/5599780 (65%)]\tLoss: 3.689362\n",
      "Train Epoch: 5 [3624960/5599780 (65%)]\tLoss: 3.708802\n",
      "Train Epoch: 5 [3630080/5599780 (65%)]\tLoss: 3.722571\n",
      "Train Epoch: 5 [3635200/5599780 (65%)]\tLoss: 3.718590\n",
      "Train Epoch: 5 [3640320/5599780 (65%)]\tLoss: 3.724453\n",
      "Train Epoch: 5 [3645440/5599780 (65%)]\tLoss: 3.699120\n",
      "Train Epoch: 5 [3650560/5599780 (65%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [3655680/5599780 (65%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [3660800/5599780 (65%)]\tLoss: 3.722476\n",
      "Train Epoch: 5 [3665920/5599780 (65%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [3671040/5599780 (66%)]\tLoss: 3.673660\n",
      "Train Epoch: 5 [3676160/5599780 (66%)]\tLoss: 3.720535\n",
      "Train Epoch: 5 [3681280/5599780 (66%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [3686400/5599780 (66%)]\tLoss: 3.702963\n",
      "Train Epoch: 5 [3691520/5599780 (66%)]\tLoss: 3.710816\n",
      "Train Epoch: 5 [3696640/5599780 (66%)]\tLoss: 3.728349\n",
      "Train Epoch: 5 [3701760/5599780 (66%)]\tLoss: 3.698914\n",
      "Train Epoch: 5 [3706880/5599780 (66%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [3712000/5599780 (66%)]\tLoss: 3.745928\n",
      "Train Epoch: 5 [3717120/5599780 (66%)]\tLoss: 3.693183\n",
      "Train Epoch: 5 [3722240/5599780 (66%)]\tLoss: 3.699050\n",
      "Train Epoch: 5 [3727360/5599780 (67%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [3732480/5599780 (67%)]\tLoss: 3.677566\n",
      "Train Epoch: 5 [3737600/5599780 (67%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [3742720/5599780 (67%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [3747840/5599780 (67%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [3752960/5599780 (67%)]\tLoss: 3.763679\n",
      "Train Epoch: 5 [3758080/5599780 (67%)]\tLoss: 3.722627\n",
      "Train Epoch: 5 [3763200/5599780 (67%)]\tLoss: 3.742019\n",
      "Train Epoch: 5 [3768320/5599780 (67%)]\tLoss: 3.703025\n",
      "Train Epoch: 5 [3773440/5599780 (67%)]\tLoss: 3.734318\n",
      "Train Epoch: 5 [3778560/5599780 (67%)]\tLoss: 3.722484\n",
      "Train Epoch: 5 [3783680/5599780 (68%)]\tLoss: 3.722302\n",
      "Train Epoch: 5 [3788800/5599780 (68%)]\tLoss: 3.771316\n",
      "Train Epoch: 5 [3793920/5599780 (68%)]\tLoss: 3.697097\n",
      "Train Epoch: 5 [3799040/5599780 (68%)]\tLoss: 3.781310\n",
      "Train Epoch: 5 [3804160/5599780 (68%)]\tLoss: 3.714687\n",
      "Train Epoch: 5 [3809280/5599780 (68%)]\tLoss: 3.679519\n",
      "Train Epoch: 5 [3814400/5599780 (68%)]\tLoss: 3.695145\n",
      "Train Epoch: 5 [3819520/5599780 (68%)]\tLoss: 3.734219\n",
      "Train Epoch: 5 [3824640/5599780 (68%)]\tLoss: 3.691239\n",
      "Train Epoch: 5 [3829760/5599780 (68%)]\tLoss: 3.714676\n",
      "Train Epoch: 5 [3834880/5599780 (68%)]\tLoss: 3.730300\n",
      "Train Epoch: 5 [3840000/5599780 (69%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [3845120/5599780 (69%)]\tLoss: 3.742018\n",
      "Train Epoch: 5 [3850240/5599780 (69%)]\tLoss: 3.714675\n",
      "Train Epoch: 5 [3855360/5599780 (69%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [3860480/5599780 (69%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [3865600/5599780 (69%)]\tLoss: 3.722491\n",
      "Train Epoch: 5 [3870720/5599780 (69%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [3875840/5599780 (69%)]\tLoss: 3.716630\n",
      "Train Epoch: 5 [3880960/5599780 (69%)]\tLoss: 3.745926\n",
      "Train Epoch: 5 [3886080/5599780 (69%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [3891200/5599780 (69%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [3896320/5599780 (70%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [3901440/5599780 (70%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [3906560/5599780 (70%)]\tLoss: 3.739911\n",
      "Train Epoch: 5 [3911680/5599780 (70%)]\tLoss: 3.720853\n",
      "Train Epoch: 5 [3916800/5599780 (70%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [3921920/5599780 (70%)]\tLoss: 3.712759\n",
      "Train Epoch: 5 [3927040/5599780 (70%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [3932160/5599780 (70%)]\tLoss: 3.701003\n",
      "Train Epoch: 5 [3937280/5599780 (70%)]\tLoss: 3.687332\n",
      "Train Epoch: 5 [3942400/5599780 (70%)]\tLoss: 3.722491\n",
      "Train Epoch: 5 [3947520/5599780 (70%)]\tLoss: 3.702958\n",
      "Train Epoch: 5 [3952640/5599780 (71%)]\tLoss: 3.720533\n",
      "Train Epoch: 5 [3957760/5599780 (71%)]\tLoss: 3.743892\n",
      "Train Epoch: 5 [3962880/5599780 (71%)]\tLoss: 3.677566\n",
      "Train Epoch: 5 [3968000/5599780 (71%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [3973120/5599780 (71%)]\tLoss: 3.777360\n",
      "Train Epoch: 5 [3978240/5599780 (71%)]\tLoss: 3.724647\n",
      "Train Epoch: 5 [3983360/5599780 (71%)]\tLoss: 3.745908\n",
      "Train Epoch: 5 [3988480/5599780 (71%)]\tLoss: 3.753746\n",
      "Train Epoch: 5 [3993600/5599780 (71%)]\tLoss: 3.701166\n",
      "Train Epoch: 5 [3998720/5599780 (71%)]\tLoss: 3.702982\n",
      "Train Epoch: 5 [4003840/5599780 (71%)]\tLoss: 3.706872\n",
      "Train Epoch: 5 [4008960/5599780 (72%)]\tLoss: 3.745937\n",
      "Train Epoch: 5 [4014080/5599780 (72%)]\tLoss: 3.695131\n",
      "Train Epoch: 5 [4019200/5599780 (72%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [4024320/5599780 (72%)]\tLoss: 3.736244\n",
      "Train Epoch: 5 [4029440/5599780 (72%)]\tLoss: 3.704923\n",
      "Train Epoch: 5 [4034560/5599780 (72%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [4039680/5599780 (72%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [4044800/5599780 (72%)]\tLoss: 3.712716\n",
      "Train Epoch: 5 [4049920/5599780 (72%)]\tLoss: 3.693183\n",
      "Train Epoch: 5 [4055040/5599780 (72%)]\tLoss: 3.683435\n",
      "Train Epoch: 5 [4060160/5599780 (72%)]\tLoss: 3.722463\n",
      "Train Epoch: 5 [4065280/5599780 (73%)]\tLoss: 3.716630\n",
      "Train Epoch: 5 [4070400/5599780 (73%)]\tLoss: 3.716628\n",
      "Train Epoch: 5 [4075520/5599780 (73%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [4080640/5599780 (73%)]\tLoss: 3.706933\n",
      "Train Epoch: 5 [4085760/5599780 (73%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [4090880/5599780 (73%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [4096000/5599780 (73%)]\tLoss: 3.701057\n",
      "Train Epoch: 5 [4101120/5599780 (73%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [4106240/5599780 (73%)]\tLoss: 3.700913\n",
      "Train Epoch: 5 [4111360/5599780 (73%)]\tLoss: 3.693192\n",
      "Train Epoch: 5 [4116480/5599780 (74%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [4121600/5599780 (74%)]\tLoss: 3.681460\n",
      "Train Epoch: 5 [4126720/5599780 (74%)]\tLoss: 3.759588\n",
      "Train Epoch: 5 [4131840/5599780 (74%)]\tLoss: 3.700962\n",
      "Train Epoch: 5 [4136960/5599780 (74%)]\tLoss: 3.732261\n",
      "Train Epoch: 5 [4142080/5599780 (74%)]\tLoss: 3.773370\n",
      "Train Epoch: 5 [4147200/5599780 (74%)]\tLoss: 3.753737\n",
      "Train Epoch: 5 [4152320/5599780 (74%)]\tLoss: 3.691247\n",
      "Train Epoch: 5 [4157440/5599780 (74%)]\tLoss: 3.730160\n",
      "Train Epoch: 5 [4162560/5599780 (74%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [4167680/5599780 (74%)]\tLoss: 3.730300\n",
      "Train Epoch: 5 [4172800/5599780 (75%)]\tLoss: 3.722488\n",
      "Train Epoch: 5 [4177920/5599780 (75%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [4183040/5599780 (75%)]\tLoss: 3.695148\n",
      "Train Epoch: 5 [4188160/5599780 (75%)]\tLoss: 3.665847\n",
      "Train Epoch: 5 [4193280/5599780 (75%)]\tLoss: 3.687332\n",
      "Train Epoch: 5 [4198400/5599780 (75%)]\tLoss: 3.734130\n",
      "Train Epoch: 5 [4203520/5599780 (75%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [4208640/5599780 (75%)]\tLoss: 3.695182\n",
      "Train Epoch: 5 [4213760/5599780 (75%)]\tLoss: 3.679519\n",
      "Train Epoch: 5 [4218880/5599780 (75%)]\tLoss: 3.712795\n",
      "Train Epoch: 5 [4224000/5599780 (75%)]\tLoss: 3.712723\n",
      "Train Epoch: 5 [4229120/5599780 (76%)]\tLoss: 3.708822\n",
      "Train Epoch: 5 [4234240/5599780 (76%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [4239360/5599780 (76%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [4244480/5599780 (76%)]\tLoss: 3.736162\n",
      "Train Epoch: 5 [4249600/5599780 (76%)]\tLoss: 3.726415\n",
      "Train Epoch: 5 [4254720/5599780 (76%)]\tLoss: 3.724295\n",
      "Train Epoch: 5 [4259840/5599780 (76%)]\tLoss: 3.710765\n",
      "Train Epoch: 5 [4264960/5599780 (76%)]\tLoss: 3.712797\n",
      "Train Epoch: 5 [4270080/5599780 (76%)]\tLoss: 3.718848\n",
      "Train Epoch: 5 [4275200/5599780 (76%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [4280320/5599780 (76%)]\tLoss: 3.718582\n",
      "Train Epoch: 5 [4285440/5599780 (77%)]\tLoss: 3.708813\n",
      "Train Epoch: 5 [4290560/5599780 (77%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [4295680/5599780 (77%)]\tLoss: 3.761551\n",
      "Train Epoch: 5 [4300800/5599780 (77%)]\tLoss: 3.699050\n",
      "Train Epoch: 5 [4305920/5599780 (77%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [4311040/5599780 (77%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [4316160/5599780 (77%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [4321280/5599780 (77%)]\tLoss: 3.757644\n",
      "Train Epoch: 5 [4326400/5599780 (77%)]\tLoss: 3.685436\n",
      "Train Epoch: 5 [4331520/5599780 (77%)]\tLoss: 3.687367\n",
      "Train Epoch: 5 [4336640/5599780 (77%)]\tLoss: 3.740066\n",
      "Train Epoch: 5 [4341760/5599780 (78%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [4346880/5599780 (78%)]\tLoss: 3.726393\n",
      "Train Epoch: 5 [4352000/5599780 (78%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [4357120/5599780 (78%)]\tLoss: 3.687560\n",
      "Train Epoch: 5 [4362240/5599780 (78%)]\tLoss: 3.695144\n",
      "Train Epoch: 5 [4367360/5599780 (78%)]\tLoss: 3.734207\n",
      "Train Epoch: 5 [4372480/5599780 (78%)]\tLoss: 3.691239\n",
      "Train Epoch: 5 [4377600/5599780 (78%)]\tLoss: 3.712766\n",
      "Train Epoch: 5 [4382720/5599780 (78%)]\tLoss: 3.701002\n",
      "Train Epoch: 5 [4387840/5599780 (78%)]\tLoss: 3.716629\n",
      "Train Epoch: 5 [4392960/5599780 (78%)]\tLoss: 3.730296\n",
      "Train Epoch: 5 [4398080/5599780 (79%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [4403200/5599780 (79%)]\tLoss: 3.693191\n",
      "Train Epoch: 5 [4408320/5599780 (79%)]\tLoss: 3.722482\n",
      "Train Epoch: 5 [4413440/5599780 (79%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [4418560/5599780 (79%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [4423680/5599780 (79%)]\tLoss: 3.705039\n",
      "Train Epoch: 5 [4428800/5599780 (79%)]\tLoss: 3.773269\n",
      "Train Epoch: 5 [4433920/5599780 (79%)]\tLoss: 3.706863\n",
      "Train Epoch: 5 [4439040/5599780 (79%)]\tLoss: 3.730301\n",
      "Train Epoch: 5 [4444160/5599780 (79%)]\tLoss: 3.734178\n",
      "Train Epoch: 5 [4449280/5599780 (79%)]\tLoss: 3.706843\n",
      "Train Epoch: 5 [4454400/5599780 (80%)]\tLoss: 3.704894\n",
      "Train Epoch: 5 [4459520/5599780 (80%)]\tLoss: 3.718580\n",
      "Train Epoch: 5 [4464640/5599780 (80%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [4469760/5599780 (80%)]\tLoss: 3.706825\n",
      "Train Epoch: 5 [4474880/5599780 (80%)]\tLoss: 3.775222\n",
      "Train Epoch: 5 [4480000/5599780 (80%)]\tLoss: 3.720474\n",
      "Train Epoch: 5 [4485120/5599780 (80%)]\tLoss: 3.710614\n",
      "Train Epoch: 5 [4490240/5599780 (80%)]\tLoss: 3.683425\n",
      "Train Epoch: 5 [4495360/5599780 (80%)]\tLoss: 3.683426\n",
      "Train Epoch: 5 [4500480/5599780 (80%)]\tLoss: 3.702956\n",
      "Train Epoch: 5 [4505600/5599780 (80%)]\tLoss: 3.671713\n",
      "Train Epoch: 5 [4510720/5599780 (81%)]\tLoss: 3.763601\n",
      "Train Epoch: 5 [4515840/5599780 (81%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [4520960/5599780 (81%)]\tLoss: 3.701004\n",
      "Train Epoch: 5 [4526080/5599780 (81%)]\tLoss: 3.730284\n",
      "Train Epoch: 5 [4531200/5599780 (81%)]\tLoss: 3.736202\n",
      "Train Epoch: 5 [4536320/5599780 (81%)]\tLoss: 3.704910\n",
      "Train Epoch: 5 [4541440/5599780 (81%)]\tLoss: 3.736160\n",
      "Train Epoch: 5 [4546560/5599780 (81%)]\tLoss: 3.710769\n",
      "Train Epoch: 5 [4551680/5599780 (81%)]\tLoss: 3.761626\n",
      "Train Epoch: 5 [4556800/5599780 (81%)]\tLoss: 3.708816\n",
      "Train Epoch: 5 [4561920/5599780 (81%)]\tLoss: 3.743973\n",
      "Train Epoch: 5 [4567040/5599780 (82%)]\tLoss: 3.704873\n",
      "Train Epoch: 5 [4572160/5599780 (82%)]\tLoss: 3.702957\n",
      "Train Epoch: 5 [4577280/5599780 (82%)]\tLoss: 3.724441\n",
      "Train Epoch: 5 [4582400/5599780 (82%)]\tLoss: 3.728348\n",
      "Train Epoch: 5 [4587520/5599780 (82%)]\tLoss: 3.699066\n",
      "Train Epoch: 5 [4592640/5599780 (82%)]\tLoss: 3.673659\n",
      "Train Epoch: 5 [4597760/5599780 (82%)]\tLoss: 3.726471\n",
      "Train Epoch: 5 [4602880/5599780 (82%)]\tLoss: 3.738113\n",
      "Train Epoch: 5 [4608000/5599780 (82%)]\tLoss: 3.677564\n",
      "Train Epoch: 5 [4613120/5599780 (82%)]\tLoss: 3.718557\n"
     ]
    },
    {
     "ename": "ExperimentExecutionException",
     "evalue": "ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\github.com\\hellovertex\\Thesis\\venv\\lib\\site-packages\\azureml\\core\\run.py:841\u001B[0m, in \u001B[0;36mRun.wait_for_completion\u001B[1;34m(self, show_output, wait_post_processing, raise_on_error)\u001B[0m\n\u001B[0;32m    840\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 841\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stream_run_output\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfile_handle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwait_post_processing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwait_post_processing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraise_on_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraise_on_error\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    845\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_details()\n",
      "File \u001B[1;32m~\\Documents\\github.com\\hellovertex\\Thesis\\venv\\lib\\site-packages\\azureml\\core\\run.py:1032\u001B[0m, in \u001B[0;36mRun._stream_run_output\u001B[1;34m(self, file_handle, wait_post_processing, raise_on_error)\u001B[0m\n\u001B[0;32m   1031\u001B[0m file_handle\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m-> 1032\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRun\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_before_polling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtime\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpoll_start_time\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_details \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_details()  \u001B[38;5;66;03m# TODO use FileWatcher\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mExperimentExecutionException\u001B[0m              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m run \u001B[38;5;241m=\u001B[39m exp\u001B[38;5;241m.\u001B[39msubmit(config)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(run)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mrun\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_for_completion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshow_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\github.com\\hellovertex\\Thesis\\venv\\lib\\site-packages\\azureml\\core\\run.py:852\u001B[0m, in \u001B[0;36mRun.wait_for_completion\u001B[1;34m(self, show_output, wait_post_processing, raise_on_error)\u001B[0m\n\u001B[0;32m    846\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m    847\u001B[0m         error_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe output streaming for the run interrupted.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[0;32m    848\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBut the run is still executing on the compute target. \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[0;32m    849\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetails for canceling the run can be found here: \u001B[39m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[0;32m    850\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 852\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ExperimentExecutionException(error_message)\n\u001B[0;32m    853\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    854\u001B[0m     running_states \u001B[38;5;241m=\u001B[39m RUNNING_STATES\n",
      "\u001B[1;31mExperimentExecutionException\u001B[0m: ExperimentExecutionException:\n\tMessage: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"The output streaming for the run interrupted.\\nBut the run is still executing on the compute target. \\nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "exp = Experiment(ws, 'supervised_baseline_training')\n",
    "exp.start_logging()\n",
    "run = exp.submit(config)\n",
    "print(run)\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}